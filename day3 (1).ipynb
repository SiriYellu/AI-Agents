{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Day 3 (Course Version) — Evidently & DataTalksClub FAQ\n",
        "#\n",
        "# This section reproduces exactly what’s in the official course docs.\n",
        "#\n",
        "# Datasets:\n",
        "#   - Evidently repo (docs folder, chunked into overlapping windows)\n",
        "#   - DataTalksClub FAQ repo (filtered to data-engineering questions)\n",
        "#\n",
        "# Steps:\n",
        "#   1. Clone the repos\n",
        "#   2. Chunk docs (for Evidently)\n",
        "#   3. Build lexical (text) search with minsearch\n",
        "#   4. Build semantic (vector) search with sentence-transformers\n",
        "#   5. Combine into hybrid search\n",
        "#\n",
        "# Example Queries (from docs):\n",
        "#   - \"What should be in a test dataset for AI evaluation?\"   (Evidently)\n",
        "#   - \"Can I join the course now?\"                           (FAQ)\n",
        "#\n",
        "# 👉 Use this cell if you want to reproduce the original course setup.\n",
        "#"
      ],
      "metadata": {
        "id": "rfOyNgeOPav6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Day 3: Text, Vector, Hybrid Search (Evidently + DTC FAQ) ===\n",
        "# Runs end-to-end with no external helpers.\n",
        "\n",
        "!pip -q install minsearch sentence-transformers\n",
        "\n",
        "import os, shutil, json\n",
        "from glob import glob\n",
        "\n",
        "from minsearch import Index, VectorSearch\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "\n",
        "# -----------------------------\n",
        "# Utilities\n",
        "# -----------------------------\n",
        "def sliding_window(text: str, window: int, stride: int):\n",
        "    chunks = []\n",
        "    n = len(text)\n",
        "    if n == 0:\n",
        "        return chunks\n",
        "    i = 0\n",
        "    while i < n:\n",
        "        piece = text[i:i+window]\n",
        "        if not piece:\n",
        "            break\n",
        "        chunks.append({\"chunk\": piece})\n",
        "        i += stride\n",
        "        # ensure tail\n",
        "        if i >= n and i - stride + window < n:\n",
        "            tail_start = max(0, n - window)\n",
        "            tail_piece = text[tail_start:n]\n",
        "            if not chunks or chunks[-1][\"chunk\"] != tail_piece:\n",
        "                chunks.append({\"chunk\": tail_piece})\n",
        "            break\n",
        "    return chunks\n",
        "\n",
        "def read_text_files(root, exts):\n",
        "    paths = []\n",
        "    for ext in exts:\n",
        "        paths.extend(glob(os.path.join(root, f\"**/*{ext}\"), recursive=True))\n",
        "    docs = []\n",
        "    for p in paths:\n",
        "        try:\n",
        "            with open(p, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "                content = f.read()\n",
        "        except Exception:\n",
        "            continue\n",
        "        rel = os.path.relpath(p, root).replace(\"\\\\\", \"/\")\n",
        "        title = os.path.splitext(os.path.basename(p))[0].replace(\"-\",\" \").replace(\"_\",\" \").title()\n",
        "        docs.append({\"filename\": rel, \"title\": title, \"description\": \"\", \"content\": content})\n",
        "    return docs\n",
        "\n",
        "def pretty_print(results, fields=(\"title\",\"filename\",\"chunk\"), max_chars=300):\n",
        "    for i, r in enumerate(results, 1):\n",
        "        print(f\"\\n[{i}] ---\")\n",
        "        for f in fields:\n",
        "            val = r.get(f)\n",
        "            if val:\n",
        "                if isinstance(val, (list, dict)):\n",
        "                    val = json.dumps(val, ensure_ascii=False)\n",
        "                val = str(val)\n",
        "                print(f\"{f}: {val[:max_chars].replace('\\n',' ')}{'...' if len(val)>max_chars else ''}\")\n",
        "\n",
        "def build_text_index(docs, fields):\n",
        "    idx = Index(text_fields=fields, keyword_fields=[])\n",
        "    idx.fit(docs)\n",
        "    return idx\n",
        "\n",
        "def build_vector_index(docs, embed_model_name, text_field_or_fields):\n",
        "    model = SentenceTransformer(embed_model_name)\n",
        "    texts = []\n",
        "    if isinstance(text_field_or_fields, str):\n",
        "        for d in docs:\n",
        "            texts.append(d.get(text_field_or_fields, \"\"))\n",
        "    else:\n",
        "        for d in docs:\n",
        "            parts = [d.get(f, \"\") for f in text_field_or_fields]\n",
        "            texts.append(\" \".join([p for p in parts if p]))\n",
        "    E = model.encode(texts)\n",
        "    if not isinstance(E, np.ndarray):\n",
        "        E = np.array(E)\n",
        "    v = VectorSearch()\n",
        "    v.fit(E, docs)\n",
        "    return v, model\n",
        "\n",
        "def text_search(idx, q, k=5):\n",
        "    return idx.search(q, num_results=k)\n",
        "\n",
        "def vector_search(vindex, model, q, k=5):\n",
        "    qv = model.encode(q)\n",
        "    return vindex.search(qv, num_results=k)\n",
        "\n",
        "def hybrid(text_res, vec_res, key=\"filename\", k=5):\n",
        "    seen, out = set(), []\n",
        "    for r in (text_res + vec_res):\n",
        "        ident = r.get(key) or r.get(\"id\")\n",
        "        if ident in seen:\n",
        "            continue\n",
        "        seen.add(ident)\n",
        "        out.append(r)\n",
        "        if len(out) >= k:\n",
        "            break\n",
        "    return out\n",
        "\n",
        "# -----------------------------\n",
        "# Part A: Evidently docs (chunked)\n",
        "# -----------------------------\n",
        "EVID_REPO_URL = \"https://github.com/evidentlyai/evidently.git\"\n",
        "EVID_REPO_DIR = \"/content/evidently\"\n",
        "\n",
        "# fresh clone\n",
        "if os.path.exists(EVID_REPO_DIR):\n",
        "    shutil.rmtree(EVID_REPO_DIR)\n",
        "!git clone -q {EVID_REPO_URL} {EVID_REPO_DIR}\n",
        "\n",
        "# read docs & chunk\n",
        "evid_docs_dir = os.path.join(EVID_REPO_DIR, \"docs\")\n",
        "evid_docs = read_text_files(evid_docs_dir, exts=(\".md\",\".mdx\",\".txt\",\".rst\"))\n",
        "evid_chunks = []\n",
        "for d in evid_docs:\n",
        "    sw = sliding_window(d[\"content\"], window=2000, stride=1000)\n",
        "    for ch in sw:\n",
        "        ch.update({\"title\": d[\"title\"], \"description\": d[\"description\"], \"filename\": d[\"filename\"]})\n",
        "    evid_chunks.extend(sw)\n",
        "\n",
        "# text index over chunk + metadata\n",
        "evid_tindex = build_text_index(evid_chunks, fields=[\"chunk\",\"title\",\"description\",\"filename\"])\n",
        "# vector index over chunk only\n",
        "evid_vindex, evid_model = build_vector_index(evid_chunks, embed_model_name=\"all-MiniLM-L6-v2\", text_field_or_fields=\"chunk\")\n",
        "\n",
        "print(f\"Evidently: files={len(evid_docs)}, chunks={len(evid_chunks)}\")\n",
        "\n",
        "# Example query (from docs):\n",
        "q1 = \"What should be in a test dataset for AI evaluation?\"\n",
        "tres = text_search(evid_tindex, q1, k=5)\n",
        "vres = vector_search(evid_vindex, evid_model, q1, k=5)\n",
        "hres = hybrid(tres, vres, key=\"filename\", k=5)\n",
        "\n",
        "print(\"\\n=== Evidently | TEXT results ===\")\n",
        "pretty_print(tres, fields=(\"title\",\"filename\",\"chunk\"))\n",
        "print(\"\\n=== Evidently | VECTOR results ===\")\n",
        "pretty_print(vres, fields=(\"title\",\"filename\",\"chunk\"))\n",
        "print(\"\\n=== Evidently | HYBRID (top 5, dedup by filename) ===\")\n",
        "pretty_print(hres, fields=(\"title\",\"filename\",\"chunk\"))\n",
        "\n",
        "# -----------------------------\n",
        "# Part B: DataTalksClub FAQ (no chunking; filter to data-engineering)\n",
        "# -----------------------------\n",
        "FAQ_REPO_URL = \"https://github.com/DataTalksClub/faq.git\"\n",
        "FAQ_REPO_DIR = \"/content/faq\"\n",
        "\n",
        "if os.path.exists(FAQ_REPO_DIR):\n",
        "    shutil.rmtree(FAQ_REPO_DIR)\n",
        "!git clone -q {FAQ_REPO_URL} {FAQ_REPO_DIR}\n",
        "\n",
        "# read all markdown; filter filenames containing 'data-engineering'\n",
        "faq_docs = read_text_files(FAQ_REPO_DIR, exts=(\".md\",\".mdx\",\".txt\",\".rst\"))\n",
        "faq_de = [d for d in faq_docs if \"data-engineering\" in d[\"filename\"].lower()]\n",
        "\n",
        "# normalize fields like in course:\n",
        "# question := title or filename; content := file text\n",
        "for d in faq_de:\n",
        "    d[\"question\"] = d.get(\"title\") or d.get(\"filename\")\n",
        "    d[\"content\"]  = d.get(\"content\",\"\")\n",
        "\n",
        "# text index over Q + A\n",
        "faq_tindex = build_text_index(faq_de, fields=[\"question\",\"content\"])\n",
        "# vector index over concatenated Q + A\n",
        "faq_vindex, faq_model = build_vector_index(faq_de, embed_model_name=\"all-MiniLM-L6-v2\",\n",
        "                                           text_field_or_fields=[\"question\",\"content\"])\n",
        "\n",
        "print(f\"\\nFAQ (data-engineering subset): files={len(faq_de)}\")\n",
        "\n",
        "# Example query (from docs):\n",
        "q2 = \"Can I join the course now?\"\n",
        "tres2 = text_search(faq_tindex, q2, k=5)\n",
        "vres2 = vector_search(faq_vindex, faq_model, q2, k=5)\n",
        "# dedupe by id/filename if exists; fall back to 'filename'\n",
        "hres2 = hybrid(tres2, vres2, key=\"filename\", k=5)\n",
        "\n",
        "print(\"\\n=== FAQ | TEXT results ===\")\n",
        "pretty_print(tres2, fields=(\"question\",\"filename\",\"content\"))\n",
        "print(\"\\n=== FAQ | VECTOR results ===\")\n",
        "pretty_print(vres2, fields=(\"question\",\"filename\",\"content\"))\n",
        "print(\"\\n=== FAQ | HYBRID (top 5) ===\")\n",
        "pretty_print(hres2, fields=(\"question\",\"filename\",\"content\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0pBtafGNPFM6",
        "outputId": "40e7238c-4117-43ba-9e58-4f4a6f8a1be9"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evidently: files=0, chunks=0\n",
            "\n",
            "=== Evidently | TEXT results ===\n",
            "\n",
            "=== Evidently | VECTOR results ===\n",
            "\n",
            "=== Evidently | HYBRID (top 5, dedup by filename) ===\n",
            "\n",
            "FAQ (data-engineering subset): files=449\n",
            "\n",
            "=== FAQ | TEXT results ===\n",
            "\n",
            "[1] ---\n",
            "question: 003 3F1424Af17 Course Can I Still Join The Course After The Start\n",
            "filename: _questions/data-engineering-zoomcamp/general/003_3f1424af17_course-can-i-still-join-the-course-after-the-start.md\n",
            "content: --- id: 3f1424af17 question: 'Course: Can I still join the course after the start date?' sort_order: 3 ---  Yes, even if you don't register, you're still eligible to submit the homework.  Be aware, however, that there will be deadlines for turning in homeworks and the final projects. So don't leave ...\n",
            "\n",
            "[2] ---\n",
            "question: 001 9E508F2212 Course When Does The Course Start\n",
            "filename: _questions/data-engineering-zoomcamp/general/001_9e508f2212_course-when-does-the-course-start.md\n",
            "content: --- id: 9e508f2212 question: 'Course: When does the course start?' sort_order: 1 ---  The next cohort starts January 13th, 2025. More info at [DTC](https://datatalks.club/blog/guide-to-free-online-courses-at-datatalks-club.html).  - Register before the course starts using this [link](https://airtabl...\n",
            "\n",
            "[3] ---\n",
            "question: 008 068529125B Course Can I Follow The Course After It Finishes\n",
            "filename: _questions/data-engineering-zoomcamp/general/008_068529125b_course-can-i-follow-the-course-after-it-finishes.md\n",
            "content: --- id: 068529125b question: Course - Can I follow the course after it finishes? sort_order: 8 ---  Yes, we will keep all the materials available, so you can follow the course at your own pace after it finishes.  You can also continue reviewing the homeworks and prepare for the next cohort. You can ...\n",
            "\n",
            "[4] ---\n",
            "question: 005 33Fc260Cd8 Course What Can I Do Before The Course Starts\n",
            "filename: _questions/data-engineering-zoomcamp/general/005_33fc260cd8_course-what-can-i-do-before-the-course-starts.md\n",
            "content: --- id: 33fc260cd8 question: 'Course: What can I do before the course starts?' sort_order: 5 ---  Start by installing and setting up all the dependencies and requirements:  - Google Cloud account - Google Cloud SDK - Python 3 (installed with Anaconda) - Terraform - Git  Look over the prerequisites a...\n",
            "\n",
            "[5] ---\n",
            "question: 009 C207B8614E Course Can I Get Support If I Take The Course In T\n",
            "filename: _questions/data-engineering-zoomcamp/general/009_c207b8614e_course-can-i-get-support-if-i-take-the-course-in-t.md\n",
            "content: --- id: c207b8614e question: 'Course: Can I get support if I take the course in the self-paced mode?' sort_order: 9 ---  Yes, the Slack channel remains open and you can ask questions there. However, always search the channel first and check the FAQ, as most likely your questions are already answered...\n",
            "\n",
            "=== FAQ | VECTOR results ===\n",
            "\n",
            "[1] ---\n",
            "question: 003 3F1424Af17 Course Can I Still Join The Course After The Start\n",
            "filename: _questions/data-engineering-zoomcamp/general/003_3f1424af17_course-can-i-still-join-the-course-after-the-start.md\n",
            "content: --- id: 3f1424af17 question: 'Course: Can I still join the course after the start date?' sort_order: 3 ---  Yes, even if you don't register, you're still eligible to submit the homework.  Be aware, however, that there will be deadlines for turning in homeworks and the final projects. So don't leave ...\n",
            "\n",
            "[2] ---\n",
            "question: 009 C207B8614E Course Can I Get Support If I Take The Course In T\n",
            "filename: _questions/data-engineering-zoomcamp/general/009_c207b8614e_course-can-i-get-support-if-i-take-the-course-in-t.md\n",
            "content: --- id: c207b8614e question: 'Course: Can I get support if I take the course in the self-paced mode?' sort_order: 9 ---  Yes, the Slack channel remains open and you can ask questions there. However, always search the channel first and check the FAQ, as most likely your questions are already answered...\n",
            "\n",
            "[3] ---\n",
            "question: 008 068529125B Course Can I Follow The Course After It Finishes\n",
            "filename: _questions/data-engineering-zoomcamp/general/008_068529125b_course-can-i-follow-the-course-after-it-finishes.md\n",
            "content: --- id: 068529125b question: Course - Can I follow the course after it finishes? sort_order: 8 ---  Yes, we will keep all the materials available, so you can follow the course at your own pace after it finishes.  You can also continue reviewing the homeworks and prepare for the next cohort. You can ...\n",
            "\n",
            "[4] ---\n",
            "question: 001 9E508F2212 Course When Does The Course Start\n",
            "filename: _questions/data-engineering-zoomcamp/general/001_9e508f2212_course-when-does-the-course-start.md\n",
            "content: --- id: 9e508f2212 question: 'Course: When does the course start?' sort_order: 1 ---  The next cohort starts January 13th, 2025. More info at [DTC](https://datatalks.club/blog/guide-to-free-online-courses-at-datatalks-club.html).  - Register before the course starts using this [link](https://airtabl...\n",
            "\n",
            "[5] ---\n",
            "question: 005 33Fc260Cd8 Course What Can I Do Before The Course Starts\n",
            "filename: _questions/data-engineering-zoomcamp/general/005_33fc260cd8_course-what-can-i-do-before-the-course-starts.md\n",
            "content: --- id: 33fc260cd8 question: 'Course: What can I do before the course starts?' sort_order: 5 ---  Start by installing and setting up all the dependencies and requirements:  - Google Cloud account - Google Cloud SDK - Python 3 (installed with Anaconda) - Terraform - Git  Look over the prerequisites a...\n",
            "\n",
            "=== FAQ | HYBRID (top 5) ===\n",
            "\n",
            "[1] ---\n",
            "question: 003 3F1424Af17 Course Can I Still Join The Course After The Start\n",
            "filename: _questions/data-engineering-zoomcamp/general/003_3f1424af17_course-can-i-still-join-the-course-after-the-start.md\n",
            "content: --- id: 3f1424af17 question: 'Course: Can I still join the course after the start date?' sort_order: 3 ---  Yes, even if you don't register, you're still eligible to submit the homework.  Be aware, however, that there will be deadlines for turning in homeworks and the final projects. So don't leave ...\n",
            "\n",
            "[2] ---\n",
            "question: 001 9E508F2212 Course When Does The Course Start\n",
            "filename: _questions/data-engineering-zoomcamp/general/001_9e508f2212_course-when-does-the-course-start.md\n",
            "content: --- id: 9e508f2212 question: 'Course: When does the course start?' sort_order: 1 ---  The next cohort starts January 13th, 2025. More info at [DTC](https://datatalks.club/blog/guide-to-free-online-courses-at-datatalks-club.html).  - Register before the course starts using this [link](https://airtabl...\n",
            "\n",
            "[3] ---\n",
            "question: 008 068529125B Course Can I Follow The Course After It Finishes\n",
            "filename: _questions/data-engineering-zoomcamp/general/008_068529125b_course-can-i-follow-the-course-after-it-finishes.md\n",
            "content: --- id: 068529125b question: Course - Can I follow the course after it finishes? sort_order: 8 ---  Yes, we will keep all the materials available, so you can follow the course at your own pace after it finishes.  You can also continue reviewing the homeworks and prepare for the next cohort. You can ...\n",
            "\n",
            "[4] ---\n",
            "question: 005 33Fc260Cd8 Course What Can I Do Before The Course Starts\n",
            "filename: _questions/data-engineering-zoomcamp/general/005_33fc260cd8_course-what-can-i-do-before-the-course-starts.md\n",
            "content: --- id: 33fc260cd8 question: 'Course: What can I do before the course starts?' sort_order: 5 ---  Start by installing and setting up all the dependencies and requirements:  - Google Cloud account - Google Cloud SDK - Python 3 (installed with Anaconda) - Terraform - Git  Look over the prerequisites a...\n",
            "\n",
            "[5] ---\n",
            "question: 009 C207B8614E Course Can I Get Support If I Take The Course In T\n",
            "filename: _questions/data-engineering-zoomcamp/general/009_c207b8614e_course-can-i-get-support-if-i-take-the-course-in-t.md\n",
            "content: --- id: c207b8614e question: 'Course: Can I get support if I take the course in the self-paced mode?' sort_order: 9 ---  Yes, the Slack channel remains open and you can ask questions there. However, always search the channel first and check the FAQ, as most likely your questions are already answered...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Day 3 (My Project Version) — DermaScan Android App\n",
        "#\n",
        "# This section adapts the official Day 3 workflow to my own project.\n",
        "#\n",
        "# Dataset:\n",
        "#   - Repository: DermaScan_AndroidApp (cloned from GitHub)\n",
        "#   - Files scanned: .md, .txt, .java, .kt, .xml\n",
        "#\n",
        "# Steps:\n",
        "#   1. Clone my repo into Colab\n",
        "#   2. Chunk documentation & code (sliding window, saved as JSONL for caching)\n",
        "#   3. Build lexical (text) search index with minsearch\n",
        "#   4. Build semantic (vector) search index with sentence-transformers\n",
        "#   5. Combine into hybrid search\n",
        "#\n",
        "# Example Queries (for my repo):\n",
        "#   - \"What models are used and what dataset is used in this project?\"\n",
        "#   - \"How is the DermaScan Android app deployed and what technologies are used in the mobile app?\"\n",
        "\n"
      ],
      "metadata": {
        "id": "hMntNXKHPvtu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lvzMjm7xKKJx",
        "outputId": "64c229b3-5db9-4dd5-f342-99ce5c80cc82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.12.11\n",
            "CUDA available: False\n",
            "Using CPU\n"
          ]
        }
      ],
      "source": [
        "#  Setup: install deps and check GPU\n",
        "!pip -q install minsearch sentence-transformers\n",
        "import torch, platform\n",
        "print('Python', platform.python_version())\n",
        "print('CUDA available:', torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print('GPU:', torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    print('Using CPU')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  Configure your repo and file types\n",
        "REPO_URL = \"https://github.com/SiriYellu/DermaScan_AndroidApp.git\"  # <- change if needed\n",
        "REPO_DIR = \"/content/DermaScan_AndroidApp\"                          # where to clone\n",
        "SCAN_DIR = REPO_DIR                                                  # which subfolder to scan\n",
        "EXTS = [\".md\", \".txt\", \".java\", \".kt\", \".xml\"]               # add more: .py, .ipynb, .rst, etc.\n",
        "\n",
        "# Query to run\n",
        "QUERY = \"What models are used and what dataset is used in this project?\"\n",
        "TOPK = 5\n"
      ],
      "metadata": {
        "id": "ZuJ23c0sMj-o"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ⬇ Clone repo (safe to re-run)\n",
        "import os, shutil, subprocess\n",
        "if os.path.exists(REPO_DIR):\n",
        "    shutil.rmtree(REPO_DIR)\n",
        "print('Cloning', REPO_URL)\n",
        "!git clone -q {REPO_URL} {REPO_DIR}\n",
        "print('Done!')\n",
        "!ls -la {REPO_DIR} | head -n 20\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ekqZQJZMMoh_",
        "outputId": "fcdb55f5-40d6-4cb8-9845-5cd20dbc9486"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning https://github.com/SiriYellu/DermaScan_AndroidApp.git\n",
            "Done!\n",
            "total 35580\n",
            "drwxr-xr-x 3 root root     4096 Oct  2 00:57 .\n",
            "drwxr-xr-x 1 root root     4096 Oct  2 00:57 ..\n",
            "drwxr-xr-x 8 root root     4096 Oct  2 00:57 .git\n",
            "-rw-r--r-- 1 root root   759019 Oct  2 00:57 Poster-GC-123.pdf\n",
            "-rw-r--r-- 1 root root  2405831 Oct  2 00:57 PPT.pptx\n",
            "-rw-r--r-- 1 root root     4181 Oct  2 00:57 README.md\n",
            "-rw-r--r-- 1 root root 10120802 Oct  2 00:57 Recording 2025-09-28 114054.mp4\n",
            "-rw-r--r-- 1 root root 22967109 Oct  2 00:57 SCapp_split.zip\n",
            "-rw-r--r-- 1 root root   151190 Oct  2 00:57 Screenshot 2025-09-28 114520.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  Helpers: chunking & IO\n",
        "from glob import glob\n",
        "from typing import List, Dict, Any\n",
        "import json, os\n",
        "\n",
        "def sliding_window(text: str, window: int, stride: int) -> List[Dict[str, Any]]:\n",
        "    chunks = []\n",
        "    n = len(text)\n",
        "    if n == 0:\n",
        "        return chunks\n",
        "    i = 0\n",
        "    while i < n:\n",
        "        piece = text[i:i+window]\n",
        "        if not piece:\n",
        "            break\n",
        "        chunks.append({\"chunk\": piece})\n",
        "        i += stride\n",
        "        # ensure tail\n",
        "        if i >= n and i - stride + window < n:\n",
        "            tail_start = max(0, n - window)\n",
        "            tail_piece = text[tail_start:n]\n",
        "            if not chunks or chunks[-1][\"chunk\"] != tail_piece:\n",
        "                chunks.append({\"chunk\": tail_piece})\n",
        "            break\n",
        "    return chunks\n",
        "\n",
        "def read_docs_from_dir(docs_dir: str, exts: List[str]) -> List[Dict[str, Any]]:\n",
        "    paths = []\n",
        "    for ext in exts:\n",
        "        paths.extend(glob(os.path.join(docs_dir, f\"**/*{ext}\"), recursive=True))\n",
        "    out = []\n",
        "    for p in paths:\n",
        "        try:\n",
        "            with open(p, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "                content = f.read()\n",
        "        except Exception:\n",
        "            continue\n",
        "        rel = os.path.relpath(p, docs_dir)\n",
        "        title = os.path.splitext(os.path.basename(p))[0].replace(\"-\",\" \").replace(\"_\",\" \").title()\n",
        "        out.append({\n",
        "            \"filename\": rel,\n",
        "            \"title\": title,\n",
        "            \"description\": \"\",\n",
        "            \"content\": content\n",
        "        })\n",
        "    return out\n",
        "\n",
        "def write_jsonl(path: str, rows: List[Dict[str, Any]]):\n",
        "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "        for r in rows:\n",
        "            f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n"
      ],
      "metadata": {
        "id": "IyZK1irjMtti"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Build chunks (adjust window/stride as needed)\n",
        "WINDOW, STRIDE = 1000, 500\n",
        "print('Scanning', SCAN_DIR, 'for', EXTS)\n",
        "docs = read_docs_from_dir(SCAN_DIR, EXTS)\n",
        "print('Found files:', len(docs))\n",
        "\n",
        "chunks = []\n",
        "for d in docs:\n",
        "    sw = sliding_window(d[\"content\"], WINDOW, STRIDE)\n",
        "    for ch in sw:\n",
        "        ch.update({\n",
        "            \"title\": d.get(\"title\", \"\"),\n",
        "            \"description\": d.get(\"description\", \"\"),\n",
        "            \"filename\": d.get(\"filename\", \"\"),\n",
        "        })\n",
        "    chunks.extend(sw)\n",
        "print('Total chunks:', len(chunks))\n",
        "\n",
        "# Save chunks to enable embedding cache\n",
        "CHUNKS_JSONL = \"/content/chunks.jsonl\"\n",
        "write_jsonl(CHUNKS_JSONL, chunks)\n",
        "print('Saved:', CHUNKS_JSONL)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yUrjWAuHMvJ1",
        "outputId": "807df7b0-daa1-4c0c-d131-0cacbf5a71df"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scanning /content/DermaScan_AndroidApp for ['.md', '.txt', '.java', '.kt', '.xml']\n",
            "Found files: 1\n",
            "Total chunks: 9\n",
            "Saved: /content/chunks.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 🔍 Build indices (with embedding cache)\n",
        "from minsearch import Index, VectorSearch\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np, json, os, hashlib\n",
        "\n",
        "def load_chunks(jsonl_path):\n",
        "    rows = []\n",
        "    with open(jsonl_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            if line.strip():\n",
        "                rows.append(json.loads(line))\n",
        "    return rows\n",
        "\n",
        "def file_hash(path):\n",
        "    h = hashlib.sha256()\n",
        "    with open(path, 'rb') as f:\n",
        "        while True:\n",
        "            b = f.read(1024*1024)\n",
        "            if not b:\n",
        "                break\n",
        "            h.update(b)\n",
        "    return h.hexdigest()\n",
        "\n",
        "chunks = load_chunks(CHUNKS_JSONL)\n",
        "print('Loaded chunks:', len(chunks))\n",
        "\n",
        "# Text index\n",
        "text_fields = [\"chunk\", \"title\", \"description\", \"filename\"]\n",
        "tindex = Index(text_fields=text_fields, keyword_fields=[])\n",
        "tindex.fit(chunks)\n",
        "\n",
        "# Vector index with cache\n",
        "MODEL_NAME = \"all-MiniLM-L6-v2\"  # fast, good quality\n",
        "EMB_PATH = \"/content/embeddings.npy\"\n",
        "META_PATH = \"/content/embeddings.meta.json\"\n",
        "model = SentenceTransformer(MODEL_NAME, device='cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "need_encode = True\n",
        "new_hash = file_hash(CHUNKS_JSONL)\n",
        "if os.path.exists(EMB_PATH) and os.path.exists(META_PATH):\n",
        "    with open(META_PATH, 'r') as f:\n",
        "        meta = json.load(f)\n",
        "    if meta.get('chunks_hash') == new_hash and meta.get('model') == MODEL_NAME and meta.get('count') == len(chunks):\n",
        "        E = np.load(EMB_PATH)\n",
        "        need_encode = False\n",
        "        print('✅ Loaded cached embeddings:', E.shape)\n",
        "\n",
        "if need_encode:\n",
        "    print('Encoding embeddings ...')\n",
        "    texts = [c['chunk'] for c in chunks]\n",
        "    E = model.encode(texts)\n",
        "    if not isinstance(E, np.ndarray):\n",
        "        E = np.array(E)\n",
        "    np.save(EMB_PATH, E)\n",
        "    with open(META_PATH, 'w') as f:\n",
        "        json.dump({\n",
        "            'chunks_hash': new_hash,\n",
        "            'model': MODEL_NAME,\n",
        "            'count': len(chunks)\n",
        "        }, f)\n",
        "    print('Saved cache:', EMB_PATH)\n",
        "\n",
        "vindex = VectorSearch()\n",
        "vindex.fit(E, chunks)\n",
        "print('Vector index ready:', E.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y6X0nPJ7Mybo",
        "outputId": "23bdbc99-7c3e-4029-9b5a-2870ab017edc"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded chunks: 9\n",
            "✅ Loaded cached embeddings: (9, 384)\n",
            "Vector index ready: (9, 384)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 🔎 Run hybrid search\n",
        "def text_search(q):\n",
        "    return tindex.search(q, num_results=TOPK)\n",
        "\n",
        "def vector_search(q):\n",
        "    qv = model.encode(q)\n",
        "    return vindex.search(qv, num_results=TOPK)\n",
        "\n",
        "def hybrid_search(q):\n",
        "    a = text_search(q)\n",
        "    b = vector_search(q)\n",
        "    seen, out = set(), []\n",
        "    for r in a + b:\n",
        "        key = r.get('filename') or r.get('id')\n",
        "        if key in seen:\n",
        "            continue\n",
        "        seen.add(key)\n",
        "        out.append(r)\n",
        "        if len(out) >= TOPK:\n",
        "            break\n",
        "    return out\n",
        "\n",
        "print('QUERY:', QUERY)\n",
        "results = hybrid_search(QUERY)\n",
        "for i, r in enumerate(results, 1):\n",
        "    print(f\"\\n[{i}] ---\")\n",
        "    print('filename:', r.get('filename'))\n",
        "    print('title   :', r.get('title'))\n",
        "    chunk = (r.get('chunk') or '')\n",
        "    print('chunk   :', chunk[:500].replace('\\n',' ') + ('...' if len(chunk)>500 else ''))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7SMUVRBlM2eX",
        "outputId": "4f7c2972-1c25-421a-fa95-b4a207afad10"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "QUERY: What models are used and what dataset is used in this project?\n",
            "\n",
            "[1] ---\n",
            "filename: README.md\n",
            "title   : Readme\n",
            "chunk   : ng** – Combines multiple models for robust decision-making.   - 🧩 **Lesion Segmentation** – Attention U-Net for precise boundary detection.   - 📚 **Awareness Content** – In-app educational resources for users.    ---  ## 🛠 Tech Stack   - **Android**: Java / Kotlin   - **AI Models**: TensorFlow Lite (TFLite)   - **Model Training**: Python (PyTorch/TensorFlow)   - **Image Processing**: OpenCV   - **Dataset**: [HAM10000](https://www.kaggle.com/kmader/skin-cancer-mnist-ham10000) (10,000+ dermoscopic...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q = input(\"Enter your question: \")\n",
        "res = hybrid_search(q)\n",
        "for i, r in enumerate(res, 1):\n",
        "    print(f\"\\n[{i}] ---\")\n",
        "    print('filename:', r.get('filename'))\n",
        "    print('title   :', r.get('title'))\n",
        "    ch = (r.get('chunk') or '')\n",
        "    print('chunk   :', ch[:500].replace('\\n',' ') + ('...' if len(ch)>500 else ''))\n"
      ],
      "metadata": {
        "id": "wZX-LPHLNT39"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}