{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üöÄ Day 6: Publish the Agent\n",
        "\n",
        "Today I cleaned up the code, moved it into modular Python files,  \n",
        "and built a **Streamlit web app** for the DermaScan Repo Assistant.  \n",
        "\n",
        "- Created `app.py` with chat UI  \n",
        "- Added background + styling  \n",
        "- Integrated Gemini + lexical fallback  \n",
        "- Deployed on Streamlit Cloud for public access  \n",
        "\n",
        "Now anyone can interact with the repo assistant online üéâ\n"
      ],
      "metadata": {
        "id": "LgwktvqQsEtE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install streamlit google-generativeai minsearch requests python-frontmatter pyngrok\n",
        "!pkill -f streamlit || true\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ve9-g8GSyoc1",
        "outputId": "7199749e-e7f9-472e-c27b-b4ef16c190cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m75.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m123.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"GEMINI_API_KEY\"] = input(\"üîë Enter GEMINI_API_KEY: \").strip()\n",
        "print(\"‚úÖ GEMINI_API_KEY set (session only)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YYlMPHmp2D8h",
        "outputId": "26d2c3eb-83aa-45c3-e4a6-7d7e0990e676"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîë Enter GEMINI_API_KEY: AIzaSyAeLiW3r6auP6D3xiNJisqq8eoFXeVlJ-g\n",
            "‚úÖ GEMINI_API_KEY set (session only)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# üîê Set ngrok authtoken, then open the tunnel and run Streamlit\n",
        "!pip -q install pyngrok\n",
        "\n",
        "from pyngrok import ngrok, conf\n",
        "import subprocess, time\n",
        "\n",
        "# ‚¨ÖÔ∏è paste your token here (or input() if you prefer)\n",
        "AUTHTOKEN = \"33UbfNigxZFzhFVX53qAhSyEEs4_6x1AUpxmFmuJm7jspaKGa\"\n",
        "conf.get_default().auth_token = AUTHTOKEN\n",
        "\n",
        "# Kill old tunnels\n",
        "try: ngrok.kill()\n",
        "except: pass\n",
        "\n",
        "# Start tunnel to Streamlit port\n",
        "public_url = ngrok.connect(8501, \"http\").public_url\n",
        "print(\"üåê Public URL:\", public_url)\n",
        "\n",
        "# Launch Streamlit (if not already running)\n",
        "proc = subprocess.Popen([\"streamlit\", \"run\", \"app.py\"])\n",
        "time.sleep(6)\n",
        "print(\"‚úÖ Streamlit starting‚Ä¶ open the URL above. If blank, wait ~10‚Äì20s and refresh.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5t2qHpDM2Lij",
        "outputId": "91bc187f-4aeb-45df-90fa-92c2a4fdb6da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üåê Public URL: https://creepingly-metalinguistic-judson.ngrok-free.dev\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2025-10-02T05:06:05+0000 lvl=warn msg=\"failed to check for update\" obj=updater err=\"Post \\\"https://update.equinox.io/check\\\": context deadline exceeded\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Streamlit starting‚Ä¶ open the URL above. If blank, wait ~10‚Äì20s and refresh.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json, os\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "\n",
        "LOG_DIR = Path(\"logs\")\n",
        "\n",
        "if not LOG_DIR.exists():\n",
        "    print(\"No logs yet. Open your public URL and ask a question in the app first.\")\n",
        "else:\n",
        "    files = sorted(LOG_DIR.glob(\"*.json\"), key=lambda p: p.stat().st_mtime, reverse=True)\n",
        "    if not files:\n",
        "        print(\"No logs found yet. Ask something in the app, then re-run this cell.\")\n",
        "    else:\n",
        "        rows = []\n",
        "        for p in files[:10]:  # show last 10\n",
        "            try:\n",
        "                rec = json.loads(p.read_text())\n",
        "                rows.append({\n",
        "                    \"file\": p.name,\n",
        "                    \"timestamp\": rec.get(\"timestamp\", \"\"),\n",
        "                    \"model\": rec.get(\"model\", \"\"),\n",
        "                    \"question\": rec.get(\"question\", \"\")[:120],\n",
        "                    \"answer\": rec.get(\"answer\", \"\")[:140],\n",
        "                    \"results_used\": len(rec.get(\"results\", [])),\n",
        "                })\n",
        "            except Exception as e:\n",
        "                rows.append({\"file\": p.name, \"error\": str(e)})\n",
        "\n",
        "        df = pd.DataFrame(rows)\n",
        "        display(df)\n",
        "        if rows:\n",
        "            print(\"\\nOpen the app and ask more questions, then re-run this cell to see new logs.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9wQjSPlaUNMy",
        "outputId": "ce4063ad-c369-4a7a-a89e-2be67c6fb976"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No logs yet. Open your public URL and ask a question in the app first.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Kill any old servers/tunnels\n",
        "import os, subprocess, signal, time\n",
        "\n",
        "def _pkill(pattern):\n",
        "    try:\n",
        "        subprocess.call([\"pkill\",\"-f\",pattern])\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "for p in [\"streamlit\", \"cloudflared\"]:\n",
        "    _pkill(p)\n",
        "\n",
        "time.sleep(1)\n",
        "print(\"‚úÖ Cleaned any old streamlit/cloudflared processes.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPja-MP_U0Rt",
        "outputId": "225e9fee-11ad-4c68-ea77-961434192278"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2025-10-02T05:09:33+0000 lvl=warn msg=\"failed to open private leg\" id=f0ae940adb54 privaddr=localhost:8501 err=\"dial tcp 127.0.0.1:8501: connect: connection refused\"\n",
            "WARNING:pyngrok.process.ngrok:t=2025-10-02T05:09:33+0000 lvl=warn msg=\"failed to open private leg\" id=d3748042ac3b privaddr=localhost:8501 err=\"dial tcp 127.0.0.1:8501: connect: connection refused\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Cleaned any old streamlit/cloudflared processes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Start Streamlit with explicit args and log to file\n",
        "!rm -f streamlit.log\n",
        "!nohup streamlit run app.py --server.port 8501 --server.address 0.0.0.0 --browser.gatherUsageStats=false --server.headless=true > streamlit.log 2>&1 &\n",
        "import time, requests\n",
        "time.sleep(5)\n",
        "\n",
        "# Quick local health check (should be 'ok')\n",
        "try:\n",
        "    print(\"Health:\", requests.get(\"http://127.0.0.1:8501/_stcore/health\", timeout=5).text)\n",
        "except Exception as e:\n",
        "    print(\"Health check error:\", e)\n",
        "\n",
        "# Show last 40 lines of Streamlit log to spot errors\n",
        "!tail -n 40 streamlit.log\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tpnukrSHU3sX",
        "outputId": "ce2c459d-75a1-46c7-8e29-346c1b726e77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Health: ok\n",
            "\n",
            "  You can now view your Streamlit app in your browser.\n",
            "\n",
            "  URL: http://0.0.0.0:8501\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the standalone Linux binary and make it executable\n",
        "!wget -q -O cloudflared https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64\n",
        "!chmod +x cloudflared\n",
        "!./cloudflared --version\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rYq1aA6iVIZV",
        "outputId": "6b52f958-dfd8-4d60-94b7-a74d2956164f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cloudflared version 2025.9.1 (built 2025-09-22-13:28 UTC)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess, re, time, threading, queue, sys\n",
        "\n",
        "# Start cloudflared tunnel to your running Streamlit on port 8501\n",
        "cf_proc = subprocess.Popen(\n",
        "    [\"./cloudflared\", \"tunnel\", \"--url\", \"http://localhost:8501\", \"--no-autoupdate\"],\n",
        "    stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True\n",
        ")\n",
        "\n",
        "q = queue.Queue()\n",
        "def reader(proc, q):\n",
        "    for line in iter(proc.stdout.readline, ''):\n",
        "        q.put(line)\n",
        "\n",
        "thr = threading.Thread(target=reader, args=(cf_proc, q), daemon=True)\n",
        "thr.start()\n",
        "\n",
        "print(\"‚è≥ Opening Cloudflare tunnel‚Ä¶\")\n",
        "public_url = None\n",
        "start = time.time()\n",
        "\n",
        "while time.time() - start < 90:\n",
        "    try:\n",
        "        line = q.get(timeout=1)\n",
        "    except queue.Empty:\n",
        "        continue\n",
        "    sys.stdout.write(line)\n",
        "    sys.stdout.flush()\n",
        "    m = re.search(r'(https://[a-z0-9\\-]+\\.trycloudflare\\.com)', line)\n",
        "    if m:\n",
        "        public_url = m.group(1)\n",
        "        break\n",
        "\n",
        "if public_url:\n",
        "    print(\"\\nüåê Public URL:\", public_url)\n",
        "    print(\"Tip: if it‚Äôs blank, wait ~10‚Äì20s and refresh. First run downloads the repo & builds the index.\")\n",
        "else:\n",
        "    print(\"\\n‚ùå Couldn‚Äôt detect Cloudflare URL. Scroll the logs above for hints.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-I5FMIk6VLVG",
        "outputId": "205aecbd-990b-4380-93c2-9d97c85f3484"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚è≥ Opening Cloudflare tunnel‚Ä¶\n",
            "2025-10-02T05:11:05Z INF Thank you for trying Cloudflare Tunnel. Doing so, without a Cloudflare account, is a quick way to experiment and try it out. However, be aware that these account-less Tunnels have no uptime guarantee, are subject to the Cloudflare Online Services Terms of Use (https://www.cloudflare.com/website-terms/), and Cloudflare reserves the right to investigate your use of Tunnels for violations of such terms. If you intend to use Tunnels in production you should use a pre-created named tunnel by following: https://developers.cloudflare.com/cloudflare-one/connections/connect-apps\n",
            "2025-10-02T05:11:05Z INF Requesting new quick Tunnel on trycloudflare.com...\n",
            "2025-10-02T05:11:08Z INF +--------------------------------------------------------------------------------------------+\n",
            "2025-10-02T05:11:08Z INF |  Your quick Tunnel has been created! Visit it at (it may take some time to be reachable):  |\n",
            "2025-10-02T05:11:08Z INF |  https://immediate-jpg-texts-stamps.trycloudflare.com                                      |\n",
            "\n",
            "üåê Public URL: https://immediate-jpg-texts-stamps.trycloudflare.com\n",
            "Tip: if it‚Äôs blank, wait ~10‚Äì20s and refresh. First run downloads the repo & builds the index.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install streamlit google-generativeai minsearch requests python-frontmatter\n"
      ],
      "metadata": {
        "id": "i9tWVHEHWdJF"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stop anything old\n",
        "import subprocess, time\n",
        "subprocess.call([\"pkill\",\"-f\",\"streamlit\"])\n",
        "time.sleep(1)\n",
        "\n",
        "# (Optional) set Gemini key if not already in THIS runtime\n",
        "# import os; os.environ[\"GEMINI_API_KEY\"] = \"YOUR_GEMINI_KEY\"\n",
        "\n",
        "# Start your real Streamlit app\n",
        "!nohup streamlit run app.py \\\n",
        "  --server.port 8501 \\\n",
        "  --server.address 0.0.0.0 \\\n",
        "  --server.headless=true \\\n",
        "  --server.enableCORS=false \\\n",
        "  --server.enableXsrfProtection=false \\\n",
        "  --browser.gatherUsageStats=false > streamlit.log 2>&1 &\n",
        "\n",
        "import time, requests\n",
        "time.sleep(6)\n",
        "\n",
        "# Health + last log lines\n",
        "try:\n",
        "    print(\"Health:\", requests.get(\"http://127.0.0.1:8501/_stcore/health\", timeout=5).text)\n",
        "except Exception as e:\n",
        "    print(\"Health check error:\", e)\n",
        "\n",
        "!tail -n 80 streamlit.log\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cOpU75xzWVQA",
        "outputId": "5b23adbb-b187-4ae6-9813-635b2a3e330d"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Health: ok\n",
            "\n",
            "  You can now view your Streamlit app in your browser.\n",
            "\n",
            "  URL: http://0.0.0.0:8501\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "# -----------------------------------------------------------\n",
        "# DermaScan Repo Assistant ‚Äî Streamlit UI with Gemini + Fallback + BG + Chat Bubbles\n",
        "# -----------------------------------------------------------\n",
        "import os, io, json, zipfile, secrets, re, base64\n",
        "from datetime import datetime, UTC\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Any, Tuple\n",
        "\n",
        "import streamlit as st\n",
        "import requests\n",
        "from minsearch import Index\n",
        "\n",
        "# Gemini optional\n",
        "try:\n",
        "    import google.generativeai as genai\n",
        "except Exception:\n",
        "    genai = None\n",
        "\n",
        "# ===================== CONFIG =====================\n",
        "DEFAULT_REPO_OWNER = \"SiriYellu\"\n",
        "DEFAULT_REPO_NAME  = \"DermaScan_AndroidApp\"\n",
        "\n",
        "TEXT_EXTS  = (\".md\", \".mdx\", \".txt\", \".java\", \".kt\", \".xml\", \".py\", \".rst\")\n",
        "WINDOW     = 1000\n",
        "STRIDE     = 500\n",
        "MODEL_TRY  = [\n",
        "    \"gemini-2.5-flash\",\n",
        "    \"gemini-pro-latest\",\n",
        "    \"gemini-2.0-flash\",\n",
        "]\n",
        "# ==================================================\n",
        "\n",
        "# ----------------- Helpers -----------------\n",
        "def now_iso() -> str:\n",
        "    return datetime.now(UTC).isoformat()\n",
        "\n",
        "def ts_compact() -> str:\n",
        "    return datetime.now(UTC).strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "def human_file(path: str) -> str:\n",
        "    return Path(path).name\n",
        "\n",
        "def badge(text: str) -> str:\n",
        "    return f\"<span class='badge'>{text}</span>\"\n",
        "\n",
        "def sanitize_md(s: str, limit: int = 1200) -> str:\n",
        "    s = (s or \"\").strip()\n",
        "    s = re.sub(r\"\\n{3,}\", \"\\n\\n\", s)\n",
        "    if len(s) > limit:\n",
        "        s = s[:limit] + \"...\"\n",
        "    return s\n",
        "\n",
        "def get_base64_of_bin_file(bin_file: str) -> str:\n",
        "    with open(bin_file, \"rb\") as f:\n",
        "        return base64.b64encode(f.read()).decode()\n",
        "# -------------------------------------------\n",
        "\n",
        "# ----------------- Theming -----------------\n",
        "st.set_page_config(page_title=\"DermaScan Agent\", page_icon=\"ü©∫\", layout=\"wide\")\n",
        "\n",
        "# Background image (base64 so it works from Colab)\n",
        "bg_path = \"/content/Gemini_Generated_Image_msjjscmsjjscmsjj.png\"  # <- your file\n",
        "bg_css = \"\"\n",
        "try:\n",
        "    bg_base64 = get_base64_of_bin_file(bg_path)\n",
        "    bg_css = f\"\"\"\n",
        "      .stApp {{\n",
        "        background: url(\"data:image/png;base64,{bg_base64}\") no-repeat center center fixed;\n",
        "        background-size: cover;\n",
        "      }}\n",
        "    \"\"\"\n",
        "except Exception as e:\n",
        "    # no image is okay ‚Äì we still render the rest\n",
        "    pass\n",
        "\n",
        "st.markdown(\n",
        "    f\"\"\"\n",
        "    <style>\n",
        "      {bg_css}\n",
        "      :root {{\n",
        "        --accent:#7c83ff; --bg:#0f1117; --panel:#111421; --text:#eaeefb; --muted:#9aa4b2;\n",
        "      }}\n",
        "      .main, .stApp {{ color: var(--text); }}\n",
        "\n",
        "      /* Small pill/badges for sources */\n",
        "      .badge {{\n",
        "        display:inline-block; padding:4px 8px; border-radius:8px;\n",
        "        background:#1b1f2e; border:1px solid #283148; color:#c6d0e4;\n",
        "        font-size:12px; margin:0 6px 6px 0;\n",
        "      }}\n",
        "\n",
        "      /* Chat ‚Äúbubble‚Äù look (semi-transparent black boxes) */\n",
        "      .bubble {{\n",
        "        background: rgba(7, 10, 18, 0.66);\n",
        "        border: 1px solid rgba(255, 255, 255, 0.08);\n",
        "        border-radius: 16px;\n",
        "        padding: 16px 18px;\n",
        "        box-shadow: 0 8px 28px rgba(0,0,0,0.35);\n",
        "        backdrop-filter: saturate(120%) blur(2px);\n",
        "      }}\n",
        "      .bubble.question {{\n",
        "        background: rgba(21, 23, 31, 0.65);\n",
        "        border: 1px solid rgba(124, 131, 255, 0.18);\n",
        "      }}\n",
        "      .bubble.answer {{\n",
        "        background: rgba(12, 14, 22, 0.72);\n",
        "        border: 1px solid rgba(72, 81, 130, 0.25);\n",
        "      }}\n",
        "      .sources-row {{ margin-top: 10px; }}\n",
        "    </style>\n",
        "    \"\"\",\n",
        "    unsafe_allow_html=True,\n",
        ")\n",
        "# -------------------------------------------\n",
        "\n",
        "# ---------------- Ingestion ----------------\n",
        "@st.cache_resource(show_spinner=True)\n",
        "def ingest_repo(owner: str, name: str, exts: tuple, window: int, stride: int) -> Tuple[Index, List[Dict[str,Any]]]:\n",
        "    \"\"\"Download GitHub repo as ZIP, extract text files, chunk, and build lexical index.\"\"\"\n",
        "    url = f\"https://codeload.github.com/{owner}/{name}/zip/refs/heads/main\"\n",
        "    resp = requests.get(url, timeout=60)\n",
        "    resp.raise_for_status()\n",
        "\n",
        "    docs: List[Dict[str,Any]] = []\n",
        "    with zipfile.ZipFile(io.BytesIO(resp.content)) as zf:\n",
        "        for info in zf.infolist():\n",
        "            fn = info.filename\n",
        "            if not fn.lower().endswith(exts):\n",
        "                continue\n",
        "            try:\n",
        "                with zf.open(info) as f:\n",
        "                    raw = f.read().decode(\"utf-8\", errors=\"ignore\")\n",
        "            except Exception:\n",
        "                continue\n",
        "            try:\n",
        "                _, rel = fn.split(\"/\", maxsplit=1)\n",
        "            except ValueError:\n",
        "                rel = fn\n",
        "            docs.append({\"filename\": rel, \"title\": Path(rel).stem, \"content\": raw})\n",
        "\n",
        "    chunks = []\n",
        "    for d in docs:\n",
        "        text = d[\"content\"] or \"\"\n",
        "        n = len(text); i = 0\n",
        "        if n == 0:\n",
        "            continue\n",
        "        while i < n:\n",
        "            piece = text[i:i+window]\n",
        "            chunks.append({\"content\": piece, \"filename\": d[\"filename\"], \"title\": d[\"title\"]})\n",
        "            i += stride\n",
        "            # tail coverage\n",
        "            if i >= n and i - stride + window < n:\n",
        "                tail_start = max(0, n - window)\n",
        "                tail_piece = text[tail_start:n]\n",
        "                if not chunks or chunks[-1][\"content\"] != tail_piece:\n",
        "                    chunks.append({\"content\": tail_piece, \"filename\": d[\"filename\"], \"title\": d[\"title\"]})\n",
        "                break\n",
        "\n",
        "    index = Index(text_fields=[\"content\", \"title\", \"filename\"])\n",
        "    index.fit(chunks)\n",
        "    return index, chunks\n",
        "# -------------------------------------------\n",
        "\n",
        "# --------------- Answering -----------------\n",
        "def make_context(results: List[Dict[str,Any]], topk: int) -> str:\n",
        "    parts = []\n",
        "    for r in results[:topk]:\n",
        "        fn = r.get(\"filename\",\"\")\n",
        "        body = sanitize_md(r.get(\"content\",\"\"), limit=2000)\n",
        "        parts.append(f\"[FILE: {fn}]\\n{body}\")\n",
        "    return \"\\n\\n---\\n\\n\".join(parts) if parts else \"(no results)\"\n",
        "\n",
        "def try_gemini_answer(question: str, context: str) -> tuple[str | None, str | None]:\n",
        "    if genai is None: return None, None\n",
        "    key = os.environ.get(\"GEMINI_API_KEY\", \"\").strip()\n",
        "    if not key: return None, None\n",
        "    try:\n",
        "        genai.configure(api_key=key)\n",
        "    except Exception:\n",
        "        return None, None\n",
        "\n",
        "    system_rules = (\n",
        "        \"Answer ONLY from the provided repo context. \"\n",
        "        \"Cite filenames inline. If not found, reply: 'Not found in repo.'\"\n",
        "    )\n",
        "    prompt = f\"{system_rules}\\n\\n# Context\\n{context}\\n\\n# Q\\n{question}\"\n",
        "\n",
        "    last_err = None\n",
        "    for model_name in MODEL_TRY:\n",
        "        try:\n",
        "            model = genai.GenerativeModel(model_name)\n",
        "            resp = model.generate_content(prompt)\n",
        "            text = (resp.text or \"\").strip()\n",
        "            if text:\n",
        "                return text, model_name\n",
        "        except Exception as e:\n",
        "            last_err = e\n",
        "    return None, None\n",
        "\n",
        "def answer_with_repo(q: str, index: Index, topk: int = 5):\n",
        "    results = index.search(q, num_results=max(1, topk))\n",
        "    if not results:\n",
        "        return \"Not found in repo.\", [], [], \"No results\"\n",
        "\n",
        "    context = make_context(results, topk)\n",
        "    ans, model_used = try_gemini_answer(q, context)\n",
        "    if ans:\n",
        "        used_files = [r.get(\"filename\",\"\") for r in results[:topk]]\n",
        "        return ans, used_files, results, f\"Gemini ({model_used})\"\n",
        "\n",
        "    # Fallback to lexical: show the best snippet\n",
        "    best = results[0]\n",
        "    snippet = sanitize_md(best.get(\"content\",\"\"), limit=800)\n",
        "    used_files = [r.get(\"filename\",\"\") for r in results[:topk]]\n",
        "    return (\n",
        "        f\"From `{best.get('filename','')}`:\\n\\n{snippet}\\n\\n*(LLM unavailable ‚Äî lexical fallback)*\",\n",
        "        used_files, results, \"Lexical\"\n",
        "    )\n",
        "# -------------------------------------------\n",
        "\n",
        "# ===================== UI =====================\n",
        "with st.sidebar:\n",
        "    st.markdown(\"## ‚öôÔ∏è Settings\")\n",
        "    repo_owner = st.text_input(\"Repo owner\", value=DEFAULT_REPO_OWNER)\n",
        "    repo_name  = st.text_input(\"Repo name\",  value=DEFAULT_REPO_NAME)\n",
        "    topk = st.slider(\"Results to use for context\", 3, 10, 6, 1)\n",
        "    gemini_on = bool(os.environ.get(\"GEMINI_API_KEY\",\"\").strip()) and (genai is not None)\n",
        "    st.markdown(f\"Gemini key detected: **{'Yes' if gemini_on else 'No'}**\")\n",
        "\n",
        "st.title(\"ü©∫ DermaScan Repo Assistant\")\n",
        "st.caption(\"Grounded answers from your repository ‚Äî datasets, models, deployment, and more.\")\n",
        "\n",
        "with st.spinner(\"üì• Indexing repo‚Ä¶\"):\n",
        "    index, chunks = ingest_repo(repo_owner, repo_name, TEXT_EXTS, WINDOW, STRIDE)\n",
        "st.success(f\"Indexed {len(chunks)} chunks.\")\n",
        "\n",
        "# Chat history store\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state.messages = []\n",
        "\n",
        "# Render chat history with bubbles\n",
        "for m in st.session_state.messages:\n",
        "    with st.chat_message(m[\"role\"]):\n",
        "        klass = \"question\" if m[\"role\"] == \"user\" else \"answer\"\n",
        "        st.markdown(f\"<div class='bubble {klass}'>{m['content']}</div>\", unsafe_allow_html=True)\n",
        "\n",
        "# Chat input -> ask\n",
        "q = st.chat_input(\"Ask about datasets, models, deployment, files, etc.\")\n",
        "if q:\n",
        "    # USER BUBBLE\n",
        "    st.session_state.messages.append({\"role\":\"user\",\"content\":q})\n",
        "    with st.chat_message(\"user\"):\n",
        "        st.markdown(f\"<div class='bubble question'>{q}</div>\", unsafe_allow_html=True)\n",
        "\n",
        "    # ASSISTANT BUBBLE\n",
        "    with st.chat_message(\"assistant\"):\n",
        "        with st.spinner(\"üîç Searching repo‚Ä¶\"):\n",
        "            answer, used_files, results, model_used = answer_with_repo(q, index=index, topk=topk)\n",
        "\n",
        "            # Sources badges\n",
        "            sources_html = \"\"\n",
        "            if used_files:\n",
        "                badges = \" \".join(badge(human_file(f)) for f in used_files if f)\n",
        "                sources_html = f\"<div class='sources-row'><strong>Sources:</strong> {badges}</div>\"\n",
        "\n",
        "            answer_html = f\"{answer}{sources_html}\"\n",
        "            st.markdown(f\"<div class='bubble answer'>{answer_html}</div>\", unsafe_allow_html=True)\n",
        "\n",
        "    st.session_state.messages.append({\"role\":\"assistant\",\"content\":answer})\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PgwCEIiTkKYl",
        "outputId": "95af45bd-a7fa-4d13-fcbe-0304ab5729f7"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py --server.port 8501 > streamlit.log 2>&1 &\n"
      ],
      "metadata": {
        "id": "bpNTfBsjXlq5"
      },
      "execution_count": 75,
      "outputs": []
    }
  ]
}