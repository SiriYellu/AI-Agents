{"source": "DataTalksClub/faq", "filename": "README.md", "metadata": {}, "content": "# DataTalks.Club FAQ\n\nMore information to come"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/general/001_9e508f2212_course-when-does-the-course-start.md", "metadata": {"id": "9e508f2212", "question": "Course: When does the course start?", "sort_order": 1}, "content": "The next cohort starts January 13th, 2025. More info at [DTC](https://datatalks.club/blog/guide-to-free-online-courses-at-datatalks-club.html).\n\n- Register before the course starts using this [link](https://airtable.com/shr6oVXeQvSI5HuWD).\n- Join the [course Telegram channel with announcements](https://t.me/dezoomcamp).\n- Don’t forget to register in DataTalks.Club's Slack and join the channel."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/general/002_bfafa427b3_course-what-are-the-prerequisites-for-this-course.md", "metadata": {"id": "bfafa427b3", "question": "Course: What are the prerequisites for this course?", "sort_order": 2}, "content": "To get the most out of this course, you should have:\n\n- Basic coding experience\n- Familiarity with SQL\n- Experience with Python (helpful but not required)\n\nNo prior data engineering experience is necessary. See [Readme on GitHub](https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/README.md#prerequisites)."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/general/003_3f1424af17_course-can-i-still-join-the-course-after-the-start.md", "metadata": {"id": "3f1424af17", "question": "Course: Can I still join the course after the start date?", "sort_order": 3}, "content": "Yes, even if you don't register, you're still eligible to submit the homework.\n\nBe aware, however, that there will be deadlines for turning in homeworks and the final projects. So don't leave everything for the last minute."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/general/004_52217fc51b_course-i-have-registered-for-the-data-engineering.md", "metadata": {"id": "52217fc51b", "question": "Course: I have registered for the Data Engineering Bootcamp. When can I expect to receive the confirmation email?", "sort_order": 4}, "content": "You don't need a confirmation email. You're accepted. You can start learning and submitting homework without registering. Registration was just to gauge interest before the start date."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/general/005_33fc260cd8_course-what-can-i-do-before-the-course-starts.md", "metadata": {"id": "33fc260cd8", "question": "Course: What can I do before the course starts?", "sort_order": 5}, "content": "Start by installing and setting up all the dependencies and requirements:\n\n- Google Cloud account\n- Google Cloud SDK\n- Python 3 (installed with Anaconda)\n- Terraform\n- Git\n\nLook over the prerequisites and syllabus to see if you are comfortable with these subjects."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/general/006_b71fb3b195_course-how-many-zoomcamps-in-a-year.md", "metadata": {"id": "b71fb3b195", "question": "Course: how many Zoomcamps in a year?", "sort_order": 6}, "content": "There are multiple Zoomcamps in a year, as of 2025. More info at [DTC Article](https://datatalks.club/blog/guide-to-free-online-courses-at-datatalks-club.html).\n\nThey are five separate courses, estimated to be during these months:\n\n- **Data-Engineering**: Jan - Apr\n- **Stock Market Analytics**: Apr - May\n- **MLOps**: May - Aug\n- **LLM**: June - Sep\n- **Machine Learning**: Sep - Jan\n\nThere's only one Data-Engineering Zoomcamp “live” cohort per year for the certification, similar to the other Zoomcamps. They follow pretty much the same schedule for each cohort. For Data-Engineering, it is generally from Jan-Apr of the year. \n\nIf you’re not interested in the Certificate, you can take any Zoomcamp at any time, at your own pace, out of sync with any “live” cohort."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/general/007_e499535e82_course-is-the-current-cohort-going-to-be-different.md", "metadata": {"id": "e499535e82", "question": "Course: Is the current cohort going to be different from the previous cohort?", "sort_order": 7}, "content": "For the 2025 edition, we are using Kestra (see [Demo](https://www.youtube.com/watch?v=R0JAFvDCmSY)) instead of MageAI (Module 2). Look out for new videos. See [Playlist](https://www.youtube.com/playlist?list=PLEK3H8YwZn1oPPShk2p5k3E9vO-gPnUCf).\n\nFor the 2024 edition, we used Mage AI instead of Prefect and re-recorded the Terraform videos. For 2023, we used Prefect instead of Airflow. See playlists on YouTube and the [cohorts folder in the GitHub repo](https://github.com/DataTalksClub/data-engineering-zoomcamp/tree/main/cohorts)."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/general/008_068529125b_course-can-i-follow-the-course-after-it-finishes.md", "metadata": {"id": "068529125b", "question": "Course - Can I follow the course after it finishes?", "sort_order": 8}, "content": "Yes, we will keep all the materials available, so you can follow the course at your own pace after it finishes.\n\nYou can also continue reviewing the homeworks and prepare for the next cohort. You can also start working on your final capstone project."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/general/009_c207b8614e_course-can-i-get-support-if-i-take-the-course-in-t.md", "metadata": {"id": "c207b8614e", "question": "Course: Can I get support if I take the course in the self-paced mode?", "sort_order": 9}, "content": "Yes, the Slack channel remains open and you can ask questions there. However, always search the channel first and check the FAQ, as most likely your questions are already answered here.\n\nYou can also tag the bot `@ZoomcampQABot` to help you conduct the search, but don’t rely on its answers 100%."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/general/010_8ac65b2225_course-which-playlist-on-youtube-should-i-refer-to.md", "metadata": {"id": "8ac65b2225", "question": "Course: Which playlist on YouTube should I refer to?", "sort_order": 10}, "content": "All the main videos are stored in the \"DATA ENGINEERING ZOOMCAMP\" main playlist. The GitHub repository is updated to include each video with a thumbnail, linking directly to the relevant playlist.\n\nRefer to the Main Playlist for the core content, and then check specific year playlists for additional videos such as office hours.\n\n- [Data Engineering Zoomcamp](https://www.youtube.com/playlist?list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb)\n- [Data Engineering Zoomcamp 2022](https://www.youtube.com/playlist?list=PL3MmuxUbc_hKVX8VnwWCPaWlIHf1qmg8s)\n- [Data Engineering Zoomcamp 2023](https://www.youtube.com/playlist?list=PL3MmuxUbc_hJjEePXIdE-LVUx_1ZZjYGW)\n- Data Engineering Bootcamp 2024\n- [Data Engineering Bootcamp 2025](https://www.youtube.com/playlist?list=PL3MmuxUbc_hJZdpLpRHp7dg6EOx828q6y)\n- [DE Zoomcamp 2025 (Module 2 Kestra)](https://www.youtube.com/playlist?list=PLEK3H8YwZn1oPPShk2p5k3E9vO-gPnUCf)"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/general/011_316180784f_course-how-many-hours-per-week-am-i-expected-to-sp.md", "metadata": {"id": "316180784f", "question": "Course: How many hours per week am I expected to spend on this course?", "sort_order": 11}, "content": "It depends on your background and previous experience with modules. It is expected to require about 5 - 15 hours per week.\n\nYou can also calculate it yourself using [this data](https://github.com/DataTalksClub/zoomcamp-analytics/tree/main/data/de-zoomcamp-2023) and then update this answer."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/general/012_a411de5004_office-hours-i-cant-attend-the-office-hours-worksh.md", "metadata": {"id": "a411de5004", "question": "Office Hours: I can’t attend the “Office hours” / workshop, will it be recorded?", "sort_order": 12}, "content": "Yes! Every \"Office Hours\" will be recorded and available a few minutes after the live session is over; so you can view (or rewatch) whenever you want."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/general/013_16005581f2_edit-course-profile.md", "metadata": {"id": "16005581f2", "question": "Edit Course Profile.", "sort_order": 13}, "content": "The display name listed on the leaderboard is an auto-generated randomized name. You can edit it to be a nickname or your real name if you prefer. Your entry on the Leaderboard is the one highlighted in light green.\n\nThe Certificate name should be your actual name that you want to appear on your certificate after completing the course.\n\nThe \"Display on Leaderboard\" option indicates whether you want your name to be listed on the course leaderboard."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/general/014_3774a79c13_certificate-do-i-need-to-do-the-homeworks-to-get-t.md", "metadata": {"id": "3774a79c13", "question": "Certificate: Do I need to do the homeworks to get the certificate?", "sort_order": 14}, "content": "No, as long as you complete the peer-reviewed capstone projects on time, you can receive the certificate. You do not need to do the homeworks if you join late, for example."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/general/015_900f60fd25_certificate-can-i-follow-the-course-in-a-self-pace.md", "metadata": {"id": "900f60fd25", "question": "Certificate - Can I follow the course in a self-paced mode and get a certificate?", "sort_order": 15}, "content": "No, you can only get a certificate if you finish the course with a “live” cohort. We don't award certificates for the self-paced mode. The reason is you need to peer-review capstone(s) after submitting a project. You can only peer-review projects at the time the course is running."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/general/016_d3f485cd10_homework-what-are-homework-and-project-deadlines.md", "metadata": {"id": "d3f485cd10", "question": "Homework: What are homework and project deadlines?", "sort_order": 16}, "content": "2025 deadlines will be announced on [the course website](https://courses.datatalks.club/de-zoomcamp-2025/) and in [Google Calendar](https://calendar.google.com/calendar/?cid=ZXIxcjA1M3ZlYjJpcXU0dTFmaG02MzVxMG9AZ3JvdXAuY2FsZW5kYXIuZ29vZ2xlLmNvbQ).\n\nYou can find the 2024 deadlines here: [2024 Deadlines Spreadsheet](https://docs.google.com/spreadsheets/d/e/2PACX-1vQACMLuutV5rvXg5qICuJGL-yZqIV0FBD84CxPdC5eZHf8TfzB-CJT_3Mo7U7oGVTXmSihPgQxuuoku/pubhtml).\n\nAlso, take note of announcements from @Au-Tomator for any extensions or other news. The form may also show the updated deadline if the instructor(s) have updated it."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/general/017_4dbd2eea47_homework-are-late-submissions-of-homework-allowed.md", "metadata": {"id": "4dbd2eea47", "question": "Homework: Are late submissions of homework allowed?", "sort_order": 17}, "content": "No, late submissions are not allowed. However, if the form is still open after the due date, you can still submit the homework. Confirm your submission by checking the date-timestamp on the Course page. Ensure you are logged in."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/general/019_9d89b52976_homework-what-is-the-homework-url-in-the-homework.md", "metadata": {"id": "9d89b52976", "images": [{"description": "image #1", "id": "image_1", "path": "images/data-engineering-zoomcamp/image_eaa6f559.png"}], "question": "Homework: What is the homework URL in the homework link?", "sort_order": 19}, "content": "<{IMAGE:image_1}>\n\nAnswer: In short, it’s your repository on GitHub, GitLab, Bitbucket, etc.\n\nIn long, your repository or any other location where you have your code, and a reasonable person would look at it and think, yes, you went through the week and exercises. Think of it like a portfolio you could present to an employer."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/general/020_7255a1c3bc_leaderboard-how-do-find-myself-on-the-leaderboard.md", "metadata": {"id": "7255a1c3bc", "images": [{"description": "image #1", "id": "image_1", "path": "images/data-engineering-zoomcamp/image_813348b4.png"}], "question": "Leaderboard: how do find myself on the leaderboard?", "sort_order": 20}, "content": "When you set up your account, you are automatically assigned a random name, such as \"Lucid Elbakyan.\" If you want to see what your display name is, follow these steps:\n\n- Go to your profile.\n  - 2025: [https://courses.datatalks.club/de-zoomcamp-2025/enrollment](https://courses.datatalks.club/de-zoomcamp-2025/enrollment)\n  - 2024: [https://courses.datatalks.club/de-zoomcamp-2024/enrollment](https://courses.datatalks.club/de-zoomcamp-2025/enrollment)\n- Log in.\n- Your display name is shown. You can also change it if you wish. \n- Ensure your certificate name is correct, as this name will later be printed on your certificate.\n\n<{IMAGE:image_1}>"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/general/021_29e58c5c37_environment-is-python-39-still-the-recommended-ver.md", "metadata": {"id": "29e58c5c37", "question": "Environment: Is Python 3.9 still the recommended version to use in 2024?", "sort_order": 21}, "content": "Yes, for simplicity and stability when troubleshooting against recorded videos.\n\nBut Python 3.10 and 3.11 should work fine."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/general/022_4f1fe161b1_environment-should-i-use-my-local-machine-gcp-or-g.md", "metadata": {"id": "4f1fe161b1", "question": "Environment - Should I use my local machine, GCP, or GitHub Codespaces for my environment?", "sort_order": 22}, "content": "You can set it up on your laptop or PC if you prefer to work locally. However, Windows users might face some challenges.\n\nIf you prefer to work on the local machine, you can start with the Week 1 Introduction to Docker.\n\nAlternatively, if you prefer to set up a virtual machine, consider the following:\n\n- **Using GitHub Codespaces**\n- **Setting up the environment on a cloud VM**: Refer to [this video](https://www.youtube.com/watch?v=ae-CV2KfoN0&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb) for guidance.\n\nWorking on a virtual machine is beneficial if you have different devices for home and office, allowing you to work virtually anywhere."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/general/023_5b4fb0c0a8_environment-is-github-codespaces-an-alternative-to.md", "metadata": {"id": "5b4fb0c0a8", "question": "Environment - Is GitHub codespaces an alternative to using cli/git bash to ingest the data and create a docker file?", "sort_order": 23}, "content": "GitHub Codespaces offers you computing Linux resources with many pre-installed tools (Docker, Docker Compose, Python).\n\nYou can also open any GitHub repository in a GitHub Codespace."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/general/024_070766ca79_environment-do-we-really-have-to-use-github-codesp.md", "metadata": {"id": "070766ca79", "question": "Environment: Do we really have to use GitHub codespaces? I already have PostgreSQL & Docker installed.", "sort_order": 24}, "content": "It's up to you which platform and environment you use for the course.\n\nGitHub Codespaces or GCP VM are just possible options, but you can do the entire course from your laptop."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/general/025_ef96ec09e6_environment-do-i-need-both-github-codespaces-and-g.md", "metadata": {"id": "ef96ec09e6", "question": "Environment - Do I need both GitHub Codespaces and GCP?", "sort_order": 25}, "content": "Choose the approach that aligns the most with your idea for the end project.\n\nOne should suffice; however, BigQuery, which is part of GCP, will be used, so learning that is probably a better option. Alternatively, you can set up a local environment for most of this course."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/general/026_b92da7c113_environment-could-not-establish-connection-to-myse.md", "metadata": {"id": "b92da7c113", "question": "Environment - Could not establish connection to \"MyServerName\": Got bad result from install script", "sort_order": 26}, "content": "This issue occurs when attempting to connect to a GCP VM using VSCode on a Windows machine. You can resolve it by changing a registry value in the registry editor.\n\nOpen the Run command window:\n- Use the shortcut keys `Windows + R`, or\n- Right-click \"Start\" and click \"Run\".\n\nOpen the Registry Editor:\n- Type `regedit` in the Run command window, then press Enter.\n\nChange the registry value:\n- Navigate to `HKEY_CURRENT_USER\\Software\\Microsoft\\Command Processor`.\n- Change the \"Autorun\" value from \"if exists\" to a blank.\n\nAlternatively, you can delete the saved fingerprint within the known_hosts file:\n\nIn Windows, locate the file at `C:\\Users\\<your_user_name>\\.ssh\\known_hosts` and remove the entry for the server."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/general/027_a8219681ec_environment-why-are-we-using-gcp-and-not-other-clo.md", "metadata": {"id": "a8219681ec", "question": "Environment - Why are we using GCP and not other cloud providers?", "sort_order": 27}, "content": "For uniformity.\n\nYou can use other cloud platforms since you get every service provided by GCP in Azure and AWS. You’re not restricted to GCP and can use other platforms like AWS if you’re more comfortable.\n\nBecause everyone usually has a Google account, GCP offers a free trial period with $300 in credits for new users. Additionally, we are working with BigQuery, which is part of GCP.\n\nNote that to sign up for a free GCP account, you must have a valid credit card."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/general/028_e7738f47c8_should-i-pay-for-cloud-services.md", "metadata": {"id": "e7738f47c8", "question": "Should I pay for cloud services?", "sort_order": 28}, "content": "It's not mandatory. You can take advantage of their free trial."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/general/029_77a076baa3_environment-the-gcp-and-other-cloud-providers-are.md", "metadata": {"id": "77a076baa3", "question": "Environment: The GCP and other cloud providers are unavailable in some countries. Is it possible to provide a guide to installing a home lab?", "sort_order": 29}, "content": "You can do most of the course without a cloud. Almost everything we use (excluding BigQuery) can be run locally. We won’t be able to provide guidelines for some things, but most of the materials are runnable without GCP.\n\nFor everything in the course, there’s a local alternative. You could even do the whole course locally. Note that Homework 3 requires BigQuery."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/general/030_109e36c115_environment-is-gcp-sandbox-enough-or-we-need-the-f.md", "metadata": {"id": "109e36c115", "question": "Environment: Is GCP Sandbox enough or we need the Free Trial?", "sort_order": 30}, "content": "Google Cloud Platform (GCP) provides two free trial options: the Free Trial with $300 credit and the Sandbox. Users can switch between these options by managing billing details.\n\nHowever, completing the course solely using the GCP Sandbox option is not feasible due to its limited features. The Sandbox lacks some services required for the course, such as VMs, GCS Buckets, and other paid services that are integral to the curriculum.\n\nThe course will eventually require utilizing the following:\n\n- **VMs and GCS Buckets**: These resources are not fully available in the Sandbox.\n- **BigQuery**: A key component of GCP, and the Sandbox may not support all necessary functionalities.\n\nTherefore, it is recommended to use the GCP Free Trial with billing details to access all needed features and ensure a smooth learning experience."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/general/031_21c45a3556_environment-i-want-to-use-aws-may-i-do-that.md", "metadata": {"id": "21c45a3556", "question": "Environment: I want to use AWS. May I do that?", "sort_order": 31}, "content": "Yes, you can. Just remember to adapt all the information from the videos to AWS. Additionally, the final capstone will be evaluated based on these tasks:\n\n- Create a data pipeline\n- Develop a visualization\n\nConsider that when seeking help, you might need to rely on fellow coursemates who use AWS, which could be fewer compared to those using GCP.\n\nAlso, see [\"Is it possible to use x tool instead of the one tool you use?\"](#4dec1f8407)"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/general/032_99bb0ceeb6_besides-the-office-hour-which-are-the-live-zoom-ca.md", "metadata": {"id": "99bb0ceeb6", "question": "Besides the “Office Hour” which are the live zoom calls?", "sort_order": 32}, "content": "We will probably have some calls during the Capstone period to clear some questions, but it will be announced in advance if that happens.\n\nSee [Google Calendar](https://calendar.google.com/calendar/?cid=ZXIxcjA1M3ZlYjJpcXU0dTFmaG02MzVxMG9AZ3JvdXAuY2FsZW5kYXIuZ29vZ2xlLmNvbQ)"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/general/033_31e9251331_can-i-use-airflow-instead-for-my-final-project.md", "metadata": {"id": "31e9251331", "question": "Can I use Airflow instead for my final project?", "sort_order": 33}, "content": "Yes, you can use any tool you want for your project."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/general/034_4dec1f8407_is-it-possible-to-use-tool-x-instead-of-the-one-to.md", "metadata": {"id": "4dec1f8407", "question": "Is it possible to use tool “X” instead of the one tool you use in the course?", "sort_order": 34}, "content": "Yes, this applies if you want to use Airflow or Prefect instead of Mage, AWS or Snowflake instead of GCP products, or Tableau instead of Metabase or Google Data Studio.\n\nThe course covers two alternative data stacks: one using GCP and one using local installation of everything. You can use one of them or use your tool of choice.\n\nConsiderations:\n- We can’t support you if you choose to use a different stack.\n- You would need to explain the different choices of tools for the peer review of your capstone project."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/general/035_721f9e0c29_how-can-we-contribute-to-the-course.md", "metadata": {"id": "721f9e0c29", "question": "How can we contribute to the course?", "sort_order": 35}, "content": "- [Star the repository](https://github.com/DataTalksClub/data-engineering-zoomcamp).\n- Share it with friends if you find it useful.\n- Create a pull request (PR) if you can improve the text or structure of the repository.\n- [Update this FAQ](https://github.com/DataTalksClub/faq/)."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/general/036_b7542b8d36_environment-is-the-course-windowsmacoslinux-friend.md", "metadata": {"id": "b7542b8d36", "question": "Environment: Is the course [Windows/macOS/Linux/...] friendly?", "sort_order": 36}, "content": "Yes! Linux is ideal but technically it should not matter. Students in the 2024 cohort used all 3 OSes successfully."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/general/037_8b0214d089_environment-roadblock-for-windows-users-in-modules.md", "metadata": {"id": "8b0214d089", "question": "Environment: Roadblock for Windows users in modules with *.sh (shell scripts)", "sort_order": 37}, "content": "Later modules (module-05 & RisingWave workshop) use shell scripts in *.sh files. Most Windows users not using WSL will encounter issues and may not be able to continue, even in Git Bash or MINGW64. It is recommended to set up a WSL environment from the start."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/general/038_86251abcdf_any-books-or-additional-resources-you-recommend.md", "metadata": {"id": "86251abcdf", "question": "Any books or additional resources you recommend?", "sort_order": 38}, "content": "Yes to both! Check out this document: [Awesome Data Engineering Resources](https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/awesome-data-engineering.md)"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/general/039_9b0561dbf8_project-what-is-project-attempt-1-and-project-atte.md", "metadata": {"id": "9b0561dbf8", "question": "Project: What is Project Attempt #1 and Project Attempt #2 exactly?", "sort_order": 39}, "content": "You will have two attempts for a project.\n\n- If the first project deadline is over and you’re late, or you submit the project and fail the first attempt, you have another chance to submit the project with the second attempt."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/general/040_a83f047f52_how-to-troubleshoot-issues.md", "metadata": {"id": "a83f047f52", "question": "How to troubleshoot issues", "sort_order": 40}, "content": "**First Steps:**\n\n- Attempt to solve the issue independently. Familiarize yourself with documentation as a crucial skill for problem-solving.\n- Use shortcuts like `[ctrl+f]` to search within documents and browsers.\n- Analyze the error message for descriptions, instructions, and possible solutions.\n- Restart your application, server, or computer as needed.\n  \n**Search for Solutions:**\n\n- Use search engines like Google, ChatGPT, or Bing AI to research the issue. It is rare to encounter a unique problem.\n- Form your search queries using: `<technology> <problem statement>`. E.g., `pgcli error column c.relhasoids does not exist.`\n- Consult the technology’s official documentation for guidance.\n\n**Uninstallation and Reinstallation:**\n\n- Uninstall and then reinstall the application if needed, including a system restart.\n- Note that reinstalling without prior uninstallation might not resolve the issue.\n\n**Seeking Help:**\n\n- Post questions on platforms like StackOverflow. Ensure your question adheres to guidelines: [how-to-ask](https://stackoverflow.com/help/how-to-ask).\n- Consider asking experts or colleagues in the future.\n\n**Community Resources:**\n\n- Check Slack channels for pinned messages and use its search function.\n- Refer to this FAQ using search `[ctrl+f]` or utilize the `@ZoomcampQABot` for assistance.\n\n**When Asking for Help:**\n\n- Provide detailed information: coding environment, OS, commands, videos followed, etc.\n- Share errors received, with specifics, including line numbers and actions taken.\n- Avoid screenshots; paste code or errors directly. Use ``` for code formatting.\n- Maintain thread consistency; respond in the same thread instead of creating multiple ones.\n\n**Re-evaluation:**\n\n- If the issue recurs, create a new post detailing changes in the environment.\n- Communicate additional troubleshooting steps in the same thread.\n- Occasionally take a break to gain a fresh perspective on the problem.\n\n**Documentation Contribution:**\n\n- If your problem solution is not listed, consider adding it to the FAQ to assist others."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/general/041_4eba13edb1_how-to-ask-questions.md", "metadata": {"id": "4eba13edb1", "question": "How to ask questions", "sort_order": 41}, "content": "When the troubleshooting guide does not help resolve your issue and you need another pair of eyes, include as much information as possible when asking a question:\n\n- What are you coding on? What operating system are you using?\n- What command did you run, and which video or tutorial did you follow?\n- What error did you get? Does it have a line number pointing to the problematic code, and have you checked it for typos?\n- What have you tried that did not work? This is crucial because, without it, helpers might suggest steps mentioned in the error log first. Or just refer to this FAQ document."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/general/042_dc06a38bc6_how-do-i-use-git-github-for-this-course.md", "metadata": {"id": "dc06a38bc6", "question": "How do I use Git / GitHub for this course?", "sort_order": 42}, "content": "After you create a GitHub account, clone the course repo to your local machine using the process outlined in this video:\n\n[Git for Everybody: How to Clone a Repository from GitHub](https://www.youtube.com/watch?v=CKcqniGu3tA).\n\nHaving this local repository on your computer will make it easy to access the instructors’ code and make pull requests if you want to add your own notes or make changes to the course content.\n\nYou will probably also create your own repositories to host your notes and versions of files. Here is a great tutorial that shows you how to do this:\n\n[How to Create a Git Repository](https://www.atlassian.com/git/tutorials/setting-up-a-repository).\n\nRemember to ignore large databases, .csv, and .gz files, and other files that should not be saved to a repository. Use `.gitignore` for this:\n\n[.gitignore file](https://www.atlassian.com/git/tutorials/saving-changes/gitignore).\n\n**Important:**\n\n**NEVER store passwords or keys in a git repo** (even if the repo is set to private). Put files containing sensitive information (.env, secret.json, etc.) in your `.gitignore`.\n\nThis is also a great resource: [Dangit, Git!?!](https://dangitgit.com/)"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/general/043_e5dc51eac9_vs-code-tab-using-spaces.md", "metadata": {"id": "e5dc51eac9", "question": "VS Code: Tab using spaces", "sort_order": 43}, "content": "Error:\n\n```\nMakefile:2: *** missing separator.  Stop.\n```\n\nSolution:\n\nTabs in documents should be converted to Tab instead of spaces. [Follow this stack](https://stackoverflow.com/questions/36814642/visual-studio-code-convert-spaces-to-tabs)."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/general/044_5b54567e89_opening-an-html-file-with-a-windows-browser-from-l.md", "metadata": {"id": "5b54567e89", "question": "Opening an HTML file with a Windows browser from Linux running on WSL", "sort_order": 44}, "content": "If you’re running Linux on Windows Subsystem for Linux (WSL) 2, you can open HTML files from the guest (Linux) with any Internet Browser installed on the host (Windows). Just install [wslu](https://wslutiliti.es/wslu/install.html) and open the page using `wslview`:\n\n```bash\nwslview index.html\n```\n\nYou can customize which browser to use by setting the `BROWSER` environment variable first. For example:\n\n```bash\nexport BROWSER='/mnt/c/Program Files/Firefox/firefox.exe'\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/general/045_1b03f3dadf_set-up-chrome-remote-desktop-for-linux-on-compute.md", "metadata": {"id": "1b03f3dadf", "question": "Set up Chrome Remote Desktop for Linux on Compute Engine", "sort_order": 45}, "content": "This [tutorial](https://cloud.google.com/architecture/chrome-desktop-remote-on-compute-engine?hl=en) shows you how to set up the Chrome Remote Desktop service on a Debian Linux virtual machine (VM) instance on Compute Engine. Chrome Remote Desktop allows you to remotely access applications with a graphical user interface."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/general/046_6314bc3029_how-do-i-get-my-certificate.md", "metadata": {"id": "6314bc3029", "images": [{"description": "image #1", "id": "image_1", "path": "images/data-engineering-zoomcamp/image_1813f02b.png"}], "question": "How do I get my certificate?", "sort_order": 46}, "content": "There'll be an announcement in Telegram and the course channel for:\n\n- Checking that your full name is displayed correctly on the Certificate (see Editing course profile on the Course Management webpage).\n- Notifying when the grading is completed.\n\nYou will find it in your course profile (you need to be\nlogged it). \n\nFor 2025 the link to the course profile is this:\n\n`https://courses.datatalks.club/de-zoomcamp-2025/enrollment`\n\nFor other editions, change \"2025\" to your edition.\n\nAfter the second announcement, follow instructions in [certificates.md](https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/certificates.md) on how to generate the Certificate document yourself."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/general/05727a95dd_homework-and-leaderboard-wha.md", "metadata": {"id": "05727a95dd", "question": "Homework and Leaderboard: What is the system for points in the course management", "sort_order": 18}, "content": "After you submit your homework, it will be graded based on the number of questions in that particular assignment. You can see the number of points you have earned at the top of the homework page. Additionally, in the [leaderboard](https://courses.datatalks.club/de-zoomcamp-2025/leaderboard), you will find the sum of all points you've earned: points for Homeworks, FAQs, and Learning in Public.\n\nPoint System Overview:\n\n- **Homework:** Points vary by assignment based on the number of questions.\n- **FAQ Contribution:** You get a maximum of 1 point for contributing to the FAQ in the respective week.\n- **Learning in Public:** For each learning in public link, you earn one point. You can achieve a maximum of 7 points.\n\nCheck this [video](https://www.loom.com/share/710e3297487b409d94df0e8da1c984ce) for more details."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/001_6affd2987c_taxi-data-yellow-taxi-trip-records-downloading-err.md", "metadata": {"id": "6affd2987c", "images": [{"description": "image #1", "id": "image_1", "path": "images/data-engineering-zoomcamp/image_1813f02b.png"}], "question": "Taxi Data: Yellow Taxi Trip Records downloading error", "sort_order": 1}, "content": "When attempting to download the 2021 data from the [TLC website](https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page), you may encounter the following error:\n\n```bash\nERROR 403: Forbidden\n```\n\n<{IMAGE:image_1}>\n\nWe have a backup, so use it instead: [nyc-tlc-data](https://github.com/DataTalksClub/nyc-tlc-data)\n\nSo the link should be [yellow_tripdata_2021-01.csv.gz](https://github.com/DataTalksClub/nyc-tlc-data/releases/download/yellow/yellow_tripdata_2021-01.csv.gz).\n\n**Note:** Make sure to [unzip the \"gz\" file](https://linuxize.com/post/how-to-unzip-gz-file/) (no, the \"unzip\" command won’t work for this)."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/002_d677df9ccb_taxi-data-how-to-handle-csvgz-taxi-data-files.md", "metadata": {"id": "d677df9ccb", "question": "Taxi Data: How to handle *.csv.gz taxi data files?", "sort_order": 2}, "content": "In [this video](https://www.youtube.com/watch?v=B1WwATwf-vY&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb), the data file is stored as `output.csv`. If the file extension is `csv.gz` instead of `csv`, it won't store correctly.\n\nTo handle this:\n\n1. Replace `csv_name = \"output.csv\"` with the file name extracted from the URL. For example, for the yellow taxi data, use:\n   \n   ```python\n   url = \"https://github.com/DataTalksClub/nyc-tlc-data/releases/download/yellow/yellow_tripdata_2021-01.csv.gz\"\n   csv_name = url.split(\"/\")[-1]\n   ```\n\n2. When you use `csv_name` with `pandas.read_csv`, it will work correctly because `pandas.read_csv` can directly read files with the `csv.gz` extension.\n\nExample:\n\n```python\nimport pandas as pd\n\nurl = \"https://github.com/DataTalksClub/nyc-tlc-data/releases/download/yellow/yellow_tripdata_2021-01.csv.gz\"\ncsv_name = url.split(\"/\")[-1]\n\ndata = pd.read_csv(csv_name)\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/003_4ee5e16952_taxi-data-data-dictionary-for-ny-taxi-data.md", "metadata": {"id": "4ee5e16952", "question": "Taxi Data: Data Dictionary for NY Taxi data?", "sort_order": 3}, "content": "Yellow Trips: [Data Dictionary](https://www1.nyc.gov/assets/tlc/downloads/pdf/data_dictionary_trip_records_yellow.pdf)\n\nGreen Trips: [Data Dictionary - LPEP Trip Records May 1, 2018](https://www1.nyc.gov/assets/tlc/downloads/pdf/data_dictionary_trip_records_green.pdf)"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/004_8585d2d7f4_taxi-data-unzip-parquet-file.md", "metadata": {"id": "8585d2d7f4", "question": "Taxi Data: Unzip Parquet file", "sort_order": 4}, "content": "You can unzip the downloaded parquet file from the command line. The result is a CSV file which can be imported with pandas using `pd.read_csv()` as shown in the videos.\n\n```bash\ngunzip green_tripdata_2019-09.csv.gz\n```\n\n### Solution for Using Parquet Files Directly in Python Script `ingest_data.py`\n\n1. In the `def main(params)`, add this line:\n   \n   ```python\n   parquet_name = 'output.parquet'\n   ```\n\n2. Edit the code which downloads the files:\n\n   ```python\n   os.system(f\"wget {url} -O {parquet_name}\")\n   ```\n\n3. Convert the downloaded `.parquet` file to CSV and rename it to `csv_name` to keep it relevant to the rest of the code:\n\n   ```python\n   df = pd.read_parquet(parquet_name)\n   df.to_csv(csv_name, index=False)\n   ```"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/005_b3030c88e7_wget-is-not-recognized-as-an-internal-or-external.md", "metadata": {"id": "b3030c88e7", "question": "wget is not recognized as an internal or external command", "sort_order": 5}, "content": "If you encounter the error \"wget is not recognized as an internal or external command,\" wget needs to be installed.\n\nThis error may also cause messages like \"No such file or directory: 'output.csv.gz'.\"\n\n### Installation Instructions:\n\n- **On Ubuntu:**\n\n  ```bash\n  sudo apt-get install wget\n  ```\n\n- **On macOS:**\n\n  Use [Homebrew](https://brew.sh/):\n\n  ```bash\n  brew install wget\n  ```\n\n- **On Windows:**\n\n  Use [Chocolatey](https://chocolatey.org/):\n\n  ```bash\n  choco install wget\n  ```\n\n  Alternatively, download a binary from [GnuWin32](https://gnuwin32.sourceforge.net/packages/wget.htm) and place it in a location that is in your PATH (e.g., `C:/tools/`).\n\n#### Alternative Windows Installation:\n\n1. Download the latest wget binary for Windows from [eternallybored](https://eternallybored.org/misc/wget/).\n2. If you downloaded the zip, extract all files (use [7-zip](https://7-zip.org/) if the built-in utility gives an error).\n3. Rename the file `wget64.exe` to `wget.exe` if necessary.\n4. Move `wget.exe` to your `Git\\mingw64\\bin\\` directory.\n\n#### Python Alternative:\n\n- Use the Python wget library:\n\n  First, install using pip:\n\n  ```bash\n  pip install wget\n  ```\n\n- Use it with Python:\n\n  ```bash\n  python -m wget\n  ```\n\nYou can also paste the file URL into your web browser to download normally, then move the file to your working directory.\n\n### Additional Recommendation:\n\nConsider using the Python library [requests](https://pypi.org/project/requests) for loading gz files."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/006_542abbcb6a_wget-error-cannot-verify-website-certificate-macos.md", "metadata": {"id": "542abbcb6a", "question": "wget - ERROR: cannot verify <website> certificate  (MacOS)", "sort_order": 6}, "content": "Firstly, make sure that you add `!` before `wget` if you’re running your command in a Jupyter Notebook or CLI. Then, you can check one of these two things (from CLI):\n\n- **Using the Python library wget installed with pip:**\n\n  ```bash\n  python -m wget <url>\n  ```\n\n- **Use the usual command and add `--no-check-certificate` at the end:**\n\n  ```bash\n  !wget <website_url> --no-check-certificate\n  ```"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/007_1ba19ed6a0_git-bash-backslash-as-an-escape-character-in-git-b.md", "metadata": {"id": "1ba19ed6a0", "question": "Git Bash: Backslash as an escape character in Git Bash for Windows", "sort_order": 7}, "content": "For those who wish to use the backslash as an escape character in Git Bash for Windows, type the following in the terminal:\n\n```bash\nbash.escapeChar=\\\n```\n\n(Note: There is no need to include this in your `.bashrc` file.)"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/008_994ef10e79_github-codespaces-how-to-store-secrets.md", "metadata": {"id": "994ef10e79", "question": "GitHub Codespaces: How to store secrets", "sort_order": 8}, "content": "Instruction on how to store secrets that will be available in GitHub Codespaces. See [Managing your account-specific secrets for GitHub Codespaces - GitHub Docs](https://docs.github.com/en/codespaces/managing-your-codespaces/managing-your-account-specific-secrets-for-github-codespaces#about-secrets-for-github-codespaces)."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/009_3d24f7796d_github-codespaces-running-pgadmin-in-docker.md", "metadata": {"id": "3d24f7796d", "question": "GitHub Codespaces: Running pgadmin in Docker", "sort_order": 9}, "content": "With the default instructions, running pgadmin in Docker may result in a blank screen after logging into the pgadmin console. To resolve this, add the following two environment variables to your pgadmin configuration to allow it to work with Codespaces’ reverse proxy:\n\n```plaintext\nPGADMIN_CONFIG_PROXY_X_HOST_COUNT: 1\nPGADMIN_CONFIG_PROXY_X_PREFIX_COUNT: 1\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/010_5e6c4090af_docker-cannot-connect-to-docker-daemon-at-unixvarr.md", "metadata": {"id": "5e6c4090af", "question": "Docker: Cannot connect to Docker daemon at unix:///var/run/docker.sock. Is the Docker daemon running?", "sort_order": 10}, "content": "Make sure you're able to start the Docker daemon. Check the issue immediately as described below:\n\n- Ensure the Docker daemon is running.\n- Update WSL in PowerShell with the following command:\n\n  ```bash\n  wsl --update\n  ```"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/011_46dbe4810d_docker-error-during-connect-in-the-default-daemon.md", "metadata": {"id": "46dbe4810d", "question": "Docker - error during connect: In the default daemon configuration on Windows, the docker client must be run with elevated privileges", "sort_order": 11}, "content": "If you get this error:\n\n```\ndocker: error during connect: In the default daemon configuration on Windows, the docker client must be run with elevated privileges to connect.: Post \"http://%2F%2F.%2Fpipe%2Fdocker_engine/v1.24/containers/create\": open //./pipe/docker_engine: The system cannot find the file specified.\nSee 'docker run --help'.\n```\n\nTo resolve it on Windows, follow these guidelines based on your version:\n\n**Windows 10 Pro / 11 Pro Users**:\n\n* Ensure Hyper-V is enabled, as Docker can use it as a backend.\n* Follow the [Enable Hyper-V Option on Windows 10 / 11](https://www.c-sharpcorner.com/article/install-and-configured-docker-desktop-in-windows-10/) tutorial.\n\n**Windows 10 Home / 11 Home Users**:\n\n* The 'Home' version doesn't support Hyper-V, so use WSL2 (Windows Subsystem for Linux).\n* Refer to [install WSL on Windows 11](https://pureinfotech.com/install-wsl-windows-11/) for detailed instructions.\n\nIf you encounter the \"WslRegisterDistribution failed with error: 0x800701bc\" error:\n\n- Update the WSL2 Linux Kernel by following the guidelines at [GitHub: WSL Issue 5393](https://github.com/microsoft/WSL/issues/5393)."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/012_bceb4aa421_docker-docker-pull-dbpage.md", "metadata": {"id": "bceb4aa421", "question": "Docker: docker pull dbpage", "sort_order": 12}, "content": "Whenever a `docker pull` is performed (either manually or by `docker-compose up`), it attempts to fetch the given image name from a repository. If the repository is public, the fetch and download occur without any issues.\n\nFor instance:\n\n```bash\ndocker pull postgres:13\ndocker pull dpage/pgadmin4\n```\n\n**Be Advised:** The Docker images we'll be using throughout the Data Engineering Zoomcamp are all public, unless otherwise specified. This means you are not required to perform a `docker login` to fetch them.\n\nIf you encounter the message:\n\n```\ndocker login': denied: requested access to the resource is denied.\n```\n\nThis is likely due to a typo in your image name. For instance:\n\n```bash\n$ docker pull dbpage/pgadmin4\n```\n\nThis command will throw an exception:\n\n```\nError response from daemon: pull access denied for dbpage/pgadmin4, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\n```\n\nThis occurs because the actual image name is `dpage/pgadmin4`, not `dbpage/pgadmin4`.\n\n**How to fix it:**\n\n```bash\n$ docker pull dpage/pgadmin4\n```\n\n**Extra Notes:** In some professional environments, the Docker image may be in a private repository that your DockerHub username has access to. In this case, you must:\n\n1. Execute:\n   ```bash\n   $ docker login\n   ```\n2. Enter your username and password.\n3. Then perform the `docker pull` against that private repository."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/013_0beb2b5df7_docker-permission-denied-error-when-creating-a-pos.md", "metadata": {"id": "0beb2b5df7", "question": "Docker: \"permission denied\" error when creating a PostgreSQL Docker with a mounted volume on macOS M1", "sort_order": 13}, "content": "When attempting to run a Docker command similar to the one below:\n\n```bash\ndocker run -it \\\n  -e POSTGRES_USER=\"root\" \\\n  -e POSTGRES_PASSWORD=\"root\" \\\n  -e POSTGRES_DB=\"ny_taxi\" \\\n  -v $(pwd)/ny_taxi_postgres_data:/var/lib/postgresql/data \\\n  -p 5432:5432 \\\n  postgres:13\n```\n\nYou encounter the error message:\n\n```\ndocker: Error response from daemon: error while creating mount source path '/path/to/ny_taxi_postgres_data': chown /path/to/ny_taxi_postgres_data: permission denied.\n```\n\nSolution\n\n1. **Stop Rancher Desktop:**  \n   If you are using Rancher Desktop and face this issue, stop Rancher Desktop to resolve compatibility problems.\n\n2. **Install Docker Desktop:**  \n   Install Docker Desktop, ensuring that it is properly configured and has the required permissions.\n\n3. **Retry Docker Command:**  \n   Run the Docker command again after switching to Docker Desktop. This step resolves compatibility issues on some systems.\n\n**Note:** The issue occurred because Rancher Desktop was in use. Switching to Docker Desktop resolves compatibility problems and allows for the successful creation of PostgreSQL containers with mounted volumes for the New York Taxi Database on macOS M1."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/014_1ae137c022_docker-cant-delete-local-folder-that-mounted-to-do.md", "metadata": {"id": "1ae137c022", "question": "Docker: can’t delete local folder that mounted to docker volume", "sort_order": 14}, "content": "When a PostgreSQL Docker container is created, it may create a folder on the local machine to mount to a volume inside the container. This folder is often owned by user 999 and has read and write protection, preventing deletion by conventional means such as dragging it to the trash.\n\nIf you encounter an access error or need to delete the folder, you can use the following command:\n\n```bash\nsudo rm -r -f docker_test/\n```\n\n- `rm` : Command to remove files or directories.\n- `-r` : Recursively remove directories and their contents.\n- `-f` : Forcefully remove files/directories without prompting.\n- `docker_test/` : The folder to be deleted."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/015_docker-docker-wont-start-or.md", "metadata": {"id": "3549528659", "question": "Docker: Docker won't start or is stuck in settings (Windows 10 / 11)", "sort_order": 15}, "content": "Ensure you are running the latest version of Docker for Windows. Download the updated version from [Docker's official site](https://docs.docker.com/desktop/install/windows-install/). If the upgrade option in the menu doesn't work, uninstall and reinstall with the latest version.\n\nIf Docker is stuck on starting, try switching the containers by right-clicking the [docker symbol](https://imgur.com/vsVUAzK) from the running programs, and switch the containers from Windows to Linux or vice versa.\n\nFor Windows 10 / 11 Pro Edition:\n\n- **Hyper-V Backend:** ensure Hyper-V is enabled by following this [tutorial](https://www.c-sharpcorner.com/article/install-and-configured-docker-desktop-in-windows-10/).\n- **WSL2 Backend:** follow the steps detailed in this [tutorial](https://pureinfotech.com/install-wsl-windows-11/)."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/016_c7e1100613_docker-should-i-run-docker-commands-from-the-windo.md", "metadata": {"id": "c7e1100613", "question": "Docker: Should I run docker commands from the windows file system or a file system of a Linux distribution in WSL?", "sort_order": 16}, "content": "If you're running a Home Edition, you can still make it work with WSL2 (Windows Subsystem for Linux) by following the [tutorial here](https://pureinfotech.com/install-wsl-windows-11/).\n\nIf even after making sure your WSL2 (or Hyper-V) is set up accordingly, Docker remains stuck, you can try the following options:\n\n- [Reset to Factory Defaults](https://imgur.com/CfESyNt)\n- Perform a fresh install."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/017_ed8dcfbb5a_docker-the-input-device-is-not-a-tty-docker-run-fo.md", "metadata": {"id": "ed8dcfbb5a", "question": "Docker: The input device is not a TTY (Docker run for Windows)", "sort_order": 17}, "content": "You may encounter this error:\n\n```bash\n$ docker run -it ubuntu bash\nthe input device is not a TTY. If you are using mintty, try prefixing the command with 'winpty'\n```\n\nSolution:\n\n- Use `winpty` before the Docker command:\n  \n  ```bash\n  $ winpty docker run -it ubuntu bash\n  ```\n\n- Alternatively, create an alias:\n  \n  ```bash\n  echo \"alias docker='winpty docker'\" >> ~/.bashrc\n  ```\n  \n  or\n  \n  ```bash\n  echo \"alias docker='winpty docker'\" >> ~/.bash_profile\n  ```\n  \nSource: [Stack Overflow](https://stackoverflow.com/a/49965690)"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/018_4c8959502e_docker-cannot-pip-install-on-docker-container-wind.md", "metadata": {"id": "4c8959502e", "question": "Docker: Cannot pip install on Docker container (Windows)", "sort_order": 18}, "content": "You may encounter this error:\n\n```\nRetrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7efe331cf790>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/pandas/\n```\n\nPossible solution:\n\nRun the following command:\n\n```bash\nwinpty docker run -it --dns=8.8.8.8 --entrypoint=bash python:3.9\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/019_87e92edf02_docker-ny_taxi_postgres_data-is-empty.md", "metadata": {"id": "87e92edf02", "question": "Docker: ny_taxi_postgres_data is empty", "sort_order": 19}, "content": "Even after properly running the Docker script, the folder may appear empty in VS Code. For Windows, try the following steps:\n\n**Solution 1:**\n\nRun the Docker command with the absolute path quoted in the `-v` parameter:\n\n```bash\nwinpty docker run -it \\\n  -e POSTGRES_USER=\"root\" \\\n  -e POSTGRES_PASSWORD=\"root\" \\\n  -e POSTGRES_DB=\"ny_taxi\" \\\n  -v \"C:\\Users\\abhin\\dataengg\\DE_Project_git_connected\\DE_OLD\\week1_set_up\\docker_sql/ny_taxi_postgres_data:/var/lib/postgresql/data\" \\\n  -p 5432:5432 \\\n  postgres:13\n```\n\nThis should resolve the visibility issue in the VS Code `ny_taxi` folder.\n\n**Note:** Ensure the correct direction for the slashes: `/` versus `\\`.\n\n**Solution 2:**\n\nAnother possible solution for Windows is to finish the folder path with a forward slash `/`:\n\n```bash\ndocker run -it \\\n  -e POSTGRES_USER=\"root\" \\\n  -e POSTGRES_PASSWORD=\"root\" \\\n  -e POSTGRES_DB=\"ny_taxi\" \\\n  -v /\"$(pwd)\"/ny_taxi_postgres_data/:/var/lib/postgresql/data/ \\\n  -p 5432:5432 \\\n  postgres:13\n```\n\nThese steps should help resolve the issue of the `ny_taxi_postgres_data` folder appearing empty in your Docker setup."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/020_b2eabcd7dc_docker-setting-up-docker-on-mac.md", "metadata": {"id": "b2eabcd7dc", "question": "Docker: Setting up Docker on Mac", "sort_order": 20}, "content": "For setting up Docker on macOS, you have two main options:\n\n1. **Download from Docker Website:**\n   - Visit the official Docker website and download the Docker Desktop for Mac as a `.dmg` file. This method is generally reliable and avoids issues related to licensing changes.\n\n2. **Using Homebrew:**\n   - Be aware that there can be conflicts when installing with Homebrew, especially between Docker Desktop and command-line tools. To avoid issues:\n     \n     - Install Docker Desktop first.\n     - Then install the command line tools.\n\n   - Commands:\n     \n     ```bash\n     brew install --cask docker\n     ```\n     \n     ```bash\n     brew install docker docker-compose\n     ```\n\n   - For more detailed issues related to `brew install`, refer to this [Issue](https://github.com/Homebrew/brew/issues/16309). \n\nFor more details, you can check the article on [Setting up Docker in macOS](https://medium.com/@vivekslair/setting-up-docker-in-macos-ee36d37b3be2)."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/021_ef201a0b0b_docker-could-not-change-permissions-of-directory-v.md", "metadata": {"id": "ef201a0b0b", "question": "Docker - Could not change permissions of directory \"/var/lib/postgresql/data\": Operation not permitted", "sort_order": 21}, "content": "```bash\ndocker run -it \\\n  -e POSTGRES_USER=\"root\" \\\n  -e POSTGRES_PASSWORD=\"admin\" \\\n  -e POSTGRES_DB=\"ny_taxi\" \\\n  -v \"/mnt/path/to/ny_taxi_postgres_data\":\"/var/lib/postgresql/data\" \\\n  -p 5432:5432 \\\n  postgres:13\n```\n\n**Error Message:**\n\n```plaintext\nThe files belonging to this database system will be owned by user \"postgres\". \nThe database cluster will be initialized with locale \"en_US.utf8\". \nThe default database encoding has accordingly been set to \"UTF8\". \nData page checksums are disabled.\nfixing permissions on existing directory /var/lib/postgresql/data ...\ninitdb: error: could not change permissions of directory \"/var/lib/postgresql/data\": Operation not permitted\n```\n\n**Solution:**\n\n1. Create a local Docker volume and map it to the Postgres data directory `/var/lib/postgresql/data`.\n   \n   - The volume name `dtc_postgres_volume_local` must match in both commands below:\n\n    ```bash\n    docker volume create --name dtc_postgres_volume_local -d local\n    ```\n\n2. Run the Docker container using the created volume:\n\n    ```bash\n    docker run -it \\\n      -e POSTGRES_USER=\"root\" \\\n      -e POSTGRES_PASSWORD=\"root\" \\\n      -e POSTGRES_DB=\"ny_taxi\" \\\n      -v dtc_postgres_volume_local:/var/lib/postgresql/data \\\n      -p 5432:5432 \\\n      postgres:13\n    ```\n\n3. Verify the command works in Docker Desktop under Volumes. The `dtc_postgres_volume_local` should be listed, but the folder `ny_taxi_postgres_data` will be empty as an alternative configuration is used.\n\n**Alternate Error:**\n\n```plaintext\ninitdb: error: directory \"/var/lib/postgresql/data\" exists but is not empty\n```\n\nTo resolve this, either remove or empty the directory \"/var/lib/postgresql/data\", or run `initdb`."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/022_3e83372387_docker-invalid-reference-format-repository-name-mu.md", "metadata": {"id": "3e83372387", "question": "Docker: invalid reference format: repository name must be lowercase (Mounting volumes with Docker on Windows)", "sort_order": 22}, "content": "Mapping volumes on Windows can be tricky. If the approach shown in the course video doesn't work for you, consider the following suggestions:\n\n- Move your data to a directory without spaces. For example, move from `C:/Users/Alexey Grigorev/git/...` to `C:/git/...`.\n\n- Replace the `-v` part with one of these options:\n\n  ```bash\n  -v /c:/some/path/ny_taxi_postgres_data:/var/lib/postgresql/data\n  -v //c:/some/path/ny_taxi_postgres_data:/var/lib/postgresql/data\n  -v /c/some/path/ny_taxi_postgres_data:/var/lib/postgresql/data\n  -v //c/some/path/ny_taxi_postgres_data:/var/lib/postgresql/data\n  --volume //driveletter/path/ny_taxi_postgres_data/:/var/lib/postgresql/data\n  ```\n\n- Add `winpty` before the whole command:\n\n  ```bash\n  winpty docker run -it \\\n  -e POSTGRES_USER=\"root\" \\\n  -e POSTGRES_PASSWORD=\"root\" \\\n  -e POSTGRES_DB=\"ny_taxi\" \\\n  -v /c:/some/path/ny_taxi_postgres_data:/var/lib/postgresql/data \\\n  -p 5432:5432 \\\n  postgres:1\n  ```\n\n- Try adding quotes:\n\n  ```bash\n  -v \"/c:/some/path/ny_taxi_postgres_data:/var/lib/postgresql/data\"\n  -v \"//c:/some/path/ny_taxi_postgres_data:/var/lib/postgresql/data\"\n  -v “/c/some/path/ny_taxi_postgres_data:/var/lib/postgresql/data\"\n  -v \"//c/some/path/ny_taxi_postgres_data:/var/lib/postgresql/data\"\n  -v \"c:\\some\\path\\ny_taxi_postgres_data\":/var/lib/postgresql/data\n  ```\n\n- Note: If Windows automatically creates a folder called `ny_taxi_postgres_data;C`, it suggests a problem with volume mapping. Try deleting both folders and replacing the `-v` part with other options. Using `//c/` instead of `/c/` might work, as it creates the correct folder `ny_taxi_postgres_data`.\n\n- A possible solution is using `\"$(pwd)\"/ny_taxi_postgres_data:/var/lib/postgresql/data` and pay attention to the placement of quotes.\n\n- If none of these work, use a volume name instead of the path:\n\n  ```bash\n  -v ny_taxi_postgres_data:/var/lib/postgresql/data\n  ```\n\n- For Mac, you can wrap `$(pwd)` with quotes:\n\n  ```bash\n  docker run -it \\\n  -e POSTGRES_USER=\"root\" \\\n  -e POSTGRES_PASSWORD=\"root\" \\\n  -e POSTGRES_DB=\"ny_taxi\" \\\n  -v \"$(pwd)\"/ny_taxi_postgres_data:/var/lib/postgresql/data \\\n  -p 5432:5432 \\\n  postgres:13\n  ```\n\nSource: [StackOverflow](https://stackoverflow.com/questions/48522615/docker-error-invalid-reference-format-repository-name-must-be-lowercase)"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/023_fb9fab513f_docker-error-response-from-daemon-invalid-mode-pro.md", "metadata": {"id": "fb9fab513f", "question": "Docker: Error response from daemon: invalid mode: \\Program Files\\Git\\var\\lib\\postgresql\\data.", "sort_order": 23}, "content": "Change the mounting path. Replace it with one of the following:\n\n```bash\n-v /e/zoomcamp/...:/var/lib/postgresql/data\n```\nOr\n\n```bash\n-v /c:/.../ny_taxi_postgres_data:/var/lib/postgresql/data\n```\n\n(Note: Include a leading slash in front of `c:`)"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/024_c1eeebf4ce_docker-error-response-from-daemon-error-while-crea.md", "metadata": {"id": "c1eeebf4ce", "question": "Docker: Error response from daemon: error while creating buildmount source", "sort_order": 24}, "content": "You may get this error:\n\n```\nerror while creating buildmount source path '/run/desktop/mnt/host/c/<your path>': mkdir /run/desktop/mnt/host/c: file exists\n```\n\nWhen you encounter the error above while rerunning your Docker command, it indicates that you should not mount on the second run. Here’s the initial problematic command:\n\n```bash\ndocker run -it \\\n  -e POSTGRES_USER=\"root\" \\\n  -e POSTGRES_PASSWORD=\"root\" \\\n  -e POSTGRES_DB=\"ny_taxi\" \\\n  -v <your path>:/var/lib/postgresql/data \\\n  -p 5432:5432 \\\n  postgres:13\n```\n\nTo resolve the issue, use the revised command without the volume mount:\n\n```bash\ndocker run -it \\\n  -e POSTGRES_USER=\"root\" \\\n  -e POSTGRES_PASSWORD=\"root\" \\\n  -e POSTGRES_DB=\"ny_taxi\" \\\n  -p 5432:5432 \\\n  postgres:13\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/025_4e92a486d1_docker-build-error-error-checking-context-cant-sta.md", "metadata": {"id": "4e92a486d1", "question": "Docker: build error: error checking context: 'can't stat '/home/user/repos/data-engineering/week_1_basics_n_setup/2_docker_sql/ny_taxi_postgres_data''.", "sort_order": 25}, "content": "This error appeared when running the command:\n\n```bash\ndocker build -t taxi_ingest:v001 .\n```\n\nThe issue often arises because the user ID of the directory `ny_taxi_postgres_data` was changed, causing permission errors when accessing it. To resolve this error, use a directory containing only the necessary files, `Dockerfile` and `ingest_data.py`.\n\nIf you need to change permissions, use the following command on Ubuntu:\n\n```bash\nsudo chown -R $USER dir_path\n```\n\nOn Windows, follow the instructions in this guide: [The Geek Page](https://thegeekpage.com/take-ownership-of-a-file-folder-through-command-prompt-in-windows-10/).\n\nFor more information, refer to this explanation on Stack Overflow: [Docker build error checking context](https://stackoverflow.com/questions/41286028/docker-build-error-checking-context-cant-stat-c-users-username-appdata)."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/026_d87e3d2a14_docker-erro0000-error-waiting-for-container-contex.md", "metadata": {"id": "d87e3d2a14", "question": "Docker: ERRO[0000] error waiting for container: context canceled", "sort_order": 26}, "content": "You might have installed Docker via snap. Run the following command to verify:\n\n```bash\nsudo snap status docker\n```\n\nIf you receive the response:\n\n```\nerror: unknown command \"status\", see 'snap help'.\n```\n\nThen uninstall Docker and install it via the [official website](https://docs.docker.com/engine/install/ubuntu/).\n\nError message: \"Bind for 0.0.0.0:5432 failed: port is already allocated.\""}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/027_7c4d422363_docker-build-error-checking-context-cant-stat-home.md", "metadata": {"id": "7c4d422363", "question": "Docker: build error checking context: can’t stat ‘/home/fhrzn/Projects/…./ny_taxi_postgres_data’", "sort_order": 27}, "content": "This issue occurs due to insufficient authorization rights to the host folder, which may cause it to appear empty.\n\n**Solution:**\n\nAdd permission for everyone to the folder:\n\n```bash\nsudo chmod -R 777 <path_to_folder>\n```\n\nExample:\n\n```bash\nsudo chmod -R 777 ny_taxi_postgres_data/\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/028_82bb8e49ea_docker-failed-to-solve-with-frontend-dockerfilev0.md", "metadata": {"id": "82bb8e49ea", "question": "Docker: failed to solve with frontend dockerfile.v0: failed to read dockerfile: error from sender: open ny_taxi_postgres_data: permission denied.", "sort_order": 28}, "content": "This issue occurs on Ubuntu/Linux systems when attempting to rebuild the Docker container.\n\n```bash\n$ docker build -t taxi_ingest:v001 .\n```\n\nA folder is created to host the Docker files. When the build command is executed again, a permission error may occur because there are no permissions on this new folder. To resolve this, grant permissions by running the command:\n\n```bash\n$ sudo chmod -R 755 ny_taxi_postgres_data\n```\n\nIf issues persist, use:\n\n```bash\n$ sudo chmod -R 777 ny_taxi_postgres_data\n```\n\nNote: 755 grants write access only to the owner."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/029_79e44b9c07_docker-docker-network-name.md", "metadata": {"id": "79e44b9c07", "question": "Docker: Docker network name", "sort_order": 29}, "content": "Get the network name via:\n\n```bash\ndocker network ls\n```\n\nFor more details, refer to the [Docker network ls documentation](https://docs.docker.com/engine/reference/commandline/network_ls/)."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/030_381dfe5145_docker-error-response-from-daemon-conflict-the-con.md", "metadata": {"id": "381dfe5145", "question": "Docker: Error response from daemon: Conflict. The container name \"pg-database\" is already in use by container \"xxx\". You have to remove (or rename) that container to be able to reuse that name.", "sort_order": 30}, "content": "Sometimes, when you try to restart a Docker container configured with a network name, the error message appears.\n\nTo resolve this issue:\n\n1. If the container is in a running state, stop it using:\n   \n   ```bash\n   docker stop <container_name>\n   ```\n   \n2. Then remove the container:\n   \n   ```bash\n   docker rm pg-database\n   ```\n\nAlternatively, you can use `docker start` instead of `docker run` to restart the Docker container without removing it."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/031_1700cb2bd4_docker-ingestion-when-using-docker-compose-could-n.md", "metadata": {"id": "1700cb2bd4", "question": "Docker: ingestion when using docker-compose could not translate host name", "sort_order": 31}, "content": "Typical error:\n```python\nn.exc.OperationalError: (psycopg2.OperationalError) could not translate host name \"pgdatabase\" to address: Name or service not known\n```\n\n**Solution:**\n\n1. Run `docker-compose up -d` to start your containers.\n2. Check which network is created by Docker, as it may differ from your expectations.\n3. Use the actual network name in your ingestion script instead of \"pg-network\".\n4. Confirm the correct database service name, replacing \"pgdatabase\" accordingly.\n\n**Example:**\n- \"pg-network\" might become \"2docker_default\"."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/032_c6cd0fedc5_docker-cannot-install-docker-on-macoswindows-11-vm.md", "metadata": {"id": "c6cd0fedc5", "question": "Docker: Cannot install docker on MacOS/Windows 11 VM running on top of Linux (due to Nested virtualization).", "sort_order": 32}, "content": "Before starting your VM, you need to enable nested virtualization. Run the following commands based on your CPU:\n\n- **For Intel CPU:**\n  \n  ```bash\n  modprobe -r kvm_intel\n  modprobe kvm_intel nested=1\n  ```\n\n- **For AMD CPU:**\n  \n  ```bash\n  modprobe -r kvm_amd\n  modprobe kvm_amd nested=1\n  ```"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/033_0cc94bef66_docker-connecting-from-vs-code.md", "metadata": {"id": "0cc94bef66", "images": [{"description": "image #1", "id": "image_1", "path": "images/data-engineering-zoomcamp/image_ea5934b5.png"}], "question": "Docker: Connecting from VS Code", "sort_order": 33}, "content": "It’s very easy to manage your Docker container, images, network, and compose projects from VS Code.\n\n- Install the [official extension](https://marketplace.visualstudio.com/items?itemName=ms-azuretools.vscode-docker) and launch it from the left side icon.\n\n  <{IMAGE:image_1}>\n\n- It will work even if your Docker runs on WSL2, as VS Code can easily connect with your Linux."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/034_9e05e54958_docker-how-to-stop-a-container.md", "metadata": {"id": "9e05e54958", "question": "Docker: How to stop a container?", "sort_order": 34}, "content": "Use the following command:\n\n```bash\ndocker stop <container_id>\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/035_0812df9bf6_docker-postgresql-database-directory-appears-to-co.md", "metadata": {"id": "0812df9bf6", "question": "Docker: PostgreSQL Database directory appears to contain a database. Database system is shut down", "sort_order": 35}, "content": "When you see this in logs, your container with PostgreSQL is not accepting any requests. Attempting to connect may result in the error:\n\n```\nconnection failed: server closed the connection unexpectedly\n\nThis probably means the server terminated abnormally before or while processing the request.\n```\n\nTo resolve this issue:\n\n1. **Delete Data Directory**: Delete the directory with data (the one you map to the container using the `-v` flag) and restart the container.\n\n2. **Preserve Critical Data**: If your data is critical, you may be able to reset the write-ahead log from within the Docker container. For more details, see [here](https://github.com/alexg9010/2025_data_engineering_zoomcamp/blob/master/01_docker/README.md#fix-broken-postgress-docker-container).\n\n   ```bash\n   docker run -it \\\n   -e POSTGRES_USER=\"root\" \\\n   -e POSTGRES_PASSWORD=\"root\" \\\n   -e POSTGRES_DB=\"ny_taxi\" \\\n   -v $(pwd)/ny_taxi_postgres_data:/var/lib/postgresql/data \\\n   -p 5432:5432 \\\n   --network pg-network \\\n   postgres:13 \\\n   /bin/bash -c 'gosu postgres pg_resetwal /var/lib/postgresql/data'\n   ```"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/036_1b727dde32_docker-docker-not-installable-on-ubuntu.md", "metadata": {"id": "1b727dde32", "question": "Docker: Docker not installable on Ubuntu", "sort_order": 36}, "content": "On some versions of Ubuntu, the `snap` command can be used to install Docker.\n\n```bash\nsudo snap install docker\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/037_3f1c2f93bd_docker-compose-mounting-error.md", "metadata": {"id": "3f1c2f93bd", "question": "Docker-Compose: mounting error", "sort_order": 37}, "content": "```\nerror: could not change permissions of directory \"/var/lib/postgresql/data\": Operation not permitted\n```\n\nIf you have used the previous answer and created a local Docker volume, then you need to inform the compose file about the named volume:\n\n```yaml\ndtc_postgres_volume_local:  # Define the named volume here\n```\n\n- Services mentioned in the compose file automatically become part of the same network.\n\n### Steps:\n\n1. Use the command:\n   ```bash\n   docker volume inspect dtc_postgres_volume_local\n   ```\n   to see the location by checking the value of `Mountpoint`.\n\n2. In some cases, after running `docker compose up`, the mounting directory created is named `docker_sql_dtc_postgres_volume_local` instead of the existing `dtc_postgres_volume_local`.\n\n3. Rename the existing `dtc_postgres_volume_local` to `docker_sql_dtc_postgres_volume_local`:\n   - Be careful when performing this operation.\n\n4. Remove the newly created one.\n\n5. Run `docker compose up` again and check if the table is there."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/038_8516ca8849_docker-compose-error-translating-host-name-to-addr.md", "metadata": {"id": "8516ca8849", "question": "Docker-Compose: Error translating host name to address", "sort_order": 38}, "content": "Couldn’t translate host name to address"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/039_edcc24a810_docker-compose-data-retention-could-not-translate.md", "metadata": {"id": "edcc24a810", "images": [{"description": "image #1", "id": "image_1", "path": "images/data-engineering-zoomcamp/image_dc508dc3.png"}], "question": "Docker-Compose: Data retention (could not translate host name \"pg-database\" to address: Name or service not known)", "sort_order": 39}, "content": "<{IMAGE:image_1}>\n\nMake sure the PostgreSQL database is running. Use the command to start containers in detached mode:\n\n```bash\ndocker-compose up -d\n```\n\nExample output:\n\n```plaintext\n% docker compose up -d\n\n[+] Running 2/2\n⠿ Container pg-admin     Started\n⠿ Container pg-database  Started\n```\n\nTo view the containers use:\n\n```bash\ndocker ps\n```\n\nExample output:\n\n```\n% docker ps\n\nCONTAINER ID   IMAGE            COMMAND                  CREATED          STATUS          PORTS                           NAMES\nfaf05090972e   postgres:13      \"docker-entrypoint.s…\"   39 seconds ago   Up 37 seconds   0.0.0.0:5432->5432/tcp          pg-database\n6344dcecd58f   dpage/pgadmin4   \"/entrypoint.sh\"         39 seconds ago   Up 37 seconds   443/tcp, 0.0.0.0:8080->80/tcp   pg-adminhw\n```\n\nTo view logs for a container:\n\n```bash\ndocker logs <containerid>\n```\n\nExample logs for PostgreSQL:\n\n```\n% docker logs faf05090972e\n\nPostgreSQL Database directory appears to contain a database; Skipping initialization\n2022-01-25 05:58:45.948 UTC [1] LOG:  starting PostgreSQL 13.5 (Debian 13.5-1.pgdg110+1) on aarch64-unknown-linux-gnu, compiled by gcc (Debian 10.2.1-6) 10.2.1 20210110, 64-bit\n2022-01-25 05:58:45.948 UTC [1] LOG:  listening on IPv4 address \"0.0.0.0\", port 5432\n2022-01-25 05:58:45.948 UTC [1] LOG:  listening on IPv6 address \"::\", port 5432\n2022-01-25 05:58:45.954 UTC [1] LOG:  listening on Unix socket \"/var/run/postgresql/.s.PGSQL.5432\"\n2022-01-25 05:58:45.984 UTC [28] LOG:  database system was interrupted; last known up at 2022-01-24 17:48:35 UTC\n2022-01-25 05:58:48.581 UTC [28] LOG:  database system was not properly shut down; automatic recovery in progress\n2022-01-25 05:58:48.602 UTC [28] LOG:  redo starts at 0/872A5910\n2022-01-25 05:59:33.726 UTC [28] LOG:  invalid record length at 0/98A3C160: wanted 24, got 0\n2022-01-25 05:59:33.726 UTC [28] LOG:  redo done at 0/98A3C128\n2022-01-25 05:59:48.051 UTC [1] LOG:  database system is ready to accept connections\n```\n\nIf `docker ps` doesn’t show `pg-database` running, use:\n\n```bash\ndocker ps -a\n```\n\n- This will show all containers, either running or stopped.\n- Get the container ID for `pg-database-1` and run the appropriate command.\n\nIf you lose database data after executing `docker-compose up` and cannot successfully execute your ingestion script due to the following error:\n\n```plaintext\nsqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not translate host name /data_pgadmin:/var/lib/pgadmin\"pg-database\" to address: Name or service not known\n```\n\n- Docker Compose may be creating its own default network since it is no longer specified in the command or file.\n- Check logs after executing `docker-compose up` to find the network name and change the network name argument in your ingestion script.\n\nIf problems persist with `pgcli`, consider using HeidiSQL."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/040_432d3f7e09_docker-compose-hostname-does-not-resolve.md", "metadata": {"id": "432d3f7e09", "question": "Docker-Compose: Hostname does not resolve", "sort_order": 40}, "content": "When encountering the error:\n\n```\nError response from daemon: network 66ae65944d643fdebbc89bd0329f1409dec2c9e12248052f5f4c4be7d1bdc6a3 not found\n```\n\nTry the following steps:\n\n1. Run `docker ps -a` to see all stopped and running containers.\n2. Remove all containers to clean up the environment.\n3. Execute `docker-compose up -d` again.\n\nIf facing issues connecting to the server at `localhost:8080` with the error:\n\n```\nUnable to connect to server: could not translate host name 'pg-database' to address: Name does not resolve\n```\n\nConsider these solutions:\n\n- Use a new hostname without dashes, e.g., `pgdatabase`.\n- Make sure to specify the Docker network and use the same network in both containers in your `docker-compose.yml` file.\n\nExample `docker-compose.yml`:\n\n```yaml\nservices:\n\n  pgdatabase:\n    image: postgres:13\n    environment:\n      - POSTGRES_USER=root\n      - POSTGRES_PASSWORD=root\n      - POSTGRES_DB=ny_taxi\n    volumes:\n      - \"./ny_taxi_postgres_data:/var/lib/postgresql/data:rw\"\n    ports:\n      - \"5431:5432\"\n    networks:\n      - pg-network\n\n  pgadmin:\n    image: dpage/pgadmin4\n    environment:\n      - PGADMIN_DEFAULT_EMAIL=admin@admin.com\n      - PGADMIN_DEFAULT_PASSWORD=root\n    ports:\n      - \"8080:80\"\n    networks:\n      - pg-network\n\nnetworks:\n  pg-network:\n    name: pg-network\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/041_a950fda271_docker-compose-pgadmin-no-database-in-pgadmin.md", "metadata": {"id": "a950fda271", "question": "Docker-Compose: PgAdmin – no database in PgAdmin", "sort_order": 41}, "content": "When you log into PgAdmin and see an empty database, the following solution can help:\n\nRun:\n   \n```bash\ndocker-compose up\n```\n\nAnd at the same time run:\n\n```bash\ndocker build -t taxi_ingest:v001 .\n\n# NETWORK NAME IS THE SAME AS THAT CREATED BY DOCKER COMPOSE\ndocker run -it \\\n  --network=pg-network \\\n  taxi_ingest:v001 \\\n  --user=postgres \\\n  --password=postgres \\\n  --host=db \\\n  --port=5432 \\\n  --db=ny_taxi \\\n  --table_name=green_tripdata \\\n  --url=${URL}\n```\n\nIt's important to use the same `--network` as stated in the `docker-compose.yaml` file.\n\nThe `docker-compose.yaml` file might not specify a network, as shown below:\n\n```yaml\nservices:\n  db:\n    container_name: postgres\n    image: postgres:17-alpine\n    environment:\n      ...\n    ports:\n      - '5433:5432'\n    volumes:\n      - ...\n  pgadmin:\n    container_name: pgadmin\n    image: dpage/pgadmin4:latest\n    environment:\n      ...\n    ports:\n      - '8080:80'\n    volumes:\n      - ...\n\nvolumes:\n  vol-pgdata:\n    name: vol-pgdata\n  vol-pgadmin_data:\n    name: vol-pgadmin_data\n```\n\nIf the network name is not specified, it is generated automatically: The name of the directory containing the `docker-compose.yaml` file in lowercase + `_default`.\n\nYou can find the network’s name when running `docker-compose up`:\n\n```\npg-database Pulling pg-database Pulled \nNetwork week_1_default  Creating\nNetwork week_1_default  Created\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/042_ac52bea382_docker-compose-persist-pgadmin-docker-contents-on.md", "metadata": {"id": "ac52bea382", "question": "Docker-Compose: Persist PGAdmin docker contents on GCP", "sort_order": 42}, "content": "One common issue when running Docker Compose on GCP is that PostgreSQL might not persist its data to the specified path. For example:\n\n```yaml\nservices:\n  ...\n  pgadmin:\n    ...\n    volumes:\n      - \"./pgadmin:/var/lib/pgadmin:wr\"\n```\n\nThis setup might not work. To resolve this, use Docker Volume to make the data persist:\n\n```yaml\nservices:\n  ...\n  pgadmin:\n    ...\n    volumes:\n      - pgadmin:/var/lib/pgadmin\n\nvolumes:\n  pgadmin:\n```\n\nThis configuration change ensures the persistence of the PGAdmin data on GCP."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/043_5baf3e8a5e_docker-docker-engine-stopped_failed-to-fetch-exten.md", "metadata": {"id": "5baf3e8a5e", "question": "Docker: Docker engine stopped_failed to fetch extensions", "sort_order": 43}, "content": "The Docker engine may crash continuously and fail to work after restart. You might see error messages like \"docker engine stopped\" and \"failed to fetch extensions\" repeatedly on the screen.\n\n**Solution:**\n\n- Check if you have the latest version of Docker installed. Update Docker if necessary.\n- If the problem persists, consider reinstalling Docker.\n  - Note: You will need to fetch images again, but there should be no other issues."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/044_docker-compose-persist-pgadm.md", "metadata": {"id": "4146155608", "question": "Docker-Compose: Persist PGAdmin configuration", "sort_order": 44}, "content": "To persist pgAdmin configuration, such as the server name, modify your `docker-compose.yml` by adding a \"volumes\" section:\n\n```yaml\nservices:\n\n  pgdatabase:\n    [...]\n\n  pgadmin:\n    image: dpage/pgadmin4\n    environment:\n      - PGADMIN_DEFAULT_EMAIL=admin@admin.com\n      - PGADMIN_DEFAULT_PASSWORD=root\n    volumes:\n      - \"./pgAdmin_data:/var/lib/pgadmin/sessions:rw\"\n    ports:\n      - \"8080:80\"\n```\n\nIn the example above, \"pgAdmin_data\" is a folder on the host machine, and \"/var/lib/pgadmin/sessions\" is the session settings folder in the pgAdmin container.\n\nBefore running `docker-compose up` on the YAML file, provide the pgAdmin container with access permissions to the \"pgAdmin_data\" folder. The container runs with a username \"5050\" and user group \"5050\". Use the following command to set permissions:\n\n```bash\nsudo chown -R 5050:5050 pgAdmin_data\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/045_63e87e7442_docker-compose-dial-unix-varrundockersock-connect.md", "metadata": {"id": "63e87e7442", "question": "Docker-Compose - dial unix /var/run/docker.sock: connect: permission denied", "sort_order": 45}, "content": "This happens if you did not create the docker group and add your user. Follow these steps from the link: [guides/docker-without-sudo.md at main · sindresorhus/guides · GitHub](https://github.com/sindresorhus/guides/blob/main/docker-without-sudo.md)\n\n1. Press `Ctrl+D` to log out and log back in again.\n\nIf you are tired of having to set up your database connection each time you start the containers, create a volume for pgAdmin:\n\nIn your `docker-compose.yaml` file, add the following under your pgAdmin service:\n\n```yaml\nservices:\n  pgadmin:\n    volumes:\n      - type: volume\n        source: pgadmin_data\n        target: /var/lib/pgadmin\n```\n\nAlso, add the following to the end of the file:\n\n```yaml\nvolumes:\n  pgadmin_data:\n```\n\nThis configuration will maintain the state so that pgAdmin remembers your previous connections."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/046_e43abaa421_docker-docker-compose-still-not-available-after-ch.md", "metadata": {"id": "e43abaa421", "question": "Docker: docker-compose still not available after changing .bashrc", "sort_order": 46}, "content": "This issue can occur after installing Docker Compose in a Google Cloud VM, as demonstrated in video 1.4.1. \n\nIf the downloaded Docker Compose file from GitHub is named `docker-compose-linux-x86_64`, you may need to rename it for convenience. Here's how to resolve the issue:\n\n1. Rename `docker-compose-linux-x86_64` to `docker-compose` using the following command:\n   \n   ```bash\n   mv docker-compose-linux-x86_64 docker-compose\n   ```\n\nBy doing this, you can use the `docker-compose` command directly."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/047_4ee3b7231a_docker-compose-error-getting-credentials-after-run.md", "metadata": {"id": "4ee3b7231a", "question": "Docker-Compose: Error getting credentials after running docker-compose up -d", "sort_order": 47}, "content": "Installing pass via `sudo apt install pass` helped to solve the issue. More about this can be found here: [https://github.com/moby/buildkit/issues/1078](https://github.com/moby/buildkit/issues/1078)"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/048_73876c8348_docker-compose-errors-pertaining-to-docker-compose.md", "metadata": {"id": "73876c8348", "images": null, "question": "Docker-Compose: Errors pertaining to docker-compose.yml and pgadmin setup", "sort_order": 48}, "content": "For those experiencing problems with Docker Compose, getting data in PostgreSQL, and similar issues, follow these steps:\n\n- **Create a new volume** on Docker, either using the command line or Docker Desktop app.\n- **Modify your `docker-compose.yml` file** as needed to fix any setup issues.\n- **Set `low_memory=False`** when importing the CSV file using pandas:\n  \n```python\ndf = pd.read_csv('yellow_tripdata_2021-01.csv', nrows=1000, low_memory=False)\n```\n\n- Use the specified function in your `upload-data.ipynb` for better tracking of the ingestion process.\n\n```python\nfrom time import time\n\ncounter = 0\ntime_counter = 0\n\nwhile True:\n    t_start = time()\n\n    df = next(df_iter)\n\n    df.tpep_pickup_datetime = pd.to_datetime(df.tpep_pickup_datetime)\n    df.tpep_dropoff_datetime = pd.to_datetime(df.tpep_dropoff_datetime)\n\n    df.to_sql(name='yellow_taxi_data', con=engine, if_exists='append')\n\n    t_end = time()\n\n    t_elapsed = t_end - t_start\n\n    print('Chunk Insertion Done! Time taken: %.2f seconds' %(t_elapsed))\n\n    counter += 1\n    time_counter += t_elapsed\n\n    if counter == 14:\n        print('All Chunks Inserted! Total Time Taken: %.2f seconds' %(time_counter))\n        break\n```\n\n### Order of Execution:\n\n1. Open the terminal in the `2_docker_sql` folder and run: `docker compose up`\n2. Ensure no other containers are running except the ones you just executed (pgAdmin and pgdatabase).\n3. Open Jupyter Notebook and begin the data ingestion.\n4. Open pgAdmin and set up a server. Make sure you use the same configurations as your `docker-compose.yml` file, such as the same name (`pgdatabase`), port, and database name (`ny_taxi`)."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/049_9ef838bb46_docker-compose-up-d-error-getting-credentials-err.md", "metadata": {"id": "9ef838bb46", "question": "Docker: Compose up -d error getting credentials - err: exec: \"docker-credential-desktop\": executable file not found in %PATH%, out: ``", "sort_order": 49}, "content": "To resolve this error, follow these steps:\n\n1. Locate the `config.json` file for Docker, typically found in your home directory at `Users/username/.docker`.\n2. Modify the `credsStore` setting to `credStore`.\n3. Save the file and re-run your Docker Compose command."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/050_4ccef7c92d_docker-compose-which-docker-compose-binary-to-use.md", "metadata": {"id": "4ccef7c92d", "question": "Docker-Compose: Which docker-compose binary to use for WSL?", "sort_order": 50}, "content": "To determine which `docker-compose` binary to download from [Docker Compose releases](https://github.com/docker/compose/releases), you can check your system with the following commands:\n\n- To check the system type:\n\n  ```bash\n  uname -s  # This will most likely return 'Linux'\n  ```\n\n- To check the system architecture:\n\n  ```bash\n  uname -m  # This will return your system's 'flavor'\n  ```\n\nAlternatively, you can use the following command to download `docker-compose` directly:\n\n```bash\nsudo curl -L \"https://github.com/docker/compose/releases/download/1.29.2/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-compose\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/051_5f056e236c_docker-compose-error-undefined-volume-in-windowsws.md", "metadata": {"id": "5f056e236c", "question": "Docker-Compose - Error undefined volume in Windows/WSL", "sort_order": 51}, "content": "If you wrote the `docker-compose.yaml` file exactly like the video, you might run into an error:\n\n```\nservice \"pgdatabase\" refers to undefined volume dtc_postgres_volume_local: invalid compose project\n```\n\nTo resolve this, include the volume definition in your `docker-compose.yaml` file by adding:\n\n```yaml\ndt_postgres_volume_local:\n```\n\nThis should be added under the `volumes` section. Make sure your file looks similar to this:\n\n```yaml\nvolumes:\n  dtc_postgres_volume_local:\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/052_e532c9677e_docker-compose-cannot-execute-binary-file-exec-for.md", "metadata": {"id": "e532c9677e", "question": "Docker-Compose: cannot execute binary file: Exec format error", "sort_order": 52}, "content": "This error indicates that the docker-compose executable cannot be opened in the current OS. Ensure that the file you download from GitHub matches your system environment.\n\nAs of 2025/1/17, docker-compose ([v2.32.4](https://github.com/docker/compose/releases/tag/v2.32.4)) [docker-compose-linux-aarch64](https://github.com/docker/compose/releases/download/v2.32.4/docker-compose-linux-aarch64) does not work. Try v2.32.3 [docker-compose-linux-x86_64](https://github.com/docker/compose/releases/download/v2.32.3/docker-compose-linux-x86_64)."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/053_d3c860aa46_docker-postgres-container-fails-to-launch-with-exi.md", "metadata": {"id": "d3c860aa46", "question": "Docker: Postgres container fails to launch with exit code (1) when attempting to compose", "sort_order": 53}, "content": "This issue arises because the Postgres database is not initialized before executing `docker-compose up -d`. While there are other potential solutions [discussed in this thread](https://forums.docker.com/t/one-of-the-postgres-containers-stops-as-soon-as-it-starts/74714/3), you can resolve it by initializing the database first. Then, the Docker Compose will work as expected.\n\n```bash\ndocker run -it \\\n  -e POSTGRES_USER=\"root\" \\\n  -e POSTGRES_PASSWORD=\"root\" \\\n  -e POSTGRES_DB=\"ny_taxi\" \\\n  -v $(pwd)/ny_taxi_data:/var/lib/postgresql/data \\\n  -p 5432:5432 \\\n  --network=pg-network \\\n  --name=pg_database \\\n  postgres:13\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/054_5bd34587a0_wsl-docker-directory-permissions-error.md", "metadata": {"id": "5bd34587a0", "question": "WSL: Docker directory permissions error", "sort_order": 54}, "content": "```\ninitdb: error: could not change permissions of directory\n```\n\nWSL and Windows do not manage permissions in the same way, causing conflict if using the Windows file system rather than the WSL file system.\n\nSolution:  **Use Docker volumes.**\n\nVolume is used for storage of persistent data and not for transferring files. A local volume is unnecessary.\n\nThis resolves permission issues and allows for better management of volumes.\n\n**Note:** The `user:` is not necessary if using Docker volumes but is required if using a local drive.\n\n\n```yaml\nservices:\n  postgres:\n    image: postgres:15-alpine\n    container_name: postgres\n    user: \"0:0\"\n    environment:\n      - POSTGRES_USER=postgres\n      - POSTGRES_PASSWORD=postgres\n      - POSTGRES_DB=ny_taxi\n    volumes:\n      - \"pg-data:/var/lib/postgresql/data\"\n    ports:\n      - \"5432:5432\"\n    networks:\n      - pg-network\n\n  pgadmin:\n    image: dpage/pgadmin4\n    container_name: pgadmin\n    user: \"${UID}:${GID}\"\n    environment:\n      - PGADMIN_DEFAULT_EMAIL=email@some-site.com\n      - PGADMIN_DEFAULT_PASSWORD=pgadmin\n    volumes:\n      - \"pg-admin:/var/lib/pgadmin\"\n    ports:\n      - \"8080:80\"\n    networks:\n      - pg-network\n\nnetworks:\n  pg-network:\n    name: pg-network\n\nvolumes:\n  pg-data:\n  pg-admin:\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/055_ec091c69a0_wsl-insufficient-system-resources-exist-to-complet.md", "metadata": {"id": "ec091c69a0", "question": "WSL: Insufficient system resources exist to complete the requested service.", "sort_order": 55}, "content": "**Cause:**\n\nThis error occurs because some applications are not updated. Specifically, check for any pending updates for Windows Terminal, WSL, and Windows Security updates.\n\n**Solution:**\n\nTo update Windows Terminal:\n\n1. Open the Microsoft Store.\n2. Go to your library of installed apps.\n3. Search for Windows Terminal.\n4. Update the app.\n5. Restart your system to apply the changes.\n\nFor updating Windows Security updates:\n\n1. Go to Windows Updates settings.\n2. Check for any pending updates, especially security updates.\n3. Restart your system once the updates are downloaded and installed successfully."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/056_62f6c8dd8d_wsl-wsl-integration-with-distro-ubuntu-unexpectedl.md", "metadata": {"id": "62f6c8dd8d", "images": [{"description": "image #1", "id": "image_1", "path": "images/data-engineering-zoomcamp/image_bc654841.png"}], "question": "WSL: WSL integration with distro Ubuntu unexpectedly stopped with exit code 1.", "sort_order": 56}, "content": "<{IMAGE:image_1}>\n\nUpon restarting, the same issue appears and occurs unexpectedly on Windows.\n\n**Solutions:**\n\n1. **Fixing DNS Issue**\n   \n   This solution is credited to [reddit](https://www.reddit.com/r/docker/comments/p98xq6/docker_failed_to_start_exit_code_1/) and has worked for some users.\n\n   ```bash\n   reg add \"HKLM\\System\\CurrentControlSet\\Services\\Dnscache\" /v \"Start\" /t REG_DWORD /d \"4\" /f\n   ```\n\n   Restart your computer and then re-enable it with the following command:\n\n   ```bash\n   reg add \"HKLM\\System\\CurrentControlSet\\Services\\Dnscache\" /v \"Start\" /t REG_DWORD /d \"2\" /f\n   ```\n\n   Restart your OS again. It should work.\n\n2. **Switch to Linux Containers**\n\n   - Right-click on the running Docker icon (next to the clock).\n   - Choose \"Switch to Linux containers.\"\n\n```bash\nbash: conda: command not found\n```\n\n```bash\nDatabase is uninitialized and superuser password is not specified.\n```\n\n```bash\nDatabase is uninitialized and superuser password is not specified.\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/057_73c8de600c_wsl-permissions-too-open-at-windows.md", "metadata": {"id": "73c8de600c", "question": "WSL: Permissions too open at Windows", "sort_order": 57}, "content": "Issue when trying to run the GPC VM through SSH via WSL2, likely because WSL2 isn’t looking for .ssh keys in the correct folder. The command attempted:\n\n```bash\nssh -i gpc [username]@[my external IP]\n```\n\n### Solutions\n\n1. **Use `sudo` Command**\n   \n   Try using `sudo` before executing the command:\n   \n   ```bash\n   sudo ssh -i gpc [username]@[my external IP]\n   ```\n\n2. **Change Permissions**\n   \n   Navigate to your folder and change the permissions for the private key SSH file:\n   \n   ```bash\n   chmod 600 gpc\n   ```\n\n3. **Create a `.ssh` Folder in WSL2**\n   \n   - Navigate to your home directory:\n     \n     ```bash\n     cd ~\n     ```\n   \n   - Create a `.ssh` folder:\n     \n     ```bash\n     mkdir .ssh\n     ```\n   \n   - Copy the content from the Windows `.ssh` folder to the newly created `.ssh` folder:\n     \n     ```bash\n     cp -r /mnt/c/Users/YourUsername/.ssh/* ~/.ssh/\n     ```\n   \n   - Adjust the permissions of the files and folders in the `.ssh` directory if necessary."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/058_16d0d756c3_wsl-could-not-resolve-host-name.md", "metadata": {"id": "16d0d756c3", "question": "WSL: Could not resolve host name", "sort_order": 58}, "content": "WSL2 may not be referencing the correct `.ssh/config` path from Windows. You can create a config file in the home directory of WSL2 by following these steps:\n\n1. Navigate to your home directory:\n   \n   ```bash\n   cd ~\n   ```\n\n2. Create the `.ssh` directory:\n   \n   ```bash\n   mkdir .ssh\n   ```\n\n3. Create a `config` file in the `.ssh` folder with the following content:\n\n   ```\n   HostName [GPC VM external IP]\n   User [username]\n   IdentityFile ~/.ssh/[private key]\n   ```"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/059_b9cd2aaccb_pgcli-connection-failed-1-port-5432-failed-could-n.md", "metadata": {"id": "b9cd2aaccb", "question": "PGCLI - connection failed: :1), port 5432 failed: could not receive data from server: Connection refused could not send SSL negotiation packet: Connection refused", "sort_order": 59}, "content": "To resolve the connection failure with PGCLI, use the following command to connect via socket:\n\n```bash\npgcli -h 127.0.0.1 -p 5432 -u root -d ny_taxi\n```\n\nEnsure the database server is running and properly configured to accept connections."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/060_8ddf54cbd0_pgcli-should-we-run-pgcli-inside-another-docker-co.md", "metadata": {"id": "8ddf54cbd0", "question": "PGCLI: Should we run pgcli inside another docker container?", "sort_order": 60}, "content": "In this section of the course, the 5432 port of PostgreSQL is mapped to your computer’s 5432 port. This means you can access the PostgreSQL database via pgcli directly from your computer.\n\nSo, no, you don’t need to run it inside another container. Your local system will suffice."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/061_e644e2b7b6_pgcli-fatal-password-authentication-failed-for-use.md", "metadata": {"id": "e644e2b7b6", "question": "PGCLI - FATAL: password authentication failed for user \"root\" (You already have Postgres)", "sort_order": 61}, "content": "For a more visual and detailed explanation, feel free to check the video [1.4.2 - Port Mapping and Networks in Docker](https://www.youtube.com/watch?v=tOr4hTsHOzU&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=16&ab_channel=DataTalksClub%E2%AC%9B).\n\nIf you want to debug the issue on MacOS, you can try the following steps:\n\n- **Check if something is blocking your port:**\n  \n  Use the `lsof` command to find out which application is using a specific port on your local machine:\n  \n  ```bash\n  lsof -i :5432\n  ```\n\n- **List running PostgreSQL services:**\n\n  Use `launchctl` to list running postgres services on your local machine.\n\n- **Unload the running service:**\n\n  Unload the launch agent for the PostgreSQL service, which will stop the service and free up the port:\n  \n  ```bash\n  launchctl unload -w ~/Library/LaunchAgents/homebrew.mxcl.postgresql.plist\n  ```\n\n- **Restart the service:**\n\n  ```bash\n  launchctl load -w ~/Library/LaunchAgents/homebrew.mxcl.postgresql.plist\n  ```\n\n- **Change the port:**\n\n  Changing the port from `5432:5432` to `5431:5432` can help avoid this error."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/062_05e6bff42c_pgcli-permissionerror-errno-13-permission-denied-s.md", "metadata": {"id": "05e6bff42c", "question": "PGCLI - PermissionError: [Errno 13] Permission denied: '/some/path/.config/pgcli'", "sort_order": 62}, "content": "I encountered this error:\n\n```bash\npgcli -h localhost -p 5432 -U root -d ny_taxi\n\nTraceback (most recent call last):\n  File \"/opt/anaconda3/bin/pgcli\", line 8, in <module>\n    sys.exit(cli())\n  File \"/opt/anaconda3/lib/python3.9/site-packages/click/core.py\", line 1128, in __call__\n    return self.main(*args, **kwargs)\n  File \"/opt/anaconda3/lib/python3.9/site-packages/click/core.py\", line 1053, in main\n    rv = self.invoke(ctx)\n  File \"/opt/anaconda3/lib/python3.9/site-packages/click/core.py\", line 1395, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"/opt/anaconda3/lib/python3.9/site-packages/click/core.py\", line 754, in invoke\n    return __callback(*args, **kwargs)\n  File \"/opt/anaconda3/lib/python3.9/site-packages/pgcli/main.py\", line 880, in cli\n    os.makedirs(config_dir)\n  File \"/opt/anaconda3/lib/python3.9/os.py\", line 225, in makedirspython\n    mkdir(name, mode)\nPermissionError: [Errno 13] Permission denied: '/Users/vray/.config/pgcli'\n```\n\n### Solution 1:\n\nThis error indicates that your user doesn’t have the necessary permissions to access or modify the directory or file (`/some/path/.config/pgcli`). This can occur in Docker environments when privileges are assigned to root instead of the current user.\n\nTo resolve this:\n\n1. Check the file permissions:\n\n   ```bash\n   ls -l /some/path/.config/pgcli\n   ```\n\n2. Change the ownership/permissions so that your user has the necessary permissions:\n\n   ```bash\n   sudo chown -R user_name /Users/user_name/.config\n   ```\n\n   - `sudo` stands for Super User DO.\n   - `chown` means change owner.\n   - `-R` applies recursively.\n   - `user_name` is your PC username (e.g., vray).\n\n### Solution 2:\n\nMake sure you install pgcli without using `sudo`. The recommended approach is to use conda/anaconda to avoid affecting your system Python.\n\nIf `conda install` gets stuck at \"Solving environment,\" try these alternatives:\n\n[https://stackoverflow.com/questions/63734508/stuck-at-solving-environment-on-anaconda](https://stackoverflow.com/questions/63734508/stuck-at-solving-environment-on-anaconda)"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/063_0d73dde53e_pgcli-no-pq-wrapper-available.md", "metadata": {"id": "0d73dde53e", "question": "PGCLI - no pq wrapper available.", "sort_order": 63}, "content": "**Error:**\n\n```\nImportError: no pq wrapper available.\n```\n\n### Problem Details:\n\n- Could not import `\\dt`\n- `opg 'c' implementation: No module named 'psycopg_c'`\n- `couldn't import psycopg 'binary' implementation: No module named 'psycopg_binary'`\n- `couldn't import psycopg 'python' implementation: libpq library not found`\n\n### Solution:\n\n1. **Check Python Version:**\n   \n   Ensure your Python version is at least 3.9. The `'psycopg2-binary'` might fail to install on older versions like 3.7.3.\n   \n   ```bash\n   $ python -V\n   ```\n\n2. **Environment Setup:**\n\n   - If your Python version is not 3.9, create a new environment:\n     \n     ```bash\n     $ conda create --name de-zoomcamp python=3.9\n     $ conda activate de-zoomcamp\n     ```\n\n3. **Install Required Libraries:**\n\n   - Install Postgres libraries:\n     \n     ```bash\n     $ pip install psycopg2-binary\n     $ pip install psycopg_binary\n     ```\n\n4. **Upgrade pgcli:**\n\n   - If the above steps do not work, try upgrading `pgcli`:\n     \n     ```bash\n     $ pip install --upgrade pgcli\n     ```\n\n5. **Install pgcli via Conda:**\n\n   - Make sure to also install `pgcli` using conda:\n     \n     ```bash\n     $ conda install -c conda-forge pgcli\n     ```\n\nIf you follow these steps, you should be able to resolve the issue."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/064_6704978d67_pgcli-stuck-on-password-prompt.md", "metadata": {"id": "6704978d67", "images": [{"description": "image #1", "id": "image_1", "path": "images/data-engineering-zoomcamp/image_93f08019.png"}], "question": "PGCLI - stuck on password prompt", "sort_order": 64}, "content": "If your Bash prompt is stuck on the password command for postgres:\n\n<{IMAGE:image_1}>\n\nUse `winpty`:\n\n```bash\nwinpty pgcli -h localhost -p 5432 -u root -d ny_taxi\n```\n\nAlternatively, try using Windows Terminal or the terminal in VS Code."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/065_12dffd5b1a_pgcli-connection-failed-fatal-password-authenticat.md", "metadata": {"id": "12dffd5b1a", "images": null, "question": "PGCLI -connection failed: FATAL: password authentication failed for user \"root\"", "sort_order": 65}, "content": "The error above was faced continually despite inputting the correct password.\n\n\n1. **Stop the PostgreSQL service on Windows**\n\n2. **Using WSL:** Completely uninstall PostgreSQL 12 from Windows and install `postgresql-client` on WSL:  \n\n```bash\nsudo apt install postgresql-client-common postgresql-client libpq-dev\n```\n\n3. **Change the port of the Docker container**\n\n4. **Keep the Database Connection:**\n   \nIf you encounter the error:\n   \n```\nPGCLI -connection failed: FATAL: password authentication failed for user \"root\"\n```\n   \nIt might be because the connection to the `Postgres:13` image was closed. Ensure you keep the database connected in order to continue with the tutorial steps, using the following command:\n\n```bash\ndocker run -it \\\n   -e POSTGRES_USER=root \\\n   -e POSTGRES_PASSWORD=root \\\n   -e POSTGRES_DB=ny_taxi \\\n   -v d:/git/data-engineering-zoomcamp/week_1/docker_sql/ny_taxi_postgres_data:/var/lib/postgresql/data \\\n   -p 5432:5432 \\\n   postgres:13\n```\n\nYou should see this \n\n```\n2024-01-26 20:14:43.124 UTC [1] LOG:  database system is ready to accept connections\n```\n\n5. **Change the Port for Docker PostgreSQL:**\n\n   After running the command `pgcli -h localhost -p 5432 -u root -d ny_taxi`, if prompted for a password, the error may persist due to local Postgres installation. To resolve this port conflict between host and container:\n\n   - Configure your Docker PostgreSQL container to use a different port. Map it to a different port on your host machine:\n   \n```bash\ndocker run -it \\\n   -e POSTGRES_USER=\"root\" \\\n   -e POSTGRES_PASSWORD=\"root\" \\\n   -e POSTGRES_DB=\"ny_taxi\" \\\n   -v c:/workspace/de-zoomcamp/1_intro_to_data_engineering/docker_sql/ny_taxi_postgres_data:/var/lib/postgresql/data \\\n   -p 5433:5432 \\\n   postgres:13\n```\n\n- `5433` refers to the port on the host machine.\n- `5432` refers to the port inside the Docker Postgres container."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/066_aa8c30b777_pgcli-pgcli-command-not-found.md", "metadata": {"id": "aa8c30b777", "images": [{"description": "image #1", "id": "image_1", "path": "images/data-engineering-zoomcamp/image_b33cbd22.png"}], "question": "PGCLI - pgcli: command not found", "sort_order": 66}, "content": "### Problem\n\nIf you have already installed `pgcli` but Bash or the Windows Terminal doesn't recognize the command:\n\n- On Git Bash: \n  ```bash\n  bash: pgcli: command not found\n  ```\n- On Windows Terminal: \n  ```\n  pgcli: The term 'pgcli' is not recognized…\n  ```\n\n### Solution\n\nTry adding the Python path to the Windows PATH variable:\n\n1. Use the command to get the location:\n   ```bash\n   pip list -v\n   ```\n2. Copy the path, which looks like:\n   ```\n   C:\\Users\\...\\AppData\\Roaming\\Python\\Python39\\site-packages\n   ```\n3. Replace `site-packages` with `Scripts`:\n   ```\n   C:\\Users\\...\\AppData\\Roaming\\Python\\Python39\\Scripts\n   ```\n\nIt might be that Python is installed elsewhere. For example, it could be under:\n\n- `c:\\python310\\lib\\site-packages`\n\nIn that case, you should add:\n\n- `c:\\python310\\lib\\Scripts` to PATH.\n\n### Instructions\n\n- Add the determined path to `Path` (or `PATH`) in System Variables.\n\n<{IMAGE:image_1}>\n\n### Reference\n\n[Stack Overflow Reference](https://stackoverflow.com/a/68233660)"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/067_c6a6a991d8_pgcli-running-in-a-docker-container.md", "metadata": {"id": "c6a6a991d8", "question": "PGCLI - running in a Docker container", "sort_order": 67}, "content": "If running pgcli locally causes issues or you do not want to install it on your machine, you can use it within a Docker container instead.\n\nBelow is the usage with values used in the course videos for:\n\n- Network name (Docker network)\n- Postgres-related variables for pgcli\n- Hostname\n- Username\n- Port\n- Database name\n\n```bash\ndocker run -it --rm --network pg-network ai2ys/dockerized-pgcli:4.0.1\n```\n\nThen execute the following pgcli command:\n\n```sql\npgcli -h pg-database -U root -p 5432 -d ny_taxi\n```\n\nYou'll be prompted for the password for the user `root`.\n\nExample Output:\n\n```\nServer: PostgreSQL 16.1 (Debian 16.1-1.pgdg120+1)\nVersion: 4.0.1\nHome: [pgcli.com](http://pgcli.com)\n```\n\nTo list tables:\n\n```sql\nroot@pg-database:ny_taxi> \\dt\n\n+--------+------------------+-------+-------+\n| Schema | Name             | Type  | Owner |\n|--------+------------------+-------+-------|\n| public | yellow_taxi_data | table | root  |\n+--------+------------------+-------+-------+\n\nSELECT 1\nTime: 0.009s\nroot@pg-database:ny_taxi>\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/068_45ffd3e213_rrpgcli-case-sensitive-use-of-quotations-around-co.md", "metadata": {"id": "45ffd3e213", "question": "RRPGCLI: Case sensitive use of “Quotations” around columns with capital letters", "sort_order": 68}, "content": "`PULocationID` will not be recognized, but `\"PULocationID\"` will be. This is because unquoted identifiers are case insensitive. [See docs](https://www.postgresql.org/docs/current/sql-syntax-lexical.html#SQL-SYNTAX-IDENTIFIERS)."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/069_0a6a3ccf35_pgcli-error-column-crelhasoids-does-not-exist.md", "metadata": {"id": "0a6a3ccf35", "question": "PGCLI - error column c.relhasoids does not exist", "sort_order": 69}, "content": "When using the command `\\d <database name>` you get the error `column c.relhasoids does not exist`.\n\nResolution:\n\n1. Uninstall pgcli.\n2. Reinstall pgcli.\n3. Restart your PC."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/070_f291e8d311_postgres-bind-address-already-in-use.md", "metadata": {"id": "f291e8d311", "question": "Postgres: bind: address already in use", "sort_order": 70}, "content": "### Issue\n\nWhen attempting to start the Docker Postgres container, you may encounter the error message:\n\n```\nError - postgres port is already in use.\n```\n\n### Solutions\n\n#### Option 1: Identify and Stop the Service\n\n1. Determine which service is using the port by running:\n   \n   ```bash\n   sudo lsof -i :5432\n   ```\n   \n2. Stop the service that is using the port:\n   \n   ```bash\n   sudo service postgresql stop\n   ```\n\n#### Option 2: Map to a Different Port\n\nFor a more long-term solution, consider mapping to a different port:\n\n- Map local port 5433 to container port 5432 in your Docker configuration (`Dockerfile` or `docker-compose.yml`).\n- If using a VM, ensure that port 5433 is forwarded in the host machine configuration.\n\nThis approach prevents conflicts and allows the Docker Postgres container to run without interruption."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/071_7f7aa5f5e6_pgcli-after-installing-pgcli-and-checking-with-pgc.md", "metadata": {"id": "7f7aa5f5e6", "question": "PGCLI - After installing PGCLI and checking with `pgcli --help` we get the error: `ImportError: no pq wrapper available`", "sort_order": 71}, "content": "The error persists because the psycopg library cannot find the required libpq library. Ensure the required PostgreSQL client library is installed:\n\n```bash\nsudo apt install libpq-dev\n```\n\nRebuild psycopg:\n\n1. Uninstall the existing packages:\n   \n   ```bash\n   pip uninstall psycopg psycopg_binary psycopg_c -y\n   ```\n\n2. Reinstall psycopg:\n   \n   ```bash\n   pip install psycopg --no-binary psycopg\n   ```\n\nThe issue should be resolved by now. However, if you still encounter the error:\n\n`ModuleNotFoundError: No module named 'psycopg2'`\n\nThen run the following:\n\n```bash\npip install psycopg2-binary\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/072_409296db3e_postgres-operationalerror-psycopg2operationalerror.md", "metadata": {"id": "409296db3e", "question": "Postgres - OperationalError: (psycopg2.OperationalError) connection to server at \"localhost\" (::1), port 5432 failed: FATAL: password authentication failed for user \"root\"", "sort_order": 72}, "content": "This error occurs when uploading data via a connection in Jupyter Notebook:\n\n```python\nengine = create_engine('postgresql://root:root@localhost:5432/ny_taxi')\n```\n\n### Possible Solutions:\n\n1. **Port Conflict:**\n   - Port 5432 might be occupied by another Postgres installation on your local machine. This can lead to your connection not reaching Docker.\n   - Try using a different port, such as 5431, or verify the port mapping.\n   - Alternatively, remove any old or unnecessary Postgres installations if they're not in use.\n\n2. **Windows Service Check:**\n   - Check for any running services on Windows that might be using Postgres.\n   - Stopping such services might resolve the issue."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/073_60c212c0e8_postgres-connection-failed-connection-to-server-at.md", "metadata": {"id": "60c212c0e8", "question": "Postgres: connection failed: connection to server at \"127.0.0.1\", port 5432 failed: FATAL: password authentication failed for user \"root\"", "sort_order": 73}, "content": "To resolve the issue of a failed connection to PostgreSQL due to password authentication, consider the following steps:\n\n- **Check Port Usage**: Ensure that port 5432 is properly forwarded. If it is being used by another process, follow these steps to kill it:\n  \n  ```bash\n  sudo lsof -i :5432\n  sudo kill -9 PID\n  ```\n\n- **For Windows Users**: If PostgreSQL is running locally and pgAdmin4 is using the 5432 port, follow these instructions:\n  \n  1. Press **Win + R** to open the Run dialog.\n  2. Type `services.msc` and press Enter.\n  3. In the Services window, scroll down to find a service named like `PostgreSQL`, `postgresql-x64-13`, or similar, depending on your PostgreSQL version.\n  4. Right-click the PostgreSQL service and select **Stop**."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/074_postgres-operationalerror.md", "metadata": {"id": "3459487271", "question": "Postgres - OperationalError: (psycopg2.OperationalError) connection to server at \"localhost\" (::1), port 5432 failed: FATAL:  role \"root\" does not exist", "sort_order": 74}, "content": "This error can occur in the following scenarios:\n\n- **Using `pgcli`**:\n  ```bash\n  pgcli -h localhost -p 5432 -U root -d ny_taxi\n  ```\n- **Uploading data via a connection in a Jupyter notebook**:\n  ```python\n  engine = create_engine('postgresql://root:root@localhost:5432/ny_taxi')\n  ```\n\n### Solutions:\n\n1. **Port Change**:\n   - Change the port from 5432 to another port (e.g., 5431).\n   - Example: Change `5432:5432` to `5431:5432`.\n\n2. **User Change**:\n   - Change `POSTGRES_USER=root` to `PGUSER=postgres`.\n\n3. **Docker Solution**:\n   - Run `docker compose down`.\n   - Remove the folder containing the Postgres volume.\n   - Run `docker compose up` again.\n\n### Additional Resources:\nFor more details, refer to [this Stack Overflow discussion](https://stackoverflow.com/questions/60193781/postgres-with-docker-compose-gives-fatal-role-root-does-not-exist-error)."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/075_6f27b71a1f_postgres-operationalerror-psycopg2operationalerror.md", "metadata": {"id": "6f27b71a1f", "question": "Postgres - OperationalError: (psycopg2.OperationalError) connection to server at \"localhost\" (::1), port 5432 failed: FATAL:  database \"ny_taxi\" does not exist", "sort_order": 75}, "content": "```\nOperationalError: (psycopg2.OperationalError) connection to server at \"localhost\" (::1), port 5432 failed: FATAL:  database \"ny_taxi\" does not exist\n```\n\nMake sure PostgreSQL is running. You can check that by running:\n\n```bash\ndocker ps\n```\n\n**Solution:**\n\n- If you have PostgreSQL software installed on your computer previously, consider building your instance on a different port, such as 8080, instead of 5432."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/076_52858dfd98_postgres-modulenotfounderror-no-module-named-psyco.md", "metadata": {"id": "52858dfd98", "images": [{"description": "image #1", "id": "image_1", "path": "images/data-engineering-zoomcamp/image_b7e005cb.png"}, {"description": "image #2", "id": "image_2", "path": "images/data-engineering-zoomcamp/image_c56a8539.png"}], "question": "Postgres - ModuleNotFoundError: No module named 'psycopg2'", "sort_order": 76}, "content": "Issue:\n\n```\nModuleNotFoundError: No module named 'psycopg2'\n```\n\n<IMAGE:image_1>\n\n<IMAGE:image_2>\n\nSolution:\n\n1. Install psycopg2-binary:\n   \n   ```bash\n   pip install psycopg2-binary\n   ```\n\n2. If psycopg2-binary is already installed, update it:\n   \n   ```bash\n   pip install psycopg2-binary --upgrade\n   ```\n\n3. Other methods if the above fails:\n\n   - If the error persists, update conda:\n     \n     ```bash\n     conda update -n base -c defaults conda\n     ```\n\n   - Alternatively, update pip:\n     \n     ```bash\n     pip install --upgrade pip\n     ```\n\n   - Reinstall psycopg:\n     \n     - Uninstall the psycopg package.\n     - Update conda or pip.\n     - Reinstall psycopg using pip.\n\n   - If an error shows about `pg_config` not being found, install PostgreSQL:\n     \n     - On Mac, use:\n       \n       ```bash\n       brew install postgresql\n       ```"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/077_4277a9cb86_postgres-column-does-not-exist-but-it-actually-doe.md", "metadata": {"id": "4277a9cb86", "question": "Postgres: \"Column does not exist\" but it actually does (Pyscopg2 error in MacBook Pro M2)", "sort_order": 77}, "content": "In join queries, if you mention the column name directly or enclose it in single quotes, you'll encounter an error saying \"column does not exist\".\n\n**Solution:** Enclose the column names in double quotes, and it will work correctly."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/078_117164c439_pgadmin-create-server-dialog-does-not-appear.md", "metadata": {"id": "117164c439", "question": "pgAdmin: Create server dialog does not appear", "sort_order": 78}, "content": "pgAdmin has a new version. The create server dialog may not appear. Try using `Register` -> `Server` instead."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/079_0cfd7f7ed6_pgadmin-blankwhite-screen-after-login-browser.md", "metadata": {"id": "0cfd7f7ed6", "question": "pgAdmin - Blank/white screen after login (browser)", "sort_order": 79}, "content": "Using GitHub Codespaces in the browser resulted in a blank screen after logging into pgAdmin (running in a Docker container). The terminal of the pgAdmin container was showing the following error message:\n\n```\nCSRFError: 400 Bad Request: The referrer does not match the host.\n```\n\n### Solution #1:\n\nAs recommended in the following issue: [GitHub Issue #5432](https://github.com/pgadmin-org/pgadmin4/issues/5432), setting the following environment variable solved it:\n\n```bash\ndocker run --rm -it \\\n  -e PGADMIN_DEFAULT_EMAIL=\"admin@admin.com\" \\\n  -e PGADMIN_DEFAULT_PASSWORD=\"root\" \\\n  -e PGADMIN_CONFIG_WTF_CSRF_ENABLED=\"False\" \\\n  -p \"8080:80\" \\\n  --name pgadmin \\\n  --network=pg-network \\\n  dpage/pgadmin4:8.2\n```\n\n### Solution #2:\n\nUsing the locally installed VSCode to display GitHub Codespaces. When using GitHub Codespaces in the locally installed VSCode (opening a Codespace or creating/starting one), this issue did not occur."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/080_2efd03d7f8_pgadmin-can-not-accessopen-the-pgadmin-address-via.md", "metadata": {"id": "2efd03d7f8", "question": "pgAdmin - Can not access/open the PgAdmin address via browser", "sort_order": 80}, "content": "I am using a Mac Pro device and connect to the GCP Compute Engine via Remote SSH - VSCode. But when trying to run the PgAdmin container via `docker run` or `docker compose`, I couldn't access the PgAdmin address via my browser. After modifications, I was able to access it.\n\n### Solution #1:\n\nModify the `docker run` command:\n\n```bash\ndocker run --rm -it \\\n  -e PGADMIN_DEFAULT_EMAIL=\"admin@admin.com\" \\\n  -e PGADMIN_DEFAULT_PASSWORD=\"pgadmin\" \\\n  -e PGADMIN_CONFIG_WTF_CSRF_ENABLED=\"False\" \\\n  -e PGADMIN_LISTEN_ADDRESS=0.0.0.0 \\\n  -e PGADMIN_LISTEN_PORT=5050 \\\n  -p 5050:5050 \\\n  --network=de-zoomcamp-network \\\n  --name pgadmin-container \\\n  --link postgres-container \\\n  -t dpage/pgadmin4\n```\n\n### Solution #2:\n\nModify the `docker-compose.yaml` configuration and use the `docker compose up` command:\n\n```yaml\npgadmin:\n  image: dpage/pgadmin4\n  container_name: pgadmin-container\n  environment:\n    - PGADMIN_DEFAULT_EMAIL=admin@admin.com\n    - PGADMIN_DEFAULT_PASSWORD=pgadmin\n    - PGADMIN_CONFIG_WTF_CSRF_ENABLED=False\n    - PGADMIN_LISTEN_ADDRESS=0.0.0.0\n    - PGADMIN_LISTEN_PORT=5050\n  volumes:\n    - \"./pgadmin_data:/var/lib/pgadmin/data\"\n  ports:\n    - \"5050:5050\"\n  networks:\n    - de-zoomcamp-network\n  depends_on:\n    - postgres-container\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/081_ca3b8ac8db_pgadmin-how-to-persist-pgadmin-configurations.md", "metadata": {"id": "ca3b8ac8db", "question": "pgAdmin: How to Persist pgAdmin Configurations", "sort_order": 81}, "content": "To keep pgAdmin settings after restarting the container, follow these steps:\n\n1. Create the directory for pgAdmin data:\n   \n   ```bash\n   mkdir -p /path/to/pgadmin-data\n   ```\n\n2. Assign ownership to pgAdmin's user (ID 5050):\n   \n   ```bash\n   sudo chown -R 5050:5050 /path/to/pgadmin-data\n   ```\n\n3. Set the appropriate permissions:\n   \n   ```bash\n   sudo chmod -R 755 /path/to/pgadmin-data\n   ```"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/082_3124f13c7c_pgadmin-unable-to-connect-to-server-errno-3-try-ag.md", "metadata": {"id": "3124f13c7c", "question": "pgAdmin - Unable to connect to server: [Errno -3] Try again", "sort_order": 82}, "content": "This error occurs when connecting pgAdmin with Docker Postgres. In the tutorial, the pgAdmin server creation under **Connection > Host name/address** uses `pg-database` and results in the above-mentioned error when saved.\n\n### Solution 1:\n\n- Verify that both containers are connected to `pg-network`:\n  \n  ```bash\n  docker network inspect pg-network\n  ```\n\n- If the Docker Postgres container is not connected, connect it to `pg-network`:\n\n  ```bash\n  docker network connect pg-network postgresContainer_name\n  ```\n  \n- Retry the connection. If the error persists, instead of using `pg-database` under **Connection > Host name/address**, try using the IP Address:\n\n  - Use the IP address of the `postgresContainer_name` container (e.g., `172.19.0.3`) in the pgAdmin configuration instead of the container name or `pg-database`."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/083_646e2067e8_python-modulenotfounderror-no-module-named-pysqlit.md", "metadata": {"id": "646e2067e8", "question": "Python - ModuleNotFoundError: No module named 'pysqlite2'", "sort_order": 83}, "content": "```\nImportError: DLL load failed while importing _sqlite3: The specified module could not be found. \nModuleNotFoundError: No module named 'pysqlite2'\n```\n\nThe issue may arise due to the absence of `sqlite3.dll` in the path `\".\\Anaconda\\Dlls\\\"`.\n\nTo resolve the issue:\n\n1. Copy the `sqlite3.dll` file from `\\Anaconda3\\Library\\bin`.\n2. Paste the file into the `\".\\Anaconda\\Dlls\\\"` directory.\n\nThis solution applies if you are using Anaconda."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/084_0d6dc0d041_python-ingestion-with-jupyter-notebook-missing-100.md", "metadata": {"id": "0d6dc0d041", "question": "Python: Ingestion with Jupyter notebook - missing 100000 records", "sort_order": 84}, "content": "If you follow the video [1.2.2 - Ingesting NY Taxi Data to Postgres](https://www.youtube.com/watch?v=2JM-ziJt0WI&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=5) and execute the same steps, you will ingest all the data (~1.3 million rows) into the table `yellow_taxi_data`. However, running the whole script in the Jupyter notebook for a second time from top to bottom will result in missing the first chunk of 100,000 records. This occurs because a call to the iterator appears before the while loop, leading to the second chunk being ingested first.\n\n### Solution:\n\n- Remove the cell `df=next(df_iter)` located higher up in the notebook than the while loop.\n- Ensure the first `w(df_iter)` call is within the while loop.\n\n📔 **Note:** The notebook is used to test the code and is not intended to be run top to bottom. The logic is organized in a later step when inserted into a `.py` file for the pipeline."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/085_5b4c133384_ipython-pandas-parsing-dates-with-read_csv.md", "metadata": {"id": "5b4c133384", "question": "IPython - Pandas parsing dates with \"read_csv\"", "sort_order": 85}, "content": "Pandas can interpret \"string\" column values as \"datetime\" directly when reading the CSV file using `pd.read_csv` with the `parse_dates` parameter. This can include a list of column names or column indices, eliminating the need for conversion afterward.\n\n[Reference: pandas.read_csv documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html)\n\n**Example from Week 1:**\n\n```python\nimport pandas as pd\n\ndf = pd.read_csv(\n    'yellow_tripdata_2021-01.csv',\n    nrows=100,\n    parse_dates=['tpep_pickup_datetime', 'tpep_dropoff_datetime']\n)\n\ndf.info()\n```\n\n**Output:**\n\n```\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 100 entries, 0 to 99\nData columns (total 18 columns):\n #   Column                 Non-Null Count  Dtype          \n---  ------                 --------------  -----          \n 0   VendorID               100 non-null    int64          \n 1   tpep_pickup_datetime   100 non-null    datetime64[ns] \n 2   tpep_dropoff_datetime  100 non-null    datetime64[ns] \n 3   passenger_count        100 non-null    int64          \n 4   trip_distance          100 non-null    float64        \n 5   RatecodeID             100 non-null    int64          \n 6   store_and_fwd_flag     100 non-null    object         \n 7   PULocationID           100 non-null    int64          \n 8   DOLocationID           100 non-null    int64          \n 9   payment_type           100 non-null    int64          \n 10  fare_amount            100 non-null    float64        \n 11  extra                  100 non-null    float64        \n 12  mta_tax                100 non-null    float64        \n 13  tip_amount             100 non-null    float64        \n 14  tolls_amount           100 non-null    float64        \n 15  improvement_surcharge  100 non-null    float64        \n 16  total_amount           100 non-null    float64        \n 17  congestion_surcharge   100 non-null    float64        \ndtypes: datetime64[ns](2), float64(9), int64(6), object(1)\nmemory usage: 14.2+ KB\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/086_e80e8216d5_python-python-cant-ingest-data-from-the-github-lin.md", "metadata": {"id": "e80e8216d5", "question": "Python: Python can't ingest data from the GitHub link provided using curl", "sort_order": 86}, "content": "```python\nos.system(f\"curl -LO {url} -o {csv_name}\")\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/087_e0c1900c47_python-pandas-can-read-csvgzip.md", "metadata": {"id": "e0c1900c47", "question": "Python: Pandas can read *.csv.gzip", "sort_order": 87}, "content": "When a CSV file is compressed using Gzip, it is saved with a \".csv.gz\" file extension. This file type is also known as a Gzip compressed CSV file. To read a Gzip compressed CSV file using Pandas, you can use the `read_csv()` function.\n\nHere is an example of how to read a Gzip compressed CSV file using Pandas:\n\n```python\nimport pandas as pd\n\ndf = pd.read_csv('file.csv.gz',\n                 compression='gzip',\n                 low_memory=False)\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/088_fbfa475350_python-how-to-iterate-through-and-ingest-parquet-f.md", "metadata": {"id": "fbfa475350", "question": "Python: How to iterate through and ingest parquet file", "sort_order": 88}, "content": "Contrary to pandas’ `read_csv` method, there’s no simple way to iterate through and set chunksize for parquet files. We can use PyArrow (Apache Arrow Python bindings) to resolve that.\n\n```python\nimport pyarrow.parquet as pq\nfrom sqlalchemy import create_engine\nimport time\n\noutput_name = \"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-01.parquet\"\n\nparquet_file = pq.ParquetFile(output_name)\nparquet_size = parquet_file.metadata.num_rows\n\nengine = create_engine(f'postgresql://{user}:{password}@{host}:{port}/{db}')\n\ntable_name = \"yellow_taxi_schema\"\n\n# Clear table if exists\npq.read_table(output_name).to_pandas().head(n=0).to_sql(name=table_name, con=engine, if_exists='replace')\n\n# Default (and max) batch size\nindex = 65536\n\nfor i in parquet_file.iter_batches(use_threads=True):\n    t_start = time.time()\n    print(f'Ingesting {index} out of {parquet_size} rows ({index / parquet_size:.0%})')\n    i.to_pandas().to_sql(name=table_name, con=engine, if_exists='append')\n    index += 65536\n    t_end = time.time()\n    print(f'\\t- it took %.1f seconds' % (t_end - t_start))\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/089_26ff32cb35_python-sqlalchemy-importerror-cannot-import-name-t.md", "metadata": {"id": "26ff32cb35", "question": "Python - SQLAlchemy - ImportError: cannot import name 'TypeAliasType' from 'typing_extensions'.", "sort_order": 89}, "content": "The following error occurs during the execution of a Jupyter notebook cell:\n\n```python\nfrom sqlalchemy import create_engine\n```\n\nSolution:\n\nThe issue can be resolved by ensuring the version of the Python module `typing_extensions` is 4.6.0 or later. You can update it using either Conda or pip:\n\n- **Using Conda:**\n  ```bash\n  conda update typing_extensions\n  ```\n\n- **Using pip:**\n  ```bash\n  pip install --upgrade typing_extensions\n  ```\n\nFor more details, you can refer to the [changelog for typing_extensions 4.6.0](https://github.com/python/typing_extensions/blob/main/CHANGELOG.md#release-460-may-22-2023)."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/090_5d8ffd5be5_python-sqlalchemy-typeerror-module-object-is-not-c.md", "metadata": {"id": "5d8ffd5be5", "question": "Python - SQLALchemy - TypeError 'module' object is not callable", "sort_order": 90}, "content": "When using `create_engine('postgresql://root:root@localhost:5432/ny_taxi')`, you may encounter the error:\n\n```\nTypeError: 'module' object is not callable\n```\nUse the correct connection string syntax:\n\n\n```python\nconn_string = \"postgresql+psycopg://root:root@localhost:5432/ny_taxi\"\nengine = create_engine(conn_string)\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/091_e6e9a25246_python-sqlalchemy-modulenotfounderror-no-module-na.md", "metadata": {"id": "e6e9a25246", "question": "Python: SQLAlchemy - ModuleNotFoundError: No module named 'psycopg2'.", "sort_order": 91}, "content": "Error raised during the Jupyter Notebook cell execution:\n\n```python\nengine = create_engine('postgresql://root:root@localhost:5432/ny_taxi')\n```\n\nSolution:\n\nInstall the Python module `psycopg2`. It can be installed using Conda or pip:\n\n- Using Conda:\n  ```bash\n  conda install psycopg2\n  ```\n- Using pip:\n  ```bash\n  pip install psycopg2\n  ```"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/092_1f21373dff_python-sqlalchemy-nosuchmoduleerror-cant-load-plug.md", "metadata": {"id": "1f21373dff", "question": "Python - SQLAlchemy: NoSuchModuleError: Can't load plugin: sqlalchemy.dialects:postgresql.psycopg", "sort_order": 92}, "content": "Error raised during the Jupyter notebook’s cell execution:\n\n```python\nconn_string = \"postgresql+psycopg://root:root@localhost:5432/ny_taxi\"\n\nengine = create_engine(conn_string)\n```\n\nSolution: \n\nWe had a scenario of a virtual environment (created by PyCharm) being run on top of another virtual environment (on conda). The solution was:\n\n1. Remove the `.venv`.\n2. Create a new virtual environment with conda:\n   \n   ```bash\n   conda create -n pyingest python=3.12\n   ```\n\n3. Install the required dependencies:\n   \n   ```bash\n   pip install pandas sqlalchemy psycopg2-binary jupyterlab\n   ```\n\n4. Re-execute the code.\n\nFor `psycopg2`, the connection string should be:\n\n```python\npostgresql+psycopg2://{db_user}:{db_password}@{db_host}:{db_port}/{db_name}\n```\n\nReference - Kayla Tinker 1/14/25"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/093_5f8cf90bb4_python-sqlalchemy-read_sql_query-throws-optionengi.md", "metadata": {"id": "5f8cf90bb4", "question": "Python - SQLAlchemy - read_sql_query() throws \"'OptionEngine' object has no attribute 'execute'\"", "sort_order": 93}, "content": "First, check the versions of SQLAlchemy and Pandas to ensure they are both up-to-date. You can upgrade them using `pip` or `conda` if needed.\n\nThen, try to wrap the query using `text`:\n\n```python\nfrom sqlalchemy import text\n\nquery = text(\"SELECT * FROM tbl\")\ndf = pd.read_sql_query(query, conn)\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/094_bd56924fcb_gcp-static-vs-ephemeral-ip-setting-up-static-ip-fo.md", "metadata": {"id": "bd56924fcb", "question": "GCP: Static vs Ephemeral IP / Setting up static IP for VM", "sort_order": 94}, "content": "When you set up a VM in Google Cloud Platform (GCP), it initially uses an ephemeral IP address, which changes each time you start or stop the VM. If you need a consistent IP for your configuration file, you should set up a static IP address.\n\n### Steps to Set Up a Static IP Address\n\n1. Navigate to **VPC Network** > **IP addresses** in the GCP console.\n2. Allocate a new static IP address.\n3. Attach the static IP to your VM instance.\n\n> **Note:** You are charged for a static IP if it is not allocated to a specific VM, so make sure it is attached to avoid extra fees.\n\nFor detailed instructions, consult the [GCP documentation](https://cloud.google.com/)."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/095_25b0348672_gcp-unable-to-add-google-cloud-sdk-path-to-windows.md", "metadata": {"id": "25b0348672", "question": "GCP: Unable to add Google Cloud SDK PATH to Windows", "sort_order": 95}, "content": "### Issue\n\nWindows error:\n\n```\nThe installer is unable to automatically update your system PATH. Please add C:\\tools\\google-cloud-sdk\\bin\n```\n\n### Solution\n\nIf you encounter this error frequently, consider the following steps:\n\n1. **Add Gitbash to Windows Path:**\n   \n   - **Using Conda:**\n     - Download the Anaconda Navigator.\n     - During installation, check the box to add Conda to the path (even though it's not recommended).\n\n2. **Install Git Bash:**\n\n   - If not installed, install Git Bash.\n   - If installed, consider reinstalling it.\n   - During installation, ensure you check:\n     - Add GitBash to Windows Terminal\n     - Use Git and optional Unix tools from the command prompt\n\n3. **Setup Git Bash:**\n\n   - Open Git Bash and type the following command:\n     \n     ```bash\n     conda init bash\n     ```\n   \n   - This will modify your bash profile.\n\n4. **Set Gitbash as Default Terminal:**\n\n   - Open the Windows Terminal.\n   - Go to settings.\n   - Change the default profile from Windows PowerShell to Git Bash.\n\nBy following these steps, you should be able to add the Google Cloud SDK path to your system on Windows without issues."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/096_bad6a11a5a_gcp-project-creation-failed-httperror-accessing-re.md", "metadata": {"id": "bad6a11a5a", "question": "GCP: Project creation failed: HttpError accessing … Requested entity already exists", "sort_order": 96}, "content": "When creating a project in GCP, you may encounter the following error:\n\n```json\nWARNING: Project creation failed: HttpError accessing cloudresourcemanager.googleapis.com: response: {\n  'content-type': 'application/json; charset=UTF-8',\n  'status': 409\n}, content {\n  \"error\": {\n    \"code\": 409,\n    \"message\": \"Requested entity already exists\",\n    \"status\": \"ALREADY_EXISTS\"\n  }\n}\n```\n\n### Explanation\n\nThis error occurs when the project ID you are trying to use is already taken. Project IDs are unique across all GCP projects. If any user ever had a project with that ID, you cannot use it.\n\n### Solution\n\n- Choose a different, more unique project ID. Avoid common names like `testproject` as they are likely to be already in use.\n\nFor more details, refer to the discussion: [Stack Overflow](https://stackoverflow.com/questions/52561383/gcloud-cli-cannot-create-project-the-project-id-you-specified-is-already-in-us?rq=1)"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/097_bc90da941d_gcp-the-project-to-be-billed-is-associated-with-an.md", "metadata": {"id": "bc90da941d", "question": "GCP: The project to be billed is associated with an absent billing account", "sort_order": 97}, "content": "If you receive the error:\n\n```\nError 403: The project to be billed is associated with an absent billing account., accountDisabled\n```\n\nIt is most likely because you did not enter your project ID correctly. The value you enter should be unique to your project. You can find this value on your GCP Dashboard when you log in.\n\nAnother possibility is that you have not linked your billing account to your current project."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/098_53b423d784_gcp-or-cbat-15-error-google-cloud-free-trial-accou.md", "metadata": {"id": "53b423d784", "images": [{"description": "image #1", "id": "image_1", "path": "images/data-engineering-zoomcamp/image_993adb67.png"}], "question": "GCP: OR-CBAT-15 ERROR Google cloud free trial account", "sort_order": 98}, "content": "If Google refuses your credit/debit card, try using a different one. For instance, a card from Kaspi (Kazakhstan) might not work, but a card from TBC (Georgia) does.\n\nUnfortunately, support assistance might not be highly effective in resolving this issue.\n\nAdditionally, a Pyypl web-card can be a viable alternative.\n\n```json\nny-rides.json\n```\n\n<{IMAGE:image_1}>"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/099_cdd91ef84a_gcp-where-can-i-find-the-ny-ridesjson-file.md", "metadata": {"id": "cdd91ef84a", "question": "GCP: Where can I find the “ny-rides.json” file?", "sort_order": 99}, "content": "The `ny-rides.json` is your private file in Google Cloud Platform (GCP). Here’s how to find it:\n\n- Navigate to GCP and select the project with your instance.\n- Go to **IAM & Admin**.\n- Select the **Service Accounts** tab.\n- Click the **Keys** tab.\n- Add a key, choosing **JSON** as the key type, then click **Create**.\n\n**Note:** Once in the Service Accounts, click the email associated to access the **KEYS** tab where you can add a key as a JSON key type."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/100_a1f6167da1_gcp-failed-to-load-when-accessing-compute-engines.md", "metadata": {"id": "a1f6167da1", "question": "GCP: \"Failed to load\" when accessing Compute Engine’s metadata section (e.g., to add a SSH key)", "sort_order": 100}, "content": "You likely didn’t enable the [Compute Engine API](https://console.cloud.google.com/marketplace/details/google/compute.googleapis.com)."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/101_a697cb2dee_gcp-do-i-need-to-delete-my-instance-in-google-clou.md", "metadata": {"id": "a697cb2dee", "question": "GCP: Do I need to delete my instance in Google Cloud?", "sort_order": 101}, "content": "[In this lecture](https://www.youtube.com/watch?v=ae-CV2KfoN0&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb), Alexey deleted his instance in Google Cloud. Do I have to do it?\n\nNo, do not delete your instance in Google Cloud Platform. Otherwise, you will have to set it up again for the week 1 readings."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/102_eb759226aa_gcp-ssh-public-key-error-multiple-users-usernames.md", "metadata": {"id": "eb759226aa", "images": [{"description": "image #1", "id": "image_1", "path": "images/data-engineering-zoomcamp/image_31ceb9bc.png"}], "question": "GCP: SSH public key error - multiple users / usernames", "sort_order": 102}, "content": "Initially, I could not SSH into my VM from my Windows laptop. I thought it was because I did not follow the tutorial exactly. Instead of generating the SSH key using MINGW/git bash with the Linux-style command, I did it in Command Prompt using the Windows-style command. I kept getting a public key error.\n\n**Permanent Solution:**\n\nIt turns out it wasn’t an issue with the key generation at all! The problem was with the username. I had given my SSH key a different username than what appeared in my VM (my Google account username). So, I had been trying to log in with `googleacctuser@[ipaddr]` instead of `mySSHuser@[ipaddr]`. Here's how I resolved it:\n\n1. Retraced my steps to check the SSH key setup in the GCP console, where it showed the user and SSH key.\n2. Changed the username to the correct one (googleacctuser) in my config file.\n3. Updated the config file and used `mySSHuser` to log in.\n\nNow, the issue was that I had created two users. I made all the installations and permissions on `googleacctuser`, not accessible from `mySSHuser`. Since I didn't need `mySSHuser`, I edited the SSH key to change the username at the end and updated the GCP console and config file accordingly.\n\nThen, I planned to delete the `mySSHuser` account in the VM terminal to keep things clean (though I got a bit attached, so I skipped this).\n\n**Temporary Solution:**\n\nBefore figuring out my issue, I used a shortcut by SSH'ing into the VM in the browser, which worked nicely for a while. But eventually, I needed to use VSCode.\n\n<{IMAGE:image_1}>"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/103_513410225e_gcp-virtual-machine-vm-size-slow-clean-up.md", "metadata": {"id": "513410225e", "question": "GCP: Virtual Machine (VM) Size, Slow, Clean Up", "sort_order": 103}, "content": "If you are progressing through the course and find that your VM is starting to become slow, you can run the following commands to inspect and detect areas where you can improve:\n\n**Recommended VM Size**\n\n- Start with a 60GB machine. A 30GB machine may not be sufficient, as you might need to restart the project with a larger size.\n\n**Commands to Inspect the Health of Your VM**\n\n- **System Resource Usage**\n  \n  ```bash\n  top\n  htop\n  ```\n  Shows real-time information about system resource usage, including CPU, memory, and processes.\n\n  ```bash\n  free -h\n  ```\n  Displays information about system memory usage and availability.\n\n  ```bash\n  df -h\n  ```\n  Shows disk space usage of file systems.\n\n  ```bash\n  du -h <directory>\n  ```\n  Displays disk usage of a specific directory.\n\n- **Running Processes**\n  \n  ```bash\n  ps aux\n  ```\n  Lists all running processes along with detailed information.\n\n- **Network**\n  \n  ```bash\n  ifconfig\n  ip addr show\n  ```\n  Shows network interface configuration.\n\n  ```bash\n  netstat -tuln\n  ```\n  Displays active network connections and listening ports.\n\n- **Hardware Information**\n  \n  ```bash\n  lscpu\n  ```\n  Displays CPU information.\n\n  ```bash\n  lsblk\n  ```\n  Lists block devices (disks and partitions).\n\n  ```bash\n  lshw\n  ```\n  Lists hardware configuration.\n\n- **User and Permissions**\n  \n  ```bash\n  who\n  ```\n  Shows who is logged on and their activities.\n\n  ```bash\n  w\n  ```\n  Displays information about currently logged-in users and their processes.\n\n- **Package Management**\n  \n  ```bash\n  apt list --installed\n  ```\n  Lists installed packages (for Ubuntu and Debian-based systems)."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/104_e34580b954_billing-billing-account-has-not-been-enabled-for-t.md", "metadata": {"id": "e34580b954", "images": [{"description": "image #1", "id": "image_1", "path": "images/data-engineering-zoomcamp/image_366e8371.png"}, {"description": "image #2", "id": "image_2", "path": "images/data-engineering-zoomcamp/image_ee2aa0ad.png"}], "question": "Billing: Billing account has not been enabled for this project. But you’ve done it indeed!", "sort_order": 104}, "content": "If you’ve got the error:\n\n```plaintext\nError: Error updating Dataset \"projects/<your-project-id>/datasets/demo_dataset\": googleapi: Error 403: Billing has not been enabled for this project. Enable billing at console.cloud.google.com. The default table expiration time must be less than 60 days, billingNotEnabled\n```\n\nbut you’ve set your billing account, try disabling billing for the project and enabling it again. This method has been successful for others.\n\n<{IMAGE:image_1}>\n\n<{IMAGE:image_2}>"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/105_8f9f91b4de_gcp-windows-google-cloud-sdk-install-issue.md", "metadata": {"id": "8f9f91b4de", "question": "GCP - Windows Google Cloud SDK install issue:", "sort_order": 105}, "content": "If you are encountering installation trouble with the Google Cloud SDK on Windows and receiving the following error:\n\n```\nThese credentials will be used by any library that requests Application Default Credentials (ADC).\n\nWARNING:\n\nCannot find a quota project to add to ADC. You might receive a \"quota exceeded\" or \"API not enabled\" error. Run $ gcloud auth application-default set-quota-project to add a quota project.\n```\n\nTry these steps:\n\n1. Reinstall the SDK using the unzip file \"install.bat\".\n2. Check the installation by running `gcloud version`.\n3. Run `gcloud init` to set up your project.\n4. Execute `gcloud auth application-default login`.\n\nFor detailed instructions, refer to the following guide: [Windows SDK Installation Guide](https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/week_1_basics_n_setup/1_terraform_gcp/windows.md)"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/106_f29d45f419_gcp-i-cannot-get-my-virtual-machine-to-start-becau.md", "metadata": {"id": "f29d45f419", "question": "GCP: I cannot get my Virtual Machine to start because GCP has no resources.", "sort_order": 106}, "content": "1. Click on your VM.\n2. Create an image of your VM.\n3. On the page of the image, tell GCP to create a new VM instance via the image.\n4. On the settings page, change the location."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/107_d8ebc91216_gcp-vm-is-it-necessary-to-use-a-gcp-vm-when-is-it.md", "metadata": {"id": "d8ebc91216", "question": "GCP VM: Is it necessary to use a GCP VM? When is it useful?", "sort_order": 107}, "content": "The reason this video about the GCP VM exists is that many students had problems configuring their environment. You can use your own environment if it works for you.\n\nAdvantages of using your own environment include:\n\n- **Commit Changes**: If you are working in a GitHub repository, you will be able to commit changes directly. In the VM, the repo is cloned via HTTPS, so it is not possible to commit directly, even if you are the owner of the repo."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/108_51bc7e280d_gcp-vm-mkdir-cannot-create-directory-ssh-permissio.md", "metadata": {"id": "51bc7e280d", "question": "GCP VM - mkdir: cannot create directory ‘.ssh’: Permission denied", "sort_order": 108}, "content": "If you encounter an error while trying to create a directory:\n\n```bash\nUser1@DESKTOP-PD6UM8A MINGW64 /\n\n$ mkdir .ssh\n\nmkdir: cannot create directory ‘.ssh’: Permission denied\n```\n\nThis error occurs because you are attempting to create the directory in the root folder (`/`).\n\nTo resolve this, create the directory in your home directory instead. Use the following steps:\n\n1. Navigate to your home directory using:\n   \n   ```bash\n   cd ~\n   ```\n\n2. Create the `.ssh` directory:\n   \n   ```bash\n   mkdir .ssh\n   ```\n\nFor further guidance, watch [this video](https://www.youtube.com/watch?v=ae-CV2KfoN0&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb)."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/109_1ba58ae0e0_gcp-vm-error-while-saving-the-file-in-vm-via-vs-co.md", "metadata": {"id": "1ba58ae0e0", "question": "GCP VM: Error while saving the file in VM via VS Code", "sort_order": 109}, "content": "```plaintext\nFailed to save '<file>': Unable to write file 'vscode-remote://ssh-remote+de-zoomcamp/home/<user>/data_engineering_course/week_2/airflow/dags/<file>' (NoPermissions (FileSystemError): Error: EACCES: permission denied, open '/home/<user>/data_engineering_course/week_2/airflow/dags/<file>')\n```\n\nTo resolve this issue, you need to change the owner of the files you are trying to edit via VS Code. Follow these steps:\n\n1. Connect to your VM using SSH.\n\n2. Run the following command to change the ownership:\n\n   ```bash\n   sudo chown -R <user> <path to your directory>\n   ```"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/110_a63d2734b2_gcp-vm-vm-connection-request-timeout.md", "metadata": {"id": "a63d2734b2", "question": "GCP VM: VM connection request timeout", "sort_order": 110}, "content": "**Question:** I connected to my VM perfectly fine last week (SSH) but when I tried again this week, the connection request keeps timing out.\n\n**Answer:**\n\n1. **Start Your VM:** Make sure the VM is running in your GCP console.\n\n2. **Update External IP:**\n   \n   - Copy its External IP once the VM is running.\n   - Update your SSH configuration file with this IP.\n\n3. **Edit SSH Config:**\n   \n   ```bash\n   cd ~/.ssh\n   code config\n   ```\n   \n   This command opens the config file in VSCode for editing."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/111_68f8a4b307_gcp-vm-connect-to-host-port-22-no-route-to-host.md", "metadata": {"id": "68f8a4b307", "question": "GCP VM: connect to host port 22 no route to host", "sort_order": 111}, "content": "Go to edit your VM.\n\n1. Navigate to the **Automation** section.\n2. Add the following Startup script:\n   \n   ```bash\n   #!/bin/bash\n   sudo ufw allow ssh\n   ```\n3. Stop and Start the VM."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/112_d6fa41adb6_gcp-vm-port-forwarding-from-gcp-without-using-vs-c.md", "metadata": {"id": "d6fa41adb6", "question": "GCP VM: Port forwarding from GCP without using VS Code", "sort_order": 112}, "content": "You can easily forward the ports of pgAdmin, PostgreSQL, and Jupyter Notebook using the built-in tools in Ubuntu without any additional client:\n\n1. **On the VM machine:**\n   - Launch Docker and Jupyter Notebook in the correct folder using:\n     ```bash\n     docker-compose up -d\n     jupyter notebook\n     ```\n\n2. **From the local machine:**\n   - Execute:\n     ```bash\n     ssh -i ~/.ssh/gcp -L 5432:localhost:5432 username@external_ip_of_vm\n     ```\n   - Execute the same command for ports 8080 and 8888.\n\n3. **Accessing Applications Locally:**\n   - For pgAdmin, open a browser and go to `localhost:8080`.\n   - For Jupyter Notebook, open a browser and go to `localhost:8888`.\n     - If you encounter issues with credentials, you may need to copy the link with the access token from the terminal logs on the VM when you launched the Jupyter Notebook.\n\n4. **Forwarding Both pgAdmin and PostgreSQL:**\n   - Use:\n     ```bash\n     ssh -i ~/.ssh/gcp -L 5432:localhost:5432 -L 8080:localhost:8080 modito@35.197.218.128\n     ```"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/113_af2b85f346_gcp-gcloud-ms-vs-code-gcloud-auth-hangs.md", "metadata": {"id": "af2b85f346", "images": [{"description": "image #1", "id": "image_1", "path": "images/data-engineering-zoomcamp/image_df9492cb.png"}, {"description": "image #2", "id": "image_2", "path": "images/data-engineering-zoomcamp/image_6b01ae01.png"}, {"description": "image #3", "id": "image_3", "path": "images/data-engineering-zoomcamp/image_bc858c4b.png"}, {"description": "image #4", "id": "image_4", "path": "images/data-engineering-zoomcamp/image_a231d54c.png"}, {"description": "image #5", "id": "image_5", "path": "images/data-engineering-zoomcamp/image_2f5bf08c.png"}], "question": "GCP gcloud + MS VS Code - gcloud auth hangs", "sort_order": 113}, "content": "If you are using MS VS Code and running `gcloud` in WSL2, when you first try to login to GCP via the `gcloud` CLI with `gcloud auth application-default login`, you may encounter an issue where a message appears and nothing happens:\n\n<{IMAGE:image_1}>\n\nThere might be a prompt asking if you want to open it via a browser. If you click on it, it will open a page with an error message:\n\n<{IMAGE:image_2}>\n\n**Solution:**\n\n- Hover over the long link.\n- `Ctrl + Click` the long link.\n- Click \"Configure Trusted Domains here.\"\n- A popup will appear; pick the first or second entry.\n\n<{IMAGE:image_3}>\n\n<{IMAGE:image_4}>\n\n<{IMAGE:image_5}>\n\nNext time you run `gcloud auth`, the login page should pop up via the default browser without issues."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/114_dec5edee6a_terraform-error-failed-to-query-available-provider.md", "metadata": {"id": "dec5edee6a", "question": "Terraform - Error: Failed to query available provider packages │ Could not retrieve the list of available versions for provider hashicorp/google: could not query │ provider registry for registry.terrafogorm.io/hashicorp/google: the request failed after 2 attempts, │ please try again later", "sort_order": 114}, "content": "This error typically occurs due to internet connectivity issues. Terraform is unable to access the online registry.\n\n**Solution:**\n\n- Check your VPN/Firewall settings.\n- Clear cookies or restart your network.\n- Run `terraform init` again after addressing the connection issues."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/115_31053272e0_terraform-error-post-storagegoogleapiscomhttpsstor.md", "metadata": {"id": "31053272e0", "question": "Terraform: Error: Post \"[storage.googleapis.com](https://storage.googleapis.com/storage/v1/b?alt=json&prettyPrint=false&project=coherent-ascent-379901): oauth2: cannot fetch token: Post \"[oauth2.googleapis.com](https://oauth2.googleapis.com/token): dial tcp 172.217.163.42:443: i/o timeout", "sort_order": 115}, "content": "The issue was related to network restrictions, as Google is not accessible in my country. I used a VPN and discovered that the terminal program does not automatically follow the system proxy, requiring separate proxy configuration settings.\n\n**Solution:**\n\n1. Open an Enhanced Mode in your VPN application, such as Clash.\n2. Run `terraform apply` again.\n\nIf you encounter this issue, consult your VPN provider for assistance with configuration."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/116_a5a163f51d_terraform-install-for-wsl.md", "metadata": {"id": "a5a163f51d", "question": "Terraform: Install for WSL", "sort_order": 116}, "content": "You can configure Terraform on Windows 10 using the Linux Subsystem (WSL) by following this guide: [Configuring Terraform on Windows 10 Linux Subsystem](https://techcommunity.microsoft.com/t5/azure-developer-community-blog/configuring-terraform-on-windows-10-linux-sub-system/ba-p/393845)."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/117_feb57d99da_terraform-error-acquiring-the-state-lock.md", "metadata": {"id": "feb57d99da", "question": "Terraform: Error acquiring the state lock", "sort_order": 117}, "content": "For more information, you can refer to the following issue on GitHub:\n\n[HashiCorp Terraform Issue #14513](https://github.com/hashicorp/terraform/issues/14513)"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/118_dd3e6999fd_terraform-error-400-bad-request-invalid-jwt-token.md", "metadata": {"id": "dd3e6999fd", "question": "Terraform: Error 400 Bad Request. Invalid JWT Token on WSL.", "sort_order": 118}, "content": "When running:\n\n```bash\nterraform apply\n```\n\non WSL2, you might encounter the following error:\n\n```\nError: Post \"https://storage.googleapis.com/storage/v1/b?alt=json&prettyPrint=false&project=<your-project-id>\": oauth2: cannot fetch token: 400 Bad Request\n\nResponse: {\"error\":\"invalid_grant\",\"error_description\":\"Invalid JWT: Token must be a short-lived token (60 minutes) and in a reasonable timeframe. Check your iat and exp values in the JWT claim.\"}\n```\n\nThis issue occurs due to potential time desynchronization on your machine, affecting JWT computation.\n\nTo fix this, run the following command to synchronize your system time:\n\n```bash\nsudo hwclock -s\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/119_55c0047f8b_terraform-error-403-access-denied.md", "metadata": {"id": "55c0047f8b", "question": "Terraform - Error 403 : Access denied", "sort_order": 119}, "content": "```\n│ Error: googleapi: Error 403: Access denied., forbidden\n```\n\nYour `$GOOGLE_APPLICATION_CREDENTIALS` might not be pointing to the correct file. Try the following steps:\n\n1. Set the correct path for your credentials:\n   \n   ```bash\n   export GOOGLE_APPLICATION_CREDENTIALS=~/.gc/YOUR_JSON.json\n   ```\n\n2. Activate the service account:\n   \n   ```bash\n   gcloud auth activate-service-account --key-file $GOOGLE_APPLICATION_CREDENTIALS\n   ```"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/120_de1924abdb_terraform-do-i-need-to-make-another-service-accoun.md", "metadata": {"id": "de1924abdb", "question": "Terraform: Do I need to make another service account for Terraform before I get the keys (.json file)?", "sort_order": 120}, "content": "One service account is enough for all the services/resources you'll use in this course. After you get the file with your credentials and set your environment variable, you should be good to go."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/121_d0f76f4669_terraform-where-can-i-find-the-terraform-113-linux.md", "metadata": {"id": "d0f76f4669", "question": "Terraform: Where can I find the Terraform 1.1.3 Linux (AMD 64)?", "sort_order": 121}, "content": "Here: [https://releases.hashicorp.com/terraform/1.1.3/terraform_1.1.3_linux_amd64.zip](https://releases.hashicorp.com/terraform/1.1.3/terraform_1.1.3_linux_amd64.zip)"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/122_76f95c1d28_terraform-terraform-initialized-in-an-empty-direct.md", "metadata": {"id": "76f95c1d28", "question": "Terraform: Terraform initialized in an empty directory! The directory has no Terraform configuration files. You may begin working with Terraform immediately by creating Terraform configuration files.", "sort_order": 122}, "content": "This error occurs when `terraform init` is run outside the working directory.\n\nTo resolve this issue:\n\n1. Navigate to the working directory that contains your Terraform configuration files.\n2. Run the `terraform init` command inside the correct directory.\n\nMake sure your configuration files (e.g., .tf files) are present in the directory before running the command."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/123_4a6d04a2c3_terraform-error-creating-dataset-googleapi-error-4.md", "metadata": {"id": "4a6d04a2c3", "question": "Terraform - Error creating Dataset: googleapi: Error 403: Request had insufficient authentication scopes", "sort_order": 123}, "content": "The error:\n\n```\nError: googleapi: Error 403: Access denied., forbidden\n\nError: Error creating Dataset: googleapi: Error 403: Request had insufficient authentication scopes.\n```\n\nSolution:\n\n1. Verify your credentials by running:\n   \n   ```bash\n   echo $GOOGLE_APPLICATION_CREDENTIALS\n   echo $?\n   ```\n\n2. Ensure you have set the `GOOGLE_APPLICATION_CREDENTIALS` environment variable correctly, as demonstrated in the environment setup video in week 1:\n\n   ```bash\n   export GOOGLE_APPLICATION_CREDENTIALS=\"<path/to/your/service-account-authkeys>.json\"\n   ```"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/124_5849afe5f6_stoterraform-error-creating-bucket-googleapi-error.md", "metadata": {"id": "5849afe5f6", "question": "stoTerraform - Error creating Bucket: googleapi: Error 403: Permission denied to access ‘storage.buckets.create’", "sort_order": 124}, "content": "The error:\n\n```\nError: googleapi: Error 403: terraform-trans-campus@trans-campus-410115.iam.gserviceaccount.com does not have storage.buckets.create access to the Google Cloud project. Permission 'storage.buckets.create' denied on resource (or it may not exist)., forbidden\n```\n\nThe solution:\n\nYou have to declare the project name as your Project ID, not your Project name, available on the GCP console Dashboard."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/125_fa8cbc8f40_terraform-google-provider-requires-credentials.md", "metadata": {"id": "fa8cbc8f40", "question": "Terraform: google provider requires credentials.", "sort_order": 125}, "content": "To ensure the sensitivity of the credentials file, use the following configuration:\n\n```hcl\nprovider \"google\" {\n  project     = var.projectId\n  credentials = file(\"${var.gcpkey}\")\n  #region      = var.region\n  zone = var.zone\n}\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/126_9f9a1b9e4f_terraform-teardown-of-bigquery-dataset.md", "metadata": {"id": "9f9a1b9e4f", "question": "Terraform: Teardown of BigQuery Dataset", "sort_order": 126}, "content": "When running `terraform destroy`, the following error can occur:\n\n```\nDo you really want to destroy all resources?\n\nTerraform will destroy all your managed infrastructure, as shown above.\n\nThere is no undo. Only 'yes' will be accepted to confirm.\n\nEnter a value: yes\n\ngoogle_bigquery_dataset.homework_dataset: Destroying... [id=projects/terraform-demo-449214/datasets/homework_dataset]\n\n╷\n\n│ Error: Error when reading or editing Dataset: googleapi: Error 400: Dataset terraform-demo-449214:homework_dataset is still in use, resourceInUse\n```\n\nThis is because the dataset is still in use by a table. To delete the dataset, set the `delete_contents_on_destroy` property to `true` in the `main.tf` file."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/127_bac852d170_sql-select-from-zones_taxi-where-zoneastoria-zone.md", "metadata": {"id": "bac852d170", "question": "SQL: SELECT * FROM zones_taxi WHERE Zone='Astoria Zone'; Error Column Zone doesn't exist", "sort_order": 127}, "content": "For this issue, you can use the following solution:\n\n```sql\nSELECT * FROM zones AS z WHERE z.\"Zone\" = 'Astoria Zone';\n```\n\nColumns that start with uppercase sometimes need to be enclosed in double quotes.\n\nAdditionally, check your dataset for the existence of `'Astoria Zone'`. You might find only `'Astoria'`:\n\n```sql\nSELECT * FROM zones AS z WHERE z.\"Zone\" = 'Astoria';\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/128_358cb2fd4d_sql-select-zone-from-taxi_zones-error-column-zone.md", "metadata": {"id": "358cb2fd4d", "question": "SQL: SELECT Zone FROM taxi_zones Error Column Zone doesn't exist", "sort_order": 128}, "content": "It is inconvenient to use quotation marks all the time, so it is better to put the data in the database all in lowercase. In Pandas, after:\n\n```python\nimport pandas as pd\n\ndf = pd.read_csv('taxi+_zone_lookup.csv')\n```\n\nAdd the row:\n\n```python\ndf.columns = df.columns.str.lower()\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/129_48cae101d6_curl-curl-6-could-not-resolve-host-outputcsv.md", "metadata": {"id": "48cae101d6", "question": "CURL: curl: (6) Could not resolve host: output.csv", "sort_order": 129}, "content": "```python\nos.system(f\"curl {url} --output {csv_name}\")\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/130_10407876a1_ssh-error-ssh-could-not-resolve-hostname-linux-nam.md", "metadata": {"id": "10407876a1", "question": "SSH Error: ssh: Could not resolve hostname linux: Name or service not known", "sort_order": 130}, "content": "To resolve this, ensure that your config file is in `C/User/Username/.ssh/config`."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/131_d02af042b1_pip-is-not-recognized-as-an-internal-or-external-c.md", "metadata": {"id": "d02af042b1", "question": "'pip' is not recognized as an internal or external command, operable program or batch file.", "sort_order": 131}, "content": "If you use Anaconda (recommended for the course), it comes with `pip`, so the issue is probably that Anaconda’s Python is not on the PATH.\n\n\n**For Linux and MacOS:**\n\n1. Open a terminal.\n2. Find the path to your Anaconda installation. This is typically `~/anaconda3` or `~/opt/anaconda3`.\n3. Add Anaconda to your PATH with the command:\n   \n   ```bash\n   export PATH=\"/path/to/anaconda3/bin:$PATH\"\n   ```\n\n4. To make this change permanent, add the command to your `.bashrc` (Linux) or `.bash_profile` (MacOS) file.\n\n**On Windows, using Git Bash:**\n\n1. Locate your Anaconda installation. The default path is usually `C:\\Users\\[YourUsername]\\Anaconda3`.\n2. Convert the Windows path to a Unix-style path for Git Bash, e.g., `C:\\Users\\[YourUsername]\\Anaconda3` becomes `/c/Users/[YourUsername]/Anaconda3`.\n3. Add Anaconda to your PATH with the command:\n\n   ```bash\n   export PATH=\"/c/Users/[YourUsername]/Anaconda3/:/c/Users/[YourUsername]/Anaconda3/Scripts/$PATH\"\n   ```\n\n4. To make this change permanent, add the command to your `.bashrc` file in your home directory.\n5. Refresh your environment with the command:\n\n   ```bash\n   source ~/.bashrc\n   ```\n\n**For Windows (without Git Bash):**\n\n1. Right-click on 'This PC' or 'My Computer' and select 'Properties'.\n2. Click on 'Advanced system settings'.\n3. In the System Properties window, click on 'Environment Variables'.\n4. In the Environment Variables window, select the 'Path' variable in the 'System variables' section and click 'Edit'.\n5. In the Edit Environment Variable window, click 'New' and add the path to your Anaconda installation (typically `C:\\Users\\[YourUsername]\\Anaconda3` and `C:\\Users\\[YourUsername]\\Anaconda3\\Scripts`).\n6. Click 'OK' in all windows to apply the changes.\n\nAfter adding Anaconda to the PATH, you should be able to use `pip` from the command line. Remember to restart your terminal (or command prompt in Windows) to apply these changes."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/132_8b082e74c0_error-error-starting-userland-proxy-listen-tcp4-00.md", "metadata": {"id": "8b082e74c0", "question": "Error: error starting userland proxy: listen tcp4 0.0.0.0:8080: bind: address already in use", "sort_order": 132}, "content": "Resolution: You need to stop the service using the port.\n\nRun the following:\n\n```bash\nsudo kill -9 `sudo lsof -t -i:<port>`\n```\n\nReplace `<port>` with `8080` in this case. This will free up the port for use."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/133_a9081d1a79_anaconda-to-pip.md", "metadata": {"id": "a9081d1a79", "question": "Anaconda to PIP", "sort_order": 133}, "content": "To get a pip-friendly `requirements.txt` file from Anaconda, use the following steps:\n\n1. Install pip in your Anaconda environment:\n   ```bash\n   conda install pip\n   ```\n2. Generate the `requirements.txt` file:\n   ```bash\n   pip list --format=freeze > requirements.txt\n   ```\n\nNote:\n- `conda list -d > requirements.txt` will not work.\n- `pip freeze > requirements.txt` may give odd pathing."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/134_35d7874a8d_jupyter-install-open-jupyter-and-convert-jupyter-n.md", "metadata": {"id": "35d7874a8d", "question": "Jupyter: Install, open Jupyter and convert Jupyter notebook to Python script", "sort_order": 134}, "content": "### Install and Open Jupyter Notebook\n\nTo install Jupyter Notebook, run:\n\n```bash\npip install jupyter\n```\n\nTo open Jupyter Notebook, use:\n\n```bash\npython3 -m notebook\n```\n\n### Convert Jupyter Notebook to Python Script\n\nFirst, ensure `nbconvert` is installed and upgraded:\n\n```bash\npip install nbconvert --upgrade\n```\n\nThen, convert a Jupyter Notebook to a Python script with the following command:\n\n```bash\npython3 -m jupyter nbconvert --to=script upload-data.ipynb\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-1/135_c2f39b0ef3_alternative-way-to-convert-jupyter-notebook-to-pyt.md", "metadata": {"id": "c2f39b0ef3", "question": "Alternative way to convert Jupyter notebook to Python script (via jupytext)", "sort_order": 135}, "content": "If you keep getting errors with nbconvert after executing:\n\n```bash\njupyter nbconvert --to script <your_notebook.ipynb>\n```\n\nYou could try converting your Jupyter notebook using another tool called Jupytext. Jupytext is an excellent tool for converting Jupyter Notebooks to Python scripts, similar to nbconvert.\n\n1. **Install Jupytext**\n   \n   ```bash\n   pip install jupytext\n   ```\n\n2. **Convert your Notebook to a Python script**\n\n   ```bash\n   jupytext --to py <your_notebook.ipynb>\n   ```"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-2/001_7889d2bad5_ssh-error-in-vs-code-could-not-establish-connectio.md", "metadata": {"id": "7889d2bad5", "images": [{"description": "image #1", "id": "image_1", "path": "images/data-engineering-zoomcamp/image_0f494026.png"}], "question": "SSH error in VS Code - “Could not establish connection to \"de-zoomcamp\": Permission denied (publickey).”", "sort_order": 1}, "content": "If you are using Windows, try the following steps to resolve the error:\n\n1. Copy the `.ssh` folder from the Linux file path to Windows.\n2. In the `config` file, use:\n   \n   ```\n   IdentityFile C:\\Users\\<username>\\.ssh\\gcp\n   ```\n   \n   Instead of:\n   \n   ```\n   IdentityFile ~/.ssh/gcp\n   ```\n\n3. Ensure the private key file located at `C:\\Users\\<username>\\.ssh\\gcp` has an extra line at the end:\n\n   <{IMAGE:image_1}>"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-2/002_f1c31510e6_where-are-the-faq-questions-from-the-previous-coho.md", "metadata": {"id": "f1c31510e6", "question": "Where are the FAQ questions from the previous cohorts for the orchestration module?", "sort_order": 2}, "content": "- [Prefect FAQ Document](https://docs.google.com/document/d/1K_LJ9RhAORQk3z4Qf_tfGQCDbu8wUWzru62IUscgiGU/edit?usp=sharing)\n- [Airflow FAQ Document](https://docs.google.com/document/d/1-BwPAsyDH_mAsn8HH5z_eNYVyBMAtawJRjHHsjEKHyY/edit?usp=sharing)\n- [Mage FAQ Document](https://docs.google.com/document/d/1CkHVelbYYTMbwuj2eurNIwWVqXWzH-9-AqKETD9IC3I/edit?tab=t.0)"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-2/003_bcc6781b6e_how-do-i-launch-kestra.md", "metadata": {"id": "bcc6781b6e", "question": "How do I launch Kestra?", "sort_order": 3}, "content": "To launch Kestra, follow these instructions:\n\n### For Linux\n\nStart Docker with the following command:\n\n```bash\ndocker run \\\n  --pull=always \\\n  --rm \\\n  -it \\\n  -p 8080:8080 \\\n  --user=root \\\n  -v /var/run/docker.sock:/var/run/docker.sock \\\n  -v /tmp:/tmp \\\n  kestra/kestra:latest server local\n```\n\nOnce it is running, you can log in to the dashboard at `localhost:8080`.\n\n### For Windows\n\nRefer to the Kestra GitHub repository for detailed instructions: [https://github.com/kestra-io/kestra](https://github.com/kestra-io/kestra)\n\n\nSample `docker-compose` for Kestra:\n\n```yaml\nkestra:\n  build: .\n  image: kestra/kestra:latest\n  container_name: kestra\n  user: \"0:0\"\n  environment:\n    DOCKER_HOST: tcp://host.docker.internal:2375  # for Windows\n    KESTRA_CONFIGURATION: |\n      kestra:\n        repository:\n          type: h2\n        queue:\n          type: memory\n        storage:\n          type: local\n          local:\n            basePath: /app/storage\n        tasks:\n          tmp-dir:\n            path: /app/tmp\n        plugins:\n          repositories:\n            - id: central\n              type: maven\n              url: [repo.maven.apache.org](https://repo.maven.apache.org/maven2)\n          definitions:\n            - io.kestra.plugin.core:core:latest\n            - io.kestra.plugin.scripts:python:1.3.4\n            - io.kestra.plugin.http:http:latest\n    KESTRA_TASKS_TMP_DIR_PATH: /app/tmp\n  ports:\n    - \"8080:8080\"\n  volumes:\n    - //var/run/docker.sock:/var/run/docker.sock  # Windows path\n    - /yourpath/.dbt:/app/.dbt\n    - /yourpath/kestra/plugins:/app/plugins\n    - /yourpath/kestra/workflows:/app/workflows\n    - /yourpath/kestra/storage:/app/storage\n    - /yourpath//kestra/tmp:/app/tmp\n    - /yourpath//dbt_prj:/app/workflows/dbt_project\n    - /yourpath//my-creds.json:/app/.dbt/my-creds.json\n  command: server standalone\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-2/004_16481ac8f7_docker-error-response-from-daemon-mkdir-cprogram-f.md", "metadata": {"id": "16481ac8f7", "question": "docker: Error response from daemon: mkdir C:\\Program Files\\Git\\var: Access is denied.", "sort_order": 4}, "content": "### Description:\n\nWhen running the following Docker command in Bash with Docker and WSL2 installed, you may encounter an error. Running Bash as admin will not resolve the issue:\n\n```bash\ndocker run \\\n  --pull=always \\\n  --rm \\\n  -it \\\n  -p 8080:8080 \\\n  --user=root \\\n  -v /var/run/docker.sock:/var/run/docker.sock \\\n  -v /tmp:/tmp \\\n  kestra/kestra:latest server local\n```\n\n```\nlatest: Pulling from kestra/kestra\nDigest: sha256:af02a309ccbb52c23ad1f1551a1a6db8cf0523cf7aac7c7eb878d7925bc85a62\nStatus: Image is up to date for kestra/kestra:latest\ndocker: Error response from daemon: mkdir C:\\\\Program Files\\\\Git\\\\var: Access is denied.\nSee 'docker run --help'.\n```\n\n\nTo resolve this issue, run Command Prompt as an administrator and use the following command:\n\n```bash\ndocker run \\\n  --pull=always \\\n  --rm \\\n  -it \\\n  -p 8080:8080 \\\n  --user=root \\\n  -v \"/var/run/docker.sock:/var/run/docker.sock\" \\\n  -v \"C:/Temp:/tmp\" \\\n  kestra/kestra:latest server local\n```\n\nAfter executing the command as described, the localhost should display the Kestra UI as expected."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-2/005_e4bce3ff6b_error-when-running-kestra-flow-connecting-to-postg.md", "metadata": {"id": "e4bce3ff6b", "question": "Error when running Kestra flow connecting to postgres", "sort_order": 5}, "content": "### Error Message\n```plaintext\norg.postgresql.util.psqlexception the connection attempt failed due to this config on kestra flow -> jdbc:postgresql://host.docker.internal:5432/postgres-zoomcamp\n```\n\n### Solution\n- Replace `host.docker.internal` with the name of the service for Postgres in your Docker Compose file.\n\n---\n\n### Additional Error Message\n```plaintext\norg.postgresql.util.PSQLException: The connection attempt failed. 2025-01-29 22:52:22.281 green_create_table The connection attempt failed. host.docker.internal\n```\n\n### Analysis and Solution\n- If using Linux, the PostgreSQL database URL differs from the tutorial. Instead of `host.docker.internal`, Linux users should use the service or container name for Postgres. For example, use:\n  \n  ```plaintext\n  jdbc:postgresql://postgres:5432/kestra\n  ```\n- Double-check the database name in your Docker Compose file. It might be different from the tutorial; for example, `kestra` instead of `postgres-zoomcamp`.\n\n### Reminder\n- Ensure that the PostgreSQL database name in the Docker Compose matches what you configure in your flow."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-2/006_db53f69d74_adding-a-pgadmin-service-with-volume-mounting-to-t.md", "metadata": {"id": "db53f69d74", "question": "Adding a pgadmin service with volume mounting to the docker-compose:", "sort_order": 6}, "content": "I encountered an error where the localhost URL for pgAdmin would just hang (I chose `localhost:8080` for my pgAdmin, and made kestra `localhost:8090`, personal preference).\n\nThe associated issue involved permissions. The resolution was to change the ownership of my local directory to the user \"5050,\" which is pgAdmin. Unlike Postgres, pgAdmin requires explicit permission. Apparently, the Postgres user inside the Docker container creates the Postgres volume/dir, so it has permissions already.\n\nThis is a useful resource:\n\n[Stack Overflow: Permission denied /var/lib/pgadmin/sessions in Docker](https://stackoverflow.com/questions/64781245/permission-denied-var-lib-pgadmin-sessions-in-docker)"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-2/007_00e8093b90_running-out-of-storage-when-using-kestra-with-post.md", "metadata": {"id": "00e8093b90", "question": "Running out of storage when using kestra with postgres on GCP VM", "sort_order": 7}, "content": "Running out of storage while trying to backfill. I realized my GCP VM only has 30GB of storage and I was using it up quickly. Here are a couple of suggestions for managing storage:\n\n- **Clean up your GCP VM drive:** Use the command below to identify what is taking up the most space:\n\n  ```bash\n  sudo du -sh *\n  ```\n\n  - **(~1GB)** For me, the Anaconda installer was consuming a lot of space. If you no longer need it, you can delete it:\n  \n    ```bash\n    rm -rf <anacondainstaller_fpath>\n    ```\n\n  - **(~3GB)** Anaconda itself takes up a lot of space. You can’t delete it entirely if you need Python, but you can clean it up significantly:\n    \n    ```bash\n    conda clean --all -y\n    ```\n\n- **Clean up your Kestra files:** Use a purge flow. You can find a generic example here:\n  \n  [https://kestra.io/docs/administrator-guide/purge](https://kestra.io/docs/administrator-guide/purge)\n  \n  I wanted to perform the cleanup immediately, rather than waiting until the end of the month, so I adjusted the `endDate` to `\"{{ now() }}\"` and removed the trigger block. You can also choose whether to remove FAILED state executions.\n\n- **Clean up your PostgreSQL database:** You can manually delete tables in pgAdmin, or set up a workflow in Kestra for it. I found it easy to do manually."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-2/008_06775a8677_how-can-kestra-access-service-account-credential.md", "metadata": {"id": "06775a8677", "question": "How can Kestra access service account credential?", "sort_order": 8}, "content": "Do not directly add the content of service account credential JSON in Kestra script, especially if you are pushing to GitHub. Follow the instruction to add the service account as a secret [Configure Google Service Account](https://kestra.io/docs/how-to-guides/google-credentials#add-service-account-as-a-secret).\n\nWhen you need to use it in Kestra, you can pull it through `{{ secret('GCP_SERVICE_ACCOUNT') }}` in the `pluginDefaults`."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-2/009_dda62d0ef0_storage-bucket-permission-denied-error-when-runnin.md", "metadata": {"id": "dda62d0ef0", "question": "Storage: Bucket Permission Denied Error when running the gcp_setup flow", "sort_order": 9}, "content": "When following the [YouTube lesson](https://www.youtube.com/watch?v=nKqjjLJ7YXs&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=23) and then running the [gcp_setup flow](https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/02-workflow-orchestration/flows/05_gcp_setup.yaml), you might encounter a permission denied error.\n\nTo resolve this:\n\n1. Verify if the bucket already exists using the GCP console.\n2. If it exists, choose a different name for the bucket.\n\n**Note:** The GCP bucket name must be unique globally across all buckets, as the bucket will be accessible by URL."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-2/010_c18119ba32_invalid-dataset-id-error-when-running-the-gcp_setu.md", "metadata": {"id": "c18119ba32", "question": "Invalid dataset ID Error when running the gcp_setup flow", "sort_order": 10}, "content": "When following the [YouTube lesson](https://www.youtube.com/watch?v=nKqjjLJ7YXs&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=23) and then running the [gcp_setup flow](https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/02-workflow-orchestration/flows/05_gcp_setup.yaml), the error occurs during the `create_bq_dataset` task.\n\nThe error is less clear, but it stems from using a dash in the dataset name. To resolve this, change the dataset name to something like \"de_zoomcamp\" to avoid using a dash. This should resolve the error."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-2/011_aeaede4fd1_how-do-i-properly-authenticate-a-google-cloud-serv.md", "metadata": {"id": "aeaede4fd1", "question": "How do I properly authenticate a Google Cloud Service Account in Kestra?", "sort_order": 11}, "content": "Several authentication methods are available; here are some of the most straightforward approaches:\n\n### Method 1:\n\nUpdate your `docker-compose.yml` file as needed.\n\n### Method 2:\n\n1. **Store the Service Account as a Secret**  \n   Run this command, specifying the correct path to your `service-account.json` file and `.env_encoded`:\n\n   ```bash\n   # Example command: Adjust according to your environment\n   base64 /path/to/service-account.json > .env_encoded\n   ```\n\n2. **Modify `docker-compose.yml` to Include the Encoded Secrets**  \n   Insert the relevant configuration within your `docker-compose.yml`.\n\n3. **Configure Kestra Plugin Defaults**  \n   This ensures all GCP tasks use the secret automatically.\n\n4. **Verify it’s Working in a Testing GCP Workflow**\n\n### Additional FAQs:\n\n**Question:** How do I update the Service Account key?\n\n**Answer:** Generate a new key, re-run the Base64 command, and restart Kestra.\n\n**Question:** Why use secrets instead of embedding the JSON key in the task?\n\n**Answer:** Secrets prevent credential exposure and make workflows easier to manage.\n\n**Question:** Can I apply this method to other GCP tasks?\n\n**Answer:** Yes, all GCP plugins will automatically inherit the secret."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-2/012_a489fb34ac_should-i-include-my-env_encoded-file-in-my-gitigno.md", "metadata": {"id": "a489fb34ac", "question": "Should I include my .env_encoded file in my .gitignore?", "sort_order": 12}, "content": "Yes, you should definitely include the `.env_encoded` file in your `.gitignore` file. Here's why:\n\n- **Security:** The `.env_encoded` file contains sensitive information, namely the base64 encoded version of your GCP Service Account key. Even though it's encoded, it's not secure to share this in a public repository as anyone can decode it back to the original JSON.\n\n- **Best Practices:** It's common practice to avoid committing environment files or any files containing secrets to version control systems like Git. This prevents accidental exposure of sensitive data.\n\n### How to do it:\n\n- Add this line to your `.gitignore`:\n\n  ```\n  .env_encoded\n  ```\n\n### More on Security\n\nBase64 encoding is easily reversible. Base64 is an encoding scheme, not an encryption method. It's designed to encode binary data into ASCII characters that can be safely transmitted over systems that are designed to deal with text. Here's why it's not secure for protecting sensitive information:\n\n- **Reversibility:** Base64 encoding simply translates binary data into a text string using a specific set of 64 characters. Decoding it back to the original data is straightforward and doesn't require any secret key or password.\n\n- **Public Availability of Tools:** Numerous online tools, software libraries, and command-line utilities exist that can decode base64 with just a few clicks or commands.\n\n- **No Security:** Since base64 encoding does not change or hide the actual content of the data, anyone with access to the encoded string can decode it back to the original data."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-2/013_5db9bca6a9_getting-sigill-in-jre-when-running-latest-kestra-i.md", "metadata": {"id": "5db9bca6a9", "question": "Getting SIGILL in JRE when running latest kestra image on Mac M4 MacOS 15.2/3", "sort_order": 13}, "content": "SIGILL in Java Runtime Environment on MacOS M4\n\nAdd the following environment variable to your Kestra container: `-e JAVA_OPTS=\"-XX:UseSVE=0\"`:\n\n```bash\ndocker run --rm -it \\\n  --pull=always \\\n  -p 8080:8080 \\\n  --user=root \\\n  -e JAVA_OPTS=\"-XX:UseSVE=0\" \\\n  -v /var/run/docker.sock:/var/run/docker.sock \\\n  -v /tmp:/tmp \\\n  kestra/kestra:latest server local\n```\nThe same in a Docker Compose file:\n\n```yaml\nservices:\n  kestra:\n    image: kestra/kestra:latest\n    environment:\n      JAVA_OPTS: \"-XX:UseSVE=0\"\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-2/014_5d66421473_taskid-yellow_create_table-the-connection-attempt.md", "metadata": {"id": "5d66421473", "images": [{"description": "image #1", "id": "image_1", "path": "images/data-engineering-zoomcamp/image_e66c0b8d.png"}], "question": "taskid: yellow_create_table The connection attempt failed. Host.docker.internal", "sort_order": 14}, "content": "If you're using Linux, you might encounter \"Connection Refused\" errors when connecting to the Postgres DB from within Kestra. This is because `host.docker.internal` works differently on Linux.\n\nTo address this issue:\n\n- Use the modified Docker Compose file mentioned in the \"02-workflow-orchestration\" README troubleshooting tips.\n- Run both Kestra and its dedicated Postgres DB, along with the Postgres DB for exercises, all together using Docker Compose.\n- Access the Postgres DB within Kestra by using the container name `postgres_zoomcamp` instead of `host.docker.internal` in `pluginDefaults`.\n\nMake sure to modify the `pluginDefaults` in the following files:\n\n- `2_postgres_taxi_scheduled.yaml`\n- `02_postgres_taxi.yaml`\n\n<{IMAGE:image_1}>"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-2/015_48b99a69ff_fix-add-extra_hosts-for-hostdockerinternal-on-linu.md", "metadata": {"id": "48b99a69ff", "question": "Fix: Add extra_hosts for host.docker.internal on Linux", "sort_order": 15}, "content": "This update corrects the Docker Compose configuration to resolve the error when using the alias `host.docker.internal` on Linux systems. Since this alias does not resolve natively on Linux, the following entry was added to the affected container:\n\n```\nyaml\nkestra:\n  image: kestra/kestra:latest\n  pull_policy: always\n  user: \"root\"\n  command: server standalone\n  volumes:\n    # Add volume configurations here\n  environment:\n    # Add environment variables here\n  ports:\n    # Add ports here\n  depends_on:\n    # Add dependencies here\n  extra_hosts:\n    - \"host.docker.internal:host-gateway\"\n```\n\nWith this change, containers that need to access host services via `host.docker.internal` will be able to do so correctly. For inter-container communication within the same network, it is recommended to use the service name directly."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-2/016_df4171cea7_fix-add-extra_hosts-for-taskrunner-in-the-dbt-buil.md", "metadata": {"id": "df4171cea7", "question": "Fix: Add extra_hosts for taskRunner in the dbt-build", "sort_order": 16}, "content": "To resolve the issue with `host.docker.internal` not being recognized on Linux, add the `extraHosts` configuration to the `taskRunner` in the `dbt-build` task:\n\n```yaml\ntaskRunner:\n  type: io.kestra.plugin.scripts.runner.docker.Docker\n  extraHosts:\n    - \"host.docker.internal:host-gateway\"\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-2/017_9abc27b002_kestra-dont-forget-to-set-gcp_creds-variable.md", "metadata": {"id": "9abc27b002", "question": "Kestra: Don’t forget to set GCP_CREDS variable", "sort_order": 17}, "content": "If you plan on using Kestra with Google Cloud Platform, make sure you set up the `GCP_CREDS` that will be used in flows with \"gcp\" in their name.\n\nTo set it:\n\n1. Go to **Namespaces** and select \"zoomcamp\" if you are using the examples from the lessons.\n2. In the **KV Store** tab, create a new key as `GCP_CREDS`.\n3. Set the type to JSON and paste the content of the `.json` file with credentials for the service account created."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-3/001_c717810bc6_kestra-backfill-showing-getting-executed-but-not-g.md", "metadata": {"id": "c717810bc6", "question": "Kestra: Backfill showing getting executed but not getting results or showing up in executions", "sort_order": 1}, "content": "It seems to be a bug. The current fix is to remove the timezone from triggers in the script. More on this bug is [here](https://github.com/kestra-io/kestra/issues/7227)."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-3/002_687d54c6ba_docker-docker-compose-takes-infinitely-long-to-ins.md", "metadata": {"id": "687d54c6ba", "question": "Docker: Docker-compose takes infinitely long to install zip unzip packages for linux, which are required to unpack datasets.", "sort_order": 2}, "content": "To resolve the issue, you can try the following solutions:\n\n1. Add the `-Y` flag to `apt-get` to automatically agree to install additional packages.\n   \n   ```bash\n   sudo apt-get install -y zip unzip\n   ```\n\n2. Use the Python `ZipFile` package, which is included in all modern Python distributions. This can bypass the need to install `zip` and `unzip` packages.\n\n   ```python\n   from zipfile import ZipFile\n\n   with ZipFile('file.zip', 'r') as zip_ref:\n       zip_ref.extractall('destination_folder')\n   ```"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-3/003_ee4a3bc34d_gcs-bucket-error-when-writing-data-from-web-to-gcs.md", "metadata": {"id": "ee4a3bc34d", "question": "GCS Bucket - error when writing data from web to GCS:", "sort_order": 3}, "content": "Make sure to use Nullable data types, such as [Int64](https://pandas.pydata.org/docs/user_guide/integer_na.html) when applicable."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-3/004_d2902a7227_gcs-bucket-te-table-error-while-reading-data-error.md", "metadata": {"id": "d2902a7227", "images": [{"description": "image #1", "id": "image_1", "path": "images/data-engineering-zoomcamp/image_5924f19e.png"}], "question": "GCS Bucket - te table: Error while reading data, error message: Parquet column 'XYZ' has type INT which does not match the target cpp_type DOUBLE. File: gs://path/to/some/blob.parquet", "sort_order": 4}, "content": "Ultimately, when trying to ingest data into a BigQuery table, all files within a given directory must have the same schema.\n\nWhen dealing with datasets, such as the FHV Datasets from 2019, you may encounter schema inconsistencies. For example, the files for '2019-05' and '2019-06' have the columns \"PUlocationID\" and \"DOlocationID\" as integers, while for the period of '2019-01' through '2019-04', the same columns are defined as floats.\n\nWhen importing these files as Parquet to BigQuery, the first file will define the table schema. All subsequent files must have the same schema to append data correctly.\n\n<{IMAGE:image_1}>\n\nTo prevent errors like this, enforce the data types for the columns on the DataFrame before serializing/uploading them to BigQuery:\n\n```python\npd.read_csv(\"path_or_url\").astype({\"col1_name\": \"datatype\", \"col2_name\": \"datatype\", ..., \"colN_name\": \"datatype\"})\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-3/005_950192cbcc_gcs-bucket-fix-error-when-importing-fhv-data-to-gc.md", "metadata": {"id": "950192cbcc", "question": "GCS Bucket: Fix Error when importing FHV data to GCS", "sort_order": 5}, "content": "If you receive the error \n\n```python\ngzip.BadGzipFile: Not a gzipped file (b'\\n\\n')\n```\n\nthis is because you have specified the wrong URL to the FHV dataset. Make sure to use:\n\n```\nhttps://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhv/{dataset_file}.csv.gz\n```\n\nEmphasize the `/releases/download` part of the URL."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-3/006_4ae927f8a0_gcs-bucket-load-data-from-url-list-in-to-gcp-bucke.md", "metadata": {"id": "4ae927f8a0", "images": [{"description": "image #1", "id": "image_1", "path": "images/data-engineering-zoomcamp/image_d49e30cd.png"}], "question": "GCS Bucket - Load Data From URL list in to GCP Bucket", "sort_order": 6}, "content": "<{IMAGE:image_1}>"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-3/007_33184c75bd_gcs-bucket-i-query-my-dataset-and-get-a-bad-charac.md", "metadata": {"id": "33184c75bd", "question": "GCS Bucket - I query my dataset and get a Bad character (ASCII 0) error?", "sort_order": 7}, "content": "- **Check the Schema**: Ensure that the schema of your dataset is correctly defined.\n\n- **Formatting Issues**: You might have incorrect formatting in your files.\n\n- **Upload Method**: Try uploading the CSV.GZ files without formatting or processing them through pandas. Use `wget` to download if necessary."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-3/008_a07b793f65_gcp-bq-bq-command-not-found.md", "metadata": {"id": "a07b793f65", "question": "GCP BQ: \"bq: command not found\"", "sort_order": 8}, "content": "Run the following command to check if \"BigQuery Command Line Tool\" is installed or not:\n\n```bash\ngcloud components list\n```\n\nYou can also use `bq.cmd` instead of `bq` to make it work."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-3/009_0131ac93ac_gcp-caution-in-using-bigquery-bigqueryno.md", "metadata": {"id": "0131ac93ac", "question": "GCP: Caution in using BigQuery - bigquery:no", "sort_order": 9}, "content": "Use BigQuery carefully:\n\n- I created my BigQuery dataset on an account where my free trial was exhausted and received a bill of $80.\n- Use BigQuery under free credits and destroy all the datasets after creation.\n- Check your billing daily, especially if you've spun up a VM."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-3/010_61b908fe84_gcp-bq-cannot-read-and-write-in-different-location.md", "metadata": {"id": "61b908fe84", "images": [{"description": "image #1", "id": "image_1", "path": "images/data-engineering-zoomcamp/image_924b3959.png"}, {"description": "image #2", "id": "image_2", "path": "images/data-engineering-zoomcamp/image_4d72f30c.png"}], "question": "GCP BQ - Cannot read and write in different locations: source: EU, destination: US - Loading data from GCS into BigQuery (different Region):", "sort_order": 10}, "content": "Be careful when you create your resources on GCP; all of them must share the same region to load data from a GCS Bucket to BigQuery. If you forgot this step, you can create a new dataset in BigQuery using the same region as your GCS Bucket.\n\n<{IMAGE:image_1}>\n\n<{IMAGE:image_2}>\n\nThis error indicates that your GCS Bucket and the BigQuery dataset are placed in different regions. You need to create a new dataset in BigQuery in the same region as your GCS Bucket and store the data in this newly created dataset."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-3/011_c8d29f6862_gcp-bq-cannot-read-and-write-in-different-location.md", "metadata": {"id": "c8d29f6862", "images": [{"description": "image #1", "id": "image_1", "path": "images/data-engineering-zoomcamp/image_1d6e2776.png"}, {"description": "image #2", "id": "image_2", "path": "images/data-engineering-zoomcamp/image_0bec8845.png"}], "question": "GCP BQ - Cannot read and write in different locations: source: <REGION_HERE>, destination: <ANOTHER_REGION_HERE>", "sort_order": 11}, "content": "Make sure to create the BigQuery dataset in the same location as your GCS Bucket. For instance, if your GCS Bucket was created in `us-central1`, then the BigQuery dataset must also be created in `us-central1`.\n\n<{IMAGE:image_1}>\n\n<{IMAGE:image_2}>"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-3/012_b98d573ce2_gcp-bq-remember-to-save-your-queries.md", "metadata": {"id": "b98d573ce2", "images": [{"description": "image #1", "id": "image_1", "path": "images/data-engineering-zoomcamp/image_85e101a4.png"}], "question": "GCP BQ: Remember to save your queries", "sort_order": 12}, "content": "It's important to save your progress in the BigQuery SQL Editor frequently.\n\nHere are some tips:\n\n- **Save Regularly:** Use the save button at the top bar in the BigQuery SQL Editor. Your saved queries will be available on the left panel.\n\n  <{IMAGE:image_1}>\n\n- **Alternative Method:** Copy and paste your queries into a file using a text editor like Notepad++ or VS Code. Save it with a `.sql` extension to benefit from syntax highlighting.\n\nBy following these methods, you can avoid losing your work in case of unexpected browser issues."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-3/013_57303f9d80_gcp-bq-can-i-use-bigquery-for-real-time-analytics.md", "metadata": {"id": "57303f9d80", "question": "GCP BQ: Can I use BigQuery for real-time analytics in this project?", "sort_order": 13}, "content": "While real-time analytics might not be explicitly mentioned, BigQuery has real-time data streaming capabilities, allowing for potential integration in future project iterations."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-3/014_1724e08426_gcp-bq-unable-to-load-data-from-external-tables-in.md", "metadata": {"id": "1724e08426", "question": "GCP BQ: Unable to load data from external tables into a materialized table in BigQuery due to an invalid timestamp error that are added while appending data to the file in Google Cloud Storage", "sort_order": 14}, "content": "```plaintext\ncould not parse 'pickup_datetime' as timestamp for field pickup_datetime (position 2)\n```\n\nThis error is caused by invalid data in the timestamp column. To resolve this issue:\n\n1. Define the schema of the external table using the `STRING` datatype for the timestamp column. This allows queries to execute without errors.\n2. Filter out the invalid timestamp rows during data import.\n3. Insert the filtered rows into the materialized table, specifying the `TIMESTAMP` datatype for the timestamp fields."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-3/015_c2a18da218_gcp-bq-error-message-in-bigquery-annotated-as-a-va.md", "metadata": {"id": "c2a18da218", "question": "GCP BQ: Error Message in BigQuery: annotated as a valid Timestamp, please annotate it as TimestampType(MICROS) or TimestampType(MILLIS)", "sort_order": 15}, "content": "When you encounter this BigQuery error, it typically relates to how timestamps are stored in Parquet files.\n\n### Solution:\n\nTo resolve this issue, you can modify the Parquet writing configuration by adding `use_deprecated_int96_timestamps=True` to the `pq.write_to_dataset` function. This setting writes timestamps in the INT96 format, which can be more compatible with BigQuery.\n\nHere’s how you can adjust the function:\n\n```python\npq.write_to_dataset(\n    table,\n    root_path=root_path,\n    filesystem=gcs,\n    use_deprecated_int96_timestamps=True  # Write timestamps to INT96 Parquet format\n)\n```\n\n### References\n\n- [Stack Overflow - Parquet compatibility with PyArrow vs PySpark](https://stackoverflow.com/questions/48314880/are-parquet-file-created-with-pyarrow-vs-pyspark-compatible)\n- [Stack Overflow - Editing Parquet files and datetime format errors](https://stackoverflow.com/questions/57798479/editing-parquet-files-with-python-causes-errors-to-datetime-format)\n- [Reddit - Parquet Timestamp to BQ issues](https://www.reddit.com/r/bigquery/comments/16aoq0u/parquet_timestamp_to_bq_coming_across_as_int/?share_id=YXqCs5Jl6hQcw-kg6-VgF&utm_content=1&utm_medium=ios_app&utm_name=ioscss&utm_source=share&utm_term=1)\n\nUse the above configuration to ensure compatibility with Google BigQuery when dealing with timestamps in Parquet files."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-3/016_a83247e572_gcp-bq-datetime-columns-in-parquet-files-created-f.md", "metadata": {"id": "a83247e572", "question": "GCP BQ - Datetime columns in Parquet files created from Pandas show up as integer columns in BigQuery", "sort_order": 16}, "content": "### Solution:\n\nIf you’re using Mage, in the last Data Exporter that writes to Google Cloud Storage, use PyArrow to generate the Parquet file with the correct logical type for the datetime columns. Otherwise, they won't be converted to a timestamp when loaded by BigQuery later on.\n\n```python\nimport pyarrow as pa\nimport pyarrow.parquet as pq\nimport os\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n# Replace with the location of your service account key JSON file.\nos.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '/home/src/personal-gcp.json'\n\nbucket_name = \"<YOUR_BUCKET_NAME>\"\nobject_key = 'nyc_taxi_data_2022.parquet'\nwhere = f'{bucket_name}/{object_key}'\n\n@data_exporter\ndef export_data(data, *args, **kwargs):\n    table = pa.Table.from_pandas(data, preserve_index=False)\n    gcs = pa.fs.GcsFileSystem()\n    pq.write_table(\n        table,\n        where,\n        # Convert integer columns in Epoch milliseconds\n        # to Timestamp columns in microseconds ('us') so\n        # they can be loaded into BigQuery with the right\n        # data type\n        coerce_timestamps='us',\n        filesystem=gcs\n    )\n```\n\n### Solution 2:\n\nIf you’re using Mage, in the last Data Exporter that writes to Google Cloud Storage, provide PyArrow with an explicit schema to generate the Parquet file with the correct logical type for the datetime columns.\n\n```python\nschema = pa.schema([\n    ('vendor_id', pa.int64()),\n    ('lpep_pickup_datetime', pa.timestamp('ns')),\n    ('lpep_dropoff_datetime', pa.timestamp('ns')),\n    ('store_and_fwd_flag', pa.string()),\n    ('ratecode_id', pa.int64()),\n    ('pu_location_id', pa.int64()),\n    ('do_location_id', pa.int64()),\n    ('passenger_count', pa.int64()),\n    ('trip_distance', pa.float64()),\n    ('fare_amount', pa.float64()),\n    ('extra', pa.float64()),\n    ('mta_tax', pa.float64()),\n    ('tip_amount', pa.float64()),\n    ('tolls_amount', pa.float64()),\n    ('improvement_surcharge', pa.float64()),\n    ('total_amount', pa.float64()),\n    ('payment_type', pa.int64()),\n    ('trip_type', pa.int64()),\n    ('congestion_surcharge', pa.float64()),\n    ('lpep_pickup_month', pa.int64())\n])\n\ntable = pa.Table.from_pandas(data, schema=schema)\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-3/017_7c97a61529_gcp-bq-create-external-table-using-python.md", "metadata": {"id": "7c97a61529", "question": "GCP: BQ - Create External Table using Python", "sort_order": 17}, "content": "Reference:\n\n[https://cloud.google.com/bigquery/docs/external-data-cloud-storage](https://cloud.google.com/bigquery/docs/external-data-cloud-storage)\n\nSolution:\n\n```python\nfrom google.cloud import bigquery\n\n# Set table_id to the ID of the table to create\n\ntable_id = f\"{project_id}.{dataset_name}.{table_name}\"\n\n# Construct a BigQuery client object\n\nclient = bigquery.Client()\n\n# Set the external source format of your table\n\nexternal_source_format = \"PARQUET\"\n\n# Set the source_uris to point to your data in Google Cloud\n\nsource_uris = [ f'gs://{bucket_name}/{object_key}/*']\n\n# Create ExternalConfig object with external source format\n\nexternal_config = bigquery.ExternalConfig(external_source_format)\n\n# Set source_uris that point to your data in Google Cloud\n\nexternal_config.source_uris = source_uris\n\nexternal_config.autodetect = True\n\ntable = bigquery.Table(table_id)\n\n# Set the external data configuration of the table\n\ntable.external_data_configuration = external_config\n\ntable = client.create_table(table)  # Make an API request.\n\nprint(f'Created table with external source: {table_id}')\n\nprint(f'Format: {table.external_data_configuration.source_format}')\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-3/018_88f2fb38b0_gcp-bq-check-bigquery-table-exist-and-delete.md", "metadata": {"id": "88f2fb38b0", "question": "GCP BQ: Check BigQuery Table Exist And Delete", "sort_order": 18}, "content": "### Reference\n\n[Stack Overflow - BigQuery Overwrite Table](https://stackoverflow.com/questions/60941726/can-bigquery-api-overwrite-existing-table-view-with-create-table-tables-inser)\n\n### Solution\n\nTo check if a BigQuery table exists and possibly delete it, utilize the following Python function before using `client.create_table`:\n\n```python\nfrom google.cloud import bigquery\n\n# Initialize client\nclient = bigquery.Client()\n\ndef table_exists(table_id, client):\n    \"\"\"\n    Check if a table already exists using the tableID.\n\n    :param table_id: str, the ID of the table\n    :param client: bigquery.Client instance\n    :return: Boolean\n    \"\"\"\n    try:\n        client.get_table(table_id)\n        return True\n    except Exception as e:  # NotFound:\n        return False\n```\n\nUse this function to check table existence before creating a new table or taking further actions."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-3/019_21e010f826_gcp-bq-error-missing-close-double-quote-character.md", "metadata": {"id": "21e010f826", "question": "GCP BQ - Error: Missing close double quote (\") character", "sort_order": 19}, "content": "To avoid this error, you can upload data from Google Cloud Storage to BigQuery through BigQuery Cloud Shell using the command:\n\n```bash\nbq load --autodetect --allow_quoted_newlines --source_format=CSV dataset_name.table_name \"gs://dtc-data-lake-bucketname/fhv/fhv_tripdata_2019-*.csv.gz\"\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-3/020_852b1d09a0_gcp-bq-cannot-read-and-write-in-different-location.md", "metadata": {"id": "852b1d09a0", "images": [{"description": "image #1", "id": "image_1", "path": "images/data-engineering-zoomcamp/image_199a39eb.jpg"}, {"description": "image #2", "id": "image_2", "path": "images/data-engineering-zoomcamp/image_e2a4f6bc.jpg"}, {"description": "image #3", "id": "image_3", "path": "images/data-engineering-zoomcamp/image_9f3cd4ef.jpg"}], "question": "GCP BQ - Cannot read and write in different locations: source: asia-south2, destination: US", "sort_order": 20}, "content": "Solution: This problem arises if your GCS and BigQuery storage are in different regions.\n\nOne potential way to solve it:\n\n- Go to your Google Cloud bucket and check the region in the field named \"Location.\"\n\n  <{IMAGE:image_1}>\n\n- In BigQuery, click on the three-dot icon near your project name and select \"Create dataset.\"\n\n  <{IMAGE:image_2}>\n\n- In the region field, choose the same region as your Google Cloud bucket.\n\n  <{IMAGE:image_3}>"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-3/021_7cbadfe131_gcp-bq-tip-using-cloud-function-to-read-csvgz-file.md", "metadata": {"id": "7cbadfe131", "question": "GCP BQ - Tip: Using Cloud Function to read csv.gz files from github directly to BigQuery in Google Cloud:", "sort_order": 21}, "content": "There are multiple benefits of using Cloud Functions to automate tasks in Google Cloud.\n\nUse the following Cloud Function Python script to load files directly into BigQuery. Set your project ID, dataset ID, and table ID accordingly.\n\n```python\nimport tempfile\nimport requests\nimport logging\nfrom google.cloud import bigquery\n\ndef hello_world(request):\n    # table_id = <project_id.dataset_id.table_id>\n    table_id = 'de-zoomcap-project.dezoomcamp.fhv-2019'\n\n    # Create a new BigQuery client\n    client = bigquery.Client()\n\n    for month in range(4, 13):\n        # Define the schema for the data in the CSV.gz files\n        url = 'https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhv/fhv_tripdata_2019-{:02d}.csv.gz'.format(month)\n\n        # Download the CSV.gz file from Github\n        response = requests.get(url)\n\n        # Create new table if loading first month data else append\n        write_disposition_string = \"WRITE_APPEND\" if month > 1 else \"WRITE_TRUNCATE\"\n\n        # Defining LoadJobConfig with schema of table to prevent it from changing with every table\n        job_config = bigquery.LoadJobConfig(\n            schema=[\n                bigquery.SchemaField(\"dispatching_base_num\", \"STRING\"),\n                bigquery.SchemaField(\"pickup_datetime\", \"TIMESTAMP\"),\n                bigquery.SchemaField(\"dropOff_datetime\", \"TIMESTAMP\"),\n                bigquery.SchemaField(\"PUlocationID\", \"STRING\"),\n                bigquery.SchemaField(\"DOlocationID\", \"STRING\"),\n                bigquery.SchemaField(\"SR_Flag\", \"STRING\"),\n                bigquery.SchemaField(\"Affiliated_base_number\", \"STRING\"),\n            ],\n            skip_leading_rows=1,\n            write_disposition=write_disposition_string,\n            autodetect=True,\n            source_format=\"CSV\",\n        )\n\n        # Load the data into BigQuery\n        # Create a temporary file to prevent the exception: AttributeError: 'bytes' object has no attribute 'tell'\n        with tempfile.NamedTemporaryFile() as f:\n            f.write(response.content)\n            f.seek(0)\n\n            job = client.load_table_from_file(\n                f,\n                table_id,\n                location=\"US\",\n                job_config=job_config,\n            )\n\n            job.result()\n\n        logging.info(\"Data for month %d successfully loaded into table %s.\", month, table_id)\n\n    return 'Data loaded into table {}.'.format(table_id)\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-3/022_8bfd81c403_gcp-bq-when-querying-two-different-tables-external.md", "metadata": {"id": "8bfd81c403", "images": [{"description": "image #1", "id": "image_1", "path": "images/data-engineering-zoomcamp/image_d9986dba.png"}, {"description": "image #2", "id": "image_2", "path": "images/data-engineering-zoomcamp/image_e9103d16.png"}], "question": "GCP BQ: When querying two different tables, external and materialized, why do you get the same result with count(distinct(*))?", "sort_order": 22}, "content": "You need to uncheck cache preferences in query settings\n\n<{IMAGE:image_1}>\n\n<{IMAGE:image_2}>"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-3/023_66d3c42dd4_gcp-bq-how-to-handle-type-error-from-bigquery-and.md", "metadata": {"id": "66d3c42dd4", "question": "GCP BQ: How to handle type error from BigQuery and Parquet data?", "sort_order": 23}, "content": "When injecting data into GCS using Pandas, some datasets might have missing values in the `DOlocationID` and `PUlocationID` columns. By default, Pandas will cast these columns as `float`, leading to inconsistent data types between the Parquet files in GCS and the schema defined in BigQuery. You might encounter the following error:\n\n```bash\nerror: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.\n```\n\n### Solution:\nFix the data type issue in the data pipeline:\n\n1. Before injecting data into GCS, use `astype` and `Int64` (which is different from `int64` and accepts both missing values and integers) to cast the columns.\n\n    Example:\n    ```python\n    df[\"PUlocationID\"] = df.PUlocationID.astype(\"Int64\")\n    df[\"DOlocationID\"] = df.DOlocationID.astype(\"Int64\")\n    ```\n\n2. It is best to define the data type of all the columns in the Transformation section of the ETL pipeline before loading to BigQuery."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-3/024_625c1f542a_gcp-bq-invalid-project-id-project-ids-must-contain.md", "metadata": {"id": "625c1f542a", "question": "GCP BQ: Invalid project ID. Project IDs must contain 6-63 lowercase letters, digits, or dashes.", "sort_order": 24}, "content": "The problem occurs when there is a misplacement of content after the `FROM` clause in BigQuery SQLs. Check to remove any extra spaces or symbols; ensure project IDs are in lowercase, digits, and dashes only."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-3/025_45b587e597_gcp-bq-does-bigquery-support-multiple-columns-part.md", "metadata": {"id": "45b587e597", "question": "GCP BQ: Does BigQuery support multiple columns partition?", "sort_order": 25}, "content": "No. Based on the documentation for BigQuery, it does not support more than one column to be partitioned.\n\n[Source](https://cloud.google.com/bigquery/docs/partitioned-tables#limitations)"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-3/026_49b5277d77_gcp-bq-date-error-in-bigquery.md", "metadata": {"id": "49b5277d77", "question": "GCP BQ: DATE() Error in BigQuery", "sort_order": 26}, "content": "**Error Message:**\n\n```\nPARTITION BY expression must be DATE(<timestamp_column>), DATE(<datetime_column>), DATETIME_TRUNC(<datetime_column>, DAY/HOUR/MONTH/YEAR), a DATE column, TIMESTAMP_TRUNC(<timestamp_column>, DAY/HOUR/MONTH/YEAR), DATE_TRUNC(<date_column>, MONTH/YEAR), or RANGE_BUCKET(<int64_column>, GENERATE_ARRAY(<int64_value>, <int64_value>[, <int64_value>]))\n```\n\n**Solution:**\n\nConvert the column to datetime first:\n\n```python\n# Convert pickup_datetime to datetime\ndf[\"pickup_datetime\"] = pd.to_datetime(df[\"pickup_datetime\"])\n\n# Convert dropOff_datetime to datetime\ndf[\"dropOff_datetime\"] = pd.to_datetime(df[\"dropOff_datetime\"])\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-3/027_8248dd9d7f_gcp-bq-when-trying-to-cluster-by-datetpep_pickup_d.md", "metadata": {"id": "8248dd9d7f", "question": "GCP BQ: When trying to cluster by DATE(tpep_pickup_datetime) it gives an error: Entries in the CLUSTER BY clause must be column names", "sort_order": 27}, "content": "No need to convert as you can cluster by a TIMESTAMP column directly in BigQuery. BigQuery supports clustering on TIMESTAMP, DATE, DATETIME, STRING, INT64, and BOOL types.\n\nClustering sorts data based on the timestamp to optimize queries with filters like `WHERE tpep_pickup_datetime BETWEEN ...`, rather than creating discrete partitions.\n\nIf your goal is to improve performance for time-based queries, combining partitioning by `DATE(event_time)` and clustering by `tpep_pickup_datetime` is a good approach."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-3/028_16b4ae94ef_gcp-bq-native-tables-vs-external-tables-in-bigquer.md", "metadata": {"id": "16b4ae94ef", "question": "GCP BQ - Native tables vs External tables in BigQuery?", "sort_order": 28}, "content": "Native tables are tables where the data is stored directly in BigQuery. In contrast, external tables store the data outside BigQuery, with BigQuery storing metadata about that external table.\n\n- **External tables:** These tables are not stored directly in BigQuery but are pulled in from a data lake such as Google Cloud Storage or AWS S3.\n- **Materialized table:** This is a copy of an external table with data stored in BigQuery, consuming storage space.\n\nResources:\n\n- [External Tables Documentation](https://cloud.google.com/bigquery/docs/external-tables)\n- [BigQuery Tables Introduction](https://cloud.google.com/bigquery/docs/tables-intro)"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-3/029_24c64415c0_why-does-my-partitioned-table-in-bigquery-show-as.md", "metadata": {"id": "24c64415c0", "question": "Why does my partitioned table in BigQuery show as non-partitioned even though BigQuery says it's partitioned?", "sort_order": 29}, "content": "If your partitioned table in BigQuery shows as non-partitioned, it may be due to a delay in updating the table's details in the UI. The table is likely partitioned, but it may not show the updated information immediately.\n\nHere’s what you can do:\n\n- **Refresh your BigQuery UI:** If you're already inspecting the table in the BigQuery UI, try refreshing the page after a few minutes to ensure the table details are updated correctly.\n\n- **Open a new tab:** Alternatively, try opening a new tab in BigQuery and inspect the table details again. This can sometimes help to load the most up-to-date information.\n\n- **Be patient:** In some cases, there might be a slight delay in reflecting changes, but the table is very likely partitioned."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-3/030_155949a31a_gcp-bq-ml-unable-to-run-command-shown-in-video-to.md", "metadata": {"id": "155949a31a", "question": "GCP BQ ML: Unable to run command (shown in video) to export ML model from BQ to GCS", "sort_order": 30}, "content": "**Issue:**\n\nTried running command to export ML model from BQ to GCS from Week 3:\n\n```bash\nbq --project_id taxi-rides-ny extract -m nytaxi.tip_model gs://taxi_ml_model/tip_model\n```\n\nIt is failing with the following error:\n\n```\nBigQuery error in extract operation: Error processing job Not found: Dataset was not found in location US\n```\n\nI verified the BQ dataset and GCS bucket are in the same region, `us-west1`. Not sure why it gets location `US`. I couldn’t find the solution yet.\n\n**Solution:**\n\nPlease ensure the following:\n\n- Enter the correct `project_id` and `gcs_bucket` folder address.\n- Example of a correct GCS bucket folder address:\n  \n  ```text\n  gs://dtc_data_lake_optimum-airfoil-376815/tip_model\n  ```"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-3/031_a32ed35da6_dim_zonessql-dataset-was-not-found-in-location-us.md", "metadata": {"id": "a32ed35da6", "question": "Dim_zones.sql Dataset was not found in location US When Running fact_trips.sql", "sort_order": 31}, "content": "To solve this error, specify the location as `US` when creating the `dim_zones` table:\n\n```sql\n{{ config(\n\nmaterialized='table',\n\nlocation='US'\n\n) }}\n```\n\nUpdate this part, re-run the `dim_zones` creation, and then run `fact_trips`."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-3/032_47f7c8310a_gcp-bq-ml-export-ml-model-to-make-predictions-does.md", "metadata": {"id": "47f7c8310a", "images": [{"description": "image #1", "id": "image_1", "path": "images/data-engineering-zoomcamp/image_51551549.png"}], "question": "GCP BQ ML - Export ML model to make predictions does not work for MacBook with Apple M1 chip (arm architecture).", "sort_order": 32}, "content": "**Solution:**\n\nProceed with setting up `serving_dir` on your computer as described in the `extract_model.md` file. Then, instead of using:\n\n```bash\ndocker pull tensorflow/serving\n```\n\nuse:\n\n```bash\ndocker pull emacski/tensorflow-serving\n```\n\nThen run:\n\n```bash\ndocker run -p 8500:8500 -p 8501:8501 --mount type=bind,source=`pwd`/serving_dir/tip_model,target=/models/tip_model -e MODEL_NAME=tip_model -t emacski/tensorflow-serving\n```\n\nAfter that, run the `curl` command as instructed, and you should get a prediction.\n\n**Or new since Oct 2024:**\n\nBeta release of Docker VMM - the more performant alternative to Apple Virtualization Framework on macOS (requires Apple Silicon and macOS 12.5 or later). [https://docs.docker.com/desktop/features/vmm/](https://docs.docker.com/desktop/features/vmm/)\n\n<{IMAGE:image_1}>"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-3/033_dcb8885c9b_vms-what-do-i-do-if-my-vm-runs-out-of-space.md", "metadata": {"id": "dcb8885c9b", "question": "VMs: What do I do if my VM runs out of space?", "sort_order": 33}, "content": "- Try deleting data you’ve saved to your VM locally during ETLs.\n- Kill processes related to deleted files.\n- Download `ncdu` and look for large files (pay particular attention to files related to Prefect).\n- If you delete any files related to Prefect, eliminate caching from your flow code."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-3/034_c36b3f5196_gcp-bq-external-and-regular-table.md", "metadata": {"id": "c36b3f5196", "question": "GCP BQ - External and regular table", "sort_order": 34}, "content": "**External Table**\n\n- Data remains stored in a Google Cloud Storage (GCS) bucket.\n\n**Regular Table**\n\n- Data is copied into BigQuery storage.\n\n**Example of creating an external table:**\n\n```sql\nCREATE OR REPLACE EXTERNAL TABLE `your_project.your_dataset.tablenamel`\nOPTIONS (\n  format = 'PARQUET',\n  uris = ['gs://your-bucket-name/yellow_tripdata_2024-*.parquet']\n);\n```\n\n**Example of creating a regular table from an external table:**\n\n```sql\nCREATE OR REPLACE TABLE `your_project.your_dataset.tablename`\nAS\nSELECT * FROM `your_project.your_dataset.yellow_taxi_external`;\n```\n\n**Directly loading data from GCS into a regular BigQuery table without creating an external table:**\n\n```sql\nCREATE OR REPLACE TABLE `your_project.your_dataset.yellow_taxi_table`\nOPTIONS (\n  format = 'PARQUET'\n) AS\nSELECT * FROM `your_project.your_dataset.external_table_placeholder`\nFROM EXTERNAL_QUERY(\n  'your_project.region-us.gcs_external',\n  'SELECT * FROM `gs://your-bucket-name/yellow_tripdata_2024-*.parquet`'\n);\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-3/035_18001c743b_can-bigquery-work-with-parquet-files-directly.md", "metadata": {"id": "18001c743b", "question": "Can BigQuery work with parquet files directly?", "sort_order": 35}, "content": "Yes, you can load your Parquet files directly into your GCP (Google Cloud Platform) Bucket first. Then, via BigQuery, you can create an external table of these Parquet files with a query statement like this:\n\n```sql\nCREATE OR REPLACE EXTERNAL TABLE `module-3-data-warehouse.taxi_data.external_yellow_tripdata_2024`\nOPTIONS (\n  format = 'PARQUET',\n  uris = ['gs://module3-dez/yellow_tripdata_2024-*.parquet']\n);\n```\n\nMake sure to adjust the SQL statement to your own situation and directories. The `*` symbol can be used as a wildcard to target Parquet files from all the months of 2024."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-3/036_a9c1fd2a8f_homework-what-does-it-mean-stop-with-loading-the-f.md", "metadata": {"id": "a9c1fd2a8f", "question": "Homework: What does it mean “Stop with loading the files into a bucket.”?", "sort_order": 36}, "content": "What they mean is that they don't want you to do anything more than that. You should load the files into the bucket and create an external table based on those files, but nothing like cleaning the data and putting it in parquet format."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-3/037_5e0cffbc79_homework-reading-parquets-from-nycgov-directly-int.md", "metadata": {"id": "5e0cffbc79", "question": "Homework: Reading parquets from nyc.gov directly into pandas returns Out of bounds error", "sort_order": 37}, "content": "If you try to read parquets directly from nyc.gov’s cloudfront into pandas, you might encounter this error:\n\n```python\npyarrow.lib.ArrowInvalid: Casting from timestamp[us] to timestamp[ns] would result in out of bounds\n```\n\n### Cause:\n\nThere is a data record where `dropOff_datetime` is set to the year 3019 instead of 2019. \n\nPandas uses \"timestamp[ns]\" and `int64` only allows a ~580-year range, centered on 2000. See `pd.Timestamp.max` and `pd.Timestamp.min`.\n\nThis becomes out of bounds when pandas tries to read it because 3019 > 2300 (approx value of `pd.Timestamp.max`).\n\n### Fix:\n\n1. **Use pyarrow to read the data:**\n\n   ```python\n   import pyarrow.parquet as pq\n   df = pq.read_table('fhv_tripdata_2019-02.parquet').to_pandas(safe=False)\n   ```\n\n   This will result in unusual timestamps for the offending record.\n\n2. **Read datetime columns separately:**\n\n   ```python\n   table = pq.read_table('taxi.parquet')\n   datetimes = ['list of datetime column names']\n   df_dts = pd.DataFrame()\n\n   for col in datetimes:\n       df_dts[col] = pd.to_datetime(table.column(col), errors='coerce')\n   ```\n\n   The `errors='coerce'` parameter will convert out-of-bounds timestamps into either the max or min.\n\n3. **Remove the offending rows using filter:**\n\n   ```python\n   import pyarrow.compute as pc\n\n   table = pq.read_table('taxi.parquet')\n\n   df = table.filter(\n       pc.less_equal(table[\"dropOff_datetime\"], pa.scalar(pd.Timestamp.max))\n   ).to_pandas()\n   ```"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-3/038_5f072b1cc7_homework-uploading-files-to-gcs-via-gui.md", "metadata": {"id": "5f072b1cc7", "question": "Homework: Uploading files to GCS via GUI", "sort_order": 38}, "content": "This can help avoid schema issues in the homework. Download files locally and use the 'upload files' button in GCS at the desired path. You can upload many files at once. You can also choose to upload a folder."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-3/039_c46378ea2a_homework-qn-5-the-partitionedclustered-table-isnt.md", "metadata": {"id": "c46378ea2a", "question": "Homework - Qn 5: The partitioned/clustered table isn’t giving me the prediction I expected", "sort_order": 39}, "content": "Take a careful look at the format of the dates in the question."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-3/040_53e0e2f585_homework-qn-6-did-anyone-get-an-exact-match-for-on.md", "metadata": {"id": "53e0e2f585", "question": "Homework: Qn 6: Did anyone get an exact match for one of the options given in Module 3 homework Q6?", "sort_order": 40}, "content": "Many people aren’t getting an exact match, but are very close to one of the options. It is suggested to choose the closest option."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-3/041_55b6d6256f_python-invalid-start-byte-error-message.md", "metadata": {"id": "55b6d6256f", "question": "Python: invalid start byte Error Message", "sort_order": 41}, "content": "```python\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0xa0 in position 41721: invalid start byte\n```\n\nSolution:\n\n1. When reading the data from the web into the pandas dataframe, specify the encoding:\n   \n   ```python\n   pd.read_csv(dataset_url, low_memory=False, encoding='latin1')\n   ```\n\n2. When writing the dataframe from the local system to GCS as a CSV, specify the encoding:\n   \n   ```python\n   df.to_csv(path_on_gsc, compression=\"gzip\", encoding='utf-8')\n   ```\n\nAlternative: Use `pd.read_parquet(url)`."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-3/042_0f2a26772e_python-generators-in-python.md", "metadata": {"id": "0f2a26772e", "question": "Python: Generators in Python", "sort_order": 42}, "content": "A generator is a function in Python that returns an iterator using the `yield` keyword.\n\nA generator is a special type of iterable, similar to a list or a tuple, but with a crucial difference. Instead of creating and storing all the values in memory at once, a generator generates values on-the-fly as you iterate over it. This makes generators memory-efficient, particularly when dealing with large datasets."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-3/043_577dfed594_python-easiest-way-to-read-multiple-files-at-the-s.md", "metadata": {"id": "577dfed594", "question": "Python: Easiest way to read multiple files at the same time?", "sort_order": 43}, "content": "The `read_parquet` function supports a list of files as an argument. The list of files will be merged into a single result table."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-3/044_c58d3563a6_python-these-wont-work-you-need-to-make-sure-you-u.md", "metadata": {"id": "c58d3563a6", "question": "Python: These won't work. You need to make sure you use Int64.", "sort_order": 44}, "content": "Incorrect:\n\n```python\ndf['DOlocationID'] = pd.to_numeric(df['DOlocationID'], downcast=integer)\n```\n\nor\n\n```python\ndf['DOlocationID'] = df['DOlocationID'].astype(int)\n```\n\nCorrect:\n\n```python\ndf['DOlocationID'] = df['DOlocationID'].astype('Int64')\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-4/001_b72ed00c7b_warning-when-run-load_yellow_data-python-script.md", "metadata": {"id": "b72ed00c7b", "question": "Warning when run load_yellow_data python script", "sort_order": 1}, "content": "### Warning Details:\n\n```\nRuntimeWarning: As the c extension couldn't be imported, google-crc32c is using a pure python implementation that is significantly slower. If possible, please configure a c build environment and compile extension warnings.warn(_SLOW_CRC32C_WARNING, RuntimeWarning)\n\nFailed to upload ./yellow_tripdata_2024-01.parquet to GCS: Timeout of 120.0s exceeded, last exception: ('Connection aborted.', timeout('The write operation timed out'))\n\nFailed to upload ./yellow_tripdata_2024-03.parquet to GCS: Timeout of 120.0s exceeded, last exception: ('Connection aborted.', timeout('The write operation timed out'))\n```\n\n### Issues:\n\n1. **google-crc32c Warning**: The Google Cloud Storage library is using a slow Python implementation instead of the optimized C version.\n2. **Upload Timeout Error**: Your file uploads are timing out after 120 seconds.\n\n### Solutions:\n\n1. **Install the C-optimized google-crc32c**\n   \n   ```bash\n   pip install --upgrade google-crc32c\n   ```\n\n2. **Fix Google Cloud Storage Upload Timeout**\n   \n   - **Solution 1: Increase Timeout**\n     \n     ```python\n     blob.upload_from_filename(file_path, timeout=300)  # Set timeout to 5 minutes\n     ```"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-4/002_44ff6ae3df_dbt-cloud-developer.md", "metadata": {"id": "44ff6ae3df", "question": "dbt cloud Developer", "sort_order": 2}, "content": "Please be aware that the demos are done using dbt cloud Developer licensing. Although Team license is available to you upon creation of dbt cloud account for 14 days, the interface won't fully match the demo-ed experience."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-4/003_df2ca336d5_dbt-config-error-on-cloud-ide-no-dbt_projectyml-fo.md", "metadata": {"id": "df2ca336d5", "question": "DBT-Config ERROR on CLOUD IDE: No dbt_project.yml found at expected path", "sort_order": 3}, "content": "### Error Details\n\n```\nNo dbt_project.yml found at expected path /usr/src/develop/user-70471823426120/environment-70471823422561/repository-70471823410839/dbt_project.yml\n```\n\n### Solution Steps\n\n1. **Verify Packages**:\n   - Confirm that every entry in `packages.yml` (and their transitive dependencies) includes a `dbt_project.yml` file.\n\n2. **Initialize Project**:\n   - Use the UI to initialize a new project.\n\n3. **Import Git Repo**:\n   - For importing a Git repository of an existing dbt project, follow the instructions available at:\n   \n     [Import a project by Git URL](https://docs.getdbt.com/docs/cloud/git/import-a-project-by-git-url)"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-4/004_c180431de3_dbt-cloud-production-error-prod-dataset-not-availa.md", "metadata": {"id": "c180431de3", "question": "DBT Cloud production error: prod dataset not available in location EU", "sort_order": 4}, "content": "**Problem:**\n\nI am trying to deploy my DBT models to production using DBT Cloud. The data should reside in BigQuery with the dataset location as EU. However, when running the model in production, a prod dataset is incorrectly created in BigQuery with a location of US. This leads to the build failing with the error:\n\n```\nERROR 404: project.dataset:prod not available in location EU\n```\n\nI have attempted various fixes, but I'm unsure if there is a simpler solution than creating my projects or buckets in the US location.\n\nNote: Everything functions properly in development mode; the issue arises only during job scheduling and execution in production.\n\n**Solution:**\n\n1. Manually create the `prod` dataset in BigQuery with the EU location specified.\n2. Rerun the production job."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-4/005_0f6e2d3813_how-do-i-solve-the-dbt-cloud-error-prod-was-not-fo.md", "metadata": {"id": "0f6e2d3813", "question": "How do I solve the Dbt Cloud error: prod was not found in location?", "sort_order": 5}, "content": "You might encounter this error when trying to run dbt in production after following the instructions in the video ‘DE Zoomcamp 4.4.1 - Deployment Using dbt Cloud (Alternative A’):\n\n```\nDatabase Error in model stg_yellow_tripdata (models/staging/stg_yellow_tripdata.sql)\nNot found: Dataset module-4-analytics-eng:prod was not found in location europe-west10\n```\n\nThis error is easily resolved. Here are two solutions to address this issue:\n\n**Solution #1: Matching the dataset's data location with the source dataset**\n\n- Set your ‘prod’ dataset's data location to match the data location of your ‘trips_data_all’ dataset in BigQuery. The dbt process works for the instructor because her ‘prod’ dataset is in the same region as her source data. Since your ‘trips_data_all’ is in europe-west10 (or another region besides US), your prod needs to be there too, not US (which is the default setting when dbt creates a dataset for you in BigQuery).\n\n**Solution #2: Changing the dataset to <development dataset>**\n\n1. Go to: Deploy / Environments / Production (your production environment) / Settings.\n2. Look at the Deployment credentials. There is an input field called Dataset. The input of ‘prod’ is likely here.\n3. Replace ‘prod’ with the name of the Dataset that you worked with during development (before moving to Production). This is the Dataset name inside your BigQuery where you successfully ran ‘dbt debug’ and ‘dbt build’.\n4. After saving, you are ready to rerun your Job!"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-4/006_29469cf158_setup-no-development-environment.md", "metadata": {"id": "29469cf158", "question": "Setup: No development environment", "sort_order": 6}, "content": "Error:\n\n```plaintext\nThis project does not have a development environment configured. Please create a development environment and configure your development credentials to use the dbt IDE.\n```\n\nThe error message provides guidance on resolving this issue. Follow the guide in the [dbt cloud setup documentation](https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/04-analytics-engineering/dbt_cloud_setup.md). Additional instructions can be found in the [video @1:42](https://youtu.be/J0XCDyKiU64?si=2CTg3H63wyJTf5Vy&t=102)."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-4/007_b4a5d32d6b_setup-connecting-dbt-cloud-with-bigquery-error.md", "metadata": {"id": "b4a5d32d6b", "question": "Setup: Connecting dbt Cloud with BigQuery Error", "sort_order": 7}, "content": "**Runtime Error**\n\ndbt was unable to connect to the specified database.\n\nThe database returned the following error:\n\n```plaintext\nDatabase Error\n\nAccess Denied: Project <project_name>: User does not have bigquery.jobs.create permission in project <project_name>.\n```\n\nCheck your database credentials and try again. For more information, visit:\n\n[docs.getdbt.com](https://docs.getdbt.com/docs/configure-your-profile)\n\n**Steps to resolve error in Google Cloud:**\n\n1. Navigate to IAM & Admin and select IAM.\n2. Click Grant Access if your newly created dbt service account isn't listed.\n3. In the New principals field, add your service account.\n4. Select a Role and search for BigQuery Job User to add.\n5. Go back to dbt Cloud project setup and test your connection.\n6. **Note:** Also add BigQuery Data Owner, Storage Object Admin, & Storage Admin to prevent permission issues later in the course.\n\n<{IMAGE:image_id}>"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-4/008_87fddc59b7_setup-failed-to-clone-repository.md", "metadata": {"id": "87fddc59b7", "question": "Setup: Failed to clone repository.", "sort_order": 8}, "content": "Error: Failed to clone repository.\n\n```\n$ git clone git@github.com:DataTalksClub/data-engineering-zoomcamp.git /usr/src/develop/...\n\nCloning into '/usr/src/develop/...\n\nWarning: Permanently added '[github.com](http://github.com/),140.82.114.4' (ECDSA) to the list of known hosts.\n\ngit@github.com: Permission denied (publickey).\n\nfatal: Could not read from remote repository.\n```\n\n**Issue:** You don’t have permissions to write to `DataTalksClub/data-engineering-zoomcamp.git`\n\n**Solutions:**\n\n1. **Clone the Forked Repository**\n   \n   Clone the repository using your forked repo, which contains your GitHub username. Then, specify the path as:\n   \n   ```\n   [your github username]/data-engineering-zoomcamp.git\n   ```\n\n2. **Create a Fresh Repo for dbt-lessons**\n   \n   Create a new repository for dbt-lessons. This approach is beneficial as it allows for branching and pull requests without affecting your other repositories. There's no need to create a subfolder for the dbt project files.\n\n3. **Use HTTPS Link**\n   \n   Switch to using an HTTPS link for cloning the repository if SSH access is not configured."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-4/009_32a878b1d5_errors-when-i-start-the-server-in-dbt-cloud-failed.md", "metadata": {"id": "32a878b1d5", "question": "Errors when I start the server in dbt cloud: Failed to start server. Permission denied (publickey)", "sort_order": 9}, "content": "```\nFailed to start server. Permission denied (publickey). fatal: Could not read from remote repository. Please make sure you have the correct access rights and the repository exists.\n```\n\nUse the deploy keys in dbt repo details to create a public key in your repo, the issue will be solved.\n\nSteps in detail:\n\n1. **Find dbt Cloud’s SSH Key**\n   - In dbt Cloud, go to **Settings > Account Settings > SSH Keys**\n   - Copy the public SSH key displayed there.\n\n2. **Add It to GitHub**\n   - Go to **GitHub > Settings > SSH and GPG Keys**\n   - Click \"New SSH Key\", name it \"dbt Cloud\", and paste the key.\n   - Click \"Add SSH Key\".\n\n3. **Try Restarting dbt Cloud**"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-4/010_eab34fffe6_dbt-job-triggered-by-pull-requests-is-disabled-pre.md", "metadata": {"id": "eab34fffe6", "question": "dbt: Job - Triggered by pull requests is disabled prerequisites when I try to create a new Continuous Integration job in dbt cloud.", "sort_order": 10}, "content": "Solution:\n\nCheck if you’re on the Developer Plan. As per the [prerequisites](https://docs.getdbt.com/docs/deploy/ci-jobs#prerequisites), you'll need to be enrolled in the Team Plan or Enterprise Plan to set up a CI Job in dbt Cloud.\n\n- If you're on the Developer Plan, you'll need to upgrade to utilize CI Jobs.\n\nNote: A user mentioned that while on the Team Plan (trial period), the option was still disabled. In this case, you might try other troubleshooting steps for the Developer (free) plan."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-4/011_99658f8d80_setup-your-ide-session-was-unable-to-start-please.md", "metadata": {"id": "99658f8d80", "question": "Setup: Your IDE session was unable to start. Please contact support.", "sort_order": 11}, "content": "**Issue:** If the DBT cloud IDE is loading indefinitely and then giving you this error.\n\n**Solution:**\n\n1. Check the `dbt_cloud_setup.md` file.\n2. Create an SSH Key.\n3. Use `git clone` to import the repo into the dbt project.\n4. Copy and paste the deploy key back in your repo settings."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-4/012_e8fe39dd54_dbt-i-am-having-problems-with-columns-datatype-whi.md", "metadata": {"id": "e8fe39dd54", "question": "DBT: I am having problems with columns datatype while running DBT/BigQuery", "sort_order": 12}, "content": "**Issue:** If you don’t define the column format while converting from CSV to Parquet, Python will \"choose\" based on the first rows.\n\n**Solution:** Define the schema while running the `web_to_gcp.py` pipeline.\n\nSebastian adapted the script:\n\n[GitHub Repository](https://github.com/sebastian2296/data-engineering-zoomcamp/blob/main/week_4_analytics_engineering/web_to_gcs.py)\n\nTo make the file work with gz files, add the following lines:\n\n- Ensure deletion of the file at the end of each iteration to avoid disk space issues:\n\n```python\nfile_name_gz = f\"{service}_tripdata_{year}-{month}.csv.gz\"\nopen(file_name_gz, 'wb').write(r.content)\nos.system(f\"gzip -d {file_name_gz}\")\nos.system(f\"rm {file_name_init}.*\")\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-4/013_45bd267149_parquet-parquet-column-ehail_fee-has-type-double-w.md", "metadata": {"id": "45bd267149", "question": "Parquet: “Parquet column 'ehail_fee' has type DOUBLE which does not match the target cpp_type INT64”", "sort_order": 13}, "content": "Reason: Parquet files have their own schema. Some Parquet files for green data have records with decimals in the `ehail_fee` column.\n\nThere are some possible fixes:\n\n1. **Drop `ehail_fee` column**\n   \n   Drop the `ehail_fee` column, as it is not used. For instance, when creating a partitioned table from the external table in BigQuery:\n   \n   ```sql\n   SELECT * EXCEPT (ehail_fee) FROM...\n   ```\n\n2. **Modify SQL model**\n   \n   Modify `stg_green_tripdata.sql` model with this line:\n   \n   ```sql\n   CAST(0 AS NUMERIC) AS ehail_fee\n   ```\n\n3. **Modify Airflow DAG**\n   \n   Modify the Airflow DAG to make the conversion and avoid the error:\n   \n   ```python\n   pv.read_csv(src_file, convert_options=pv.ConvertOptions(column_types={'ehail_fee': 'float64'}))\n   ```\n\n4. **Using Pandas**\n   \n   Fix using Pandas when importing the file from CSV into a DataFrame:\n\n   ```python\n   pd.from_csv(..., dtype=type_dict)\n   ```\n   \n   Note: Regular `int64` in Pandas (from the numpy library) does not accept null values (NaN). Use Pandas `Int64` instead. The `type_dict` is a Python dictionary mapping column names to dtypes.\n\nSources:\n\n- [Pandas read_csv Documentation](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html)\n- [Nullable integer data type — pandas 1.5.3 documentation](https://pandas.pydata.org/docs/user_guide/integer_na.html)"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-4/014_6435e27634_ingestion-when-attempting-to-use-the-provided-quic.md", "metadata": {"id": "6435e27634", "question": "Ingestion: When attempting to use the provided quick script to load trip data into GCS, you receive error Access Denied from the S3 bucket", "sort_order": 14}, "content": "If the provided URL isn’t working for you ([nyc-tlc.s3.amazonaws.com](https://nyc-tlc.s3.amazonaws.com/trip+data)/):\n\nWe can use the GitHub CLI to easily download the needed trip data from [GitHub](https://github.com/DataTalksClub/nyc-tlc-data) and manually upload to a GCS bucket.\n\nInstructions on how to download the CLI here: [GitHub](https://github.com/cli/cli)\n\nCommands to use:\n\n```bash\ngh auth login\n\ngh release list -R DataTalksClub/nyc-tlc-data\n\ngh release download yellow -R DataTalksClub/nyc-tlc-data\n\ngh release download green -R DataTalksClub/nyc-tlc-data\n```\n\nNow you can upload the files to a GCS bucket using the GUI."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-4/015_28f6392ce5_hack-to-load-yellow-and-green-trip-data-for-2019-a.md", "metadata": {"id": "28f6392ce5", "question": "Hack to load yellow and green trip data for 2019 and 2020", "sort_order": 15}, "content": "I initially followed [this script](https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/03-data-warehouse/extras/web_to_gcs.py) but it was taking too long for the yellow trip data. When I downloaded and uploaded the Parquet files directly to GCS, it worked, but there was a schema inconsistency issue when creating the BigQuery table.\n\nI found another solution shared on YouTube, which was suggested by Victoria. You can watch it here:\n\n[[Optional] Hack for loading data to BigQuery for Week 4 - YouTube](https://www.youtube.com/watch?v=Mork172sK_c&t=22s&ab_channel=Victoria)\n\nMake sure to watch until the end, as there are some required schema changes."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-4/016_cdbabdd71a_gcp-vm-all-of-sudden-ssh-stopped-working-for-my-vm.md", "metadata": {"id": "cdbabdd71a", "question": "GCP VM: All of sudden ssh stopped working for my VM after my last restart", "sort_order": 16}, "content": "One common cause experienced is lack of space after running Prefect several times. When running Prefect, check the folder `.prefect/storage` and delete the logs now and then to avoid the problem."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-4/017_6022cc0440_gcp-free-trial-account-error.md", "metadata": {"id": "6022cc0440", "question": "GCP FREE TRIAL ACCOUNT ERROR", "sort_order": 17}, "content": "If you're encountering an error when trying to create a GCP free trial account, and it's not related to country restrictions, credit/debit card problems, or IP issues, it might be a random problem. Here’s a workaround:\n\n- Ask friends in your country to try signing up for the free trial using their Gmail accounts and their debit/credit cards.\n- If one succeeds, you can temporarily use their Gmail to access the trial.\n\nThis method could help you bypass the issue!"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-4/018_c01d835b44_gcp-vm-if-you-have-lost-ssh-access-to-your-machine.md", "metadata": {"id": "c01d835b44", "images": [{"description": "image #1", "id": "image_1", "path": "images/data-engineering-zoomcamp/image_da7b68d7.png"}], "question": "GCP VM: If you have lost SSH access to your machine due to lack of space. Permission denied (publickey)", "sort_order": 18}, "content": "You can try to do these steps:\n\n<{IMAGE:image_1}>"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-4/019_acbc9e940e_dbt-when-running-your-first-dbt-model-if-it-fails.md", "metadata": {"id": "acbc9e940e", "question": "DBT: When running your first dbt model, if it fails with an error:", "sort_order": 19}, "content": "```\n404 Not found: Dataset was not found in location US\n\n404 Not found: Dataset eighth-zenith-372015:trip_data_all was not found in location us-west1\n```\n\nTo resolve this issue, follow these steps:\n\n1. **Verify Locations in BigQuery:**\n   - Go to BigQuery and check the location of both:\n     - The source dataset (e.g., `trips_data_all`).\n     - The schema you’re writing to. The name should be in the format `dbt_<first initial><last name>` if you didn’t change the default settings.\n\n2. **Check Region Consistency:**\n   - Ensure both datasets are in the same region. Typically, your source data is in your region, while the write location could be multi-regional (e.g., US).\n   - If there's a mismatch, delete the datasets and recreate them in the specified region with the correct naming format.\n\n3. **Specify Single-Region Location:**\n   - Instead of using a multi-regional location like `US`, specify the exact region (e.g., `US-east1`). Refer to [this Github comment](https://github.com/dbt-labs/dbt-bigquery/issues/19#issuecomment-635545315) for more details.\n   - Additional guidance is available in [this post](https://learningdataengineering540969211.wordpress.com/dbt-cloud-and-bigquery-an-effort-to-try-and-resolve-location-issues/).\n\n4. **Update Location in DBT Cloud:**\n   - Go to your profile page (top right drop-down --> profile).\n   - Under Credentials --> Analytics (or your customized name), click on BigQuery >.\n   - Hit Edit and update your location. You may need to re-upload your service account JSON to refresh your private key. Ensure the region matches exactly as specified in BigQuery.\n\nFollowing these steps should help resolve location-related errors when running your dbt models."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-4/020_e596dc3cbe_dbt-when-executing-dbt-run-after-installing-dbt-ut.md", "metadata": {"id": "e596dc3cbe", "question": "DBT: When executing dbt run after installing dbt-utils latest version i.e., 1.0.0 warning has generated", "sort_order": 20}, "content": "**Error**: `dbt_utils.surrogate_key` has been replaced by `dbt_utils.generate_surrogate_key`\n\n**Fix**:\n\n- Replace `dbt_utils.surrogate_key` with `dbt_utils.generate_surrogate_key` in `stg_green_tripdata.sql`."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-4/021_78e4da8fa6_when-executing-dbt-run-after-fact_tripssql-has-bee.md", "metadata": {"id": "78e4da8fa6", "images": [{"description": "image #1", "id": "image_1", "path": "images/data-engineering-zoomcamp/image_a3c7073f.png"}], "question": "When executing dbt run after fact_trips.sql has been created, the task failed with error: \"Access Denied: BigQuery BigQuery: Permission denied while globbing file pattern.\"", "sort_order": 21}, "content": "1. Fixed by adding the Storage Object Viewer role to the service account in use in BigQuery.\n\n2. Add the related roles to the service account in use in GCS.\n\n<{IMAGE:image_1}>"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-4/022_95b01285f5_when-you-are-getting-error-dbt_utils-not-found.md", "metadata": {"id": "95b01285f5", "question": "When you are getting error dbt_utils not found", "sort_order": 22}, "content": "To resolve the \"dbt_utils not found\" error, follow these steps:\n\n1. Create a `packages.yml` file in the main project directory and add the package metadata:\n   \n   ```yaml\n   packages:\n     - package: dbt-labs/dbt_utils\n       version: 0.8.0\n   ```\n   \n2. Run the following command:\n\n   ```bash\n   dbt deps\n   ```\n\n3. Press Enter."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-4/023_d4ec58fd16_lineage-is-currently-unavailable-check-that-your-p.md", "metadata": {"id": "d4ec58fd16", "question": "Lineage is currently unavailable. Check that your project does not contain compilation errors or contact support if this error persists.", "sort_order": 23}, "content": "Ensure you properly format your YAML file. Check the build logs if the run was completed successfully. You can expand the command history console (where you type the `--vars '{\"is_test_run\": \"false\"}'`) and click on any stage’s logs to expand and read error messages or warnings."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-4/024_55fac499b0_build-why-do-my-fact_trips-only-contain-a-few-days.md", "metadata": {"id": "55fac499b0", "question": "Build: Why do my Fact_trips only contain a few days of data?", "sort_order": 24}, "content": "Make sure you use:\n\n```bash\ndbt run --var 'is_test_run: false'\n```\nor\n\n```bash\ndbt build --var 'is_test_run: false'\n```\n\nWatch out for formatted text from this document: re-type the single quotes. If that does not work, use:\n\n```bash\n--vars '{\"is_test_run\": \"false\"}'\n```\n\nwith each phrase separately quoted."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-4/025_9cce7ad112_build-why-do-my-fact_trips-only-contain-one-month.md", "metadata": {"id": "9cce7ad112", "question": "Build: Why do my fact_trips only contain one month of data?", "sort_order": 25}, "content": "Check if you specified the `if_exists` argument correctly when writing data from GCS to BigQuery. \n\nWhen I wrote my automated flow for each month of the years 2019 and 2020 for green and yellow data, I had specified `if_exists=\"replace\"` while experimenting with the flow setup. Once you want to run the flow for all months in 2019 and 2020, make sure to set `if_exists=\"append\"`.\n\n- `if_exists=\"replace\"` will replace the whole table with only the month data that you are writing into BigQuery in that one iteration -> you end up with only one month in BigQuery (the last one you inserted).\n\n- `if_exists=\"append\"` will append the new monthly data -> you end up with data from all months."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-4/026_5a1a61b6a8_bigquery-returns-an-error-when-i-try-to-run-the-dm.md", "metadata": {"id": "5a1a61b6a8", "question": "BigQuery returns an error when I try to run the dm_monthly_zone_revenue.sql model.", "sort_order": 26}, "content": "After the second SELECT, change this line:\n\n```sql\n date_trunc('month', pickup_datetime) as revenue_month,\n```\n\nTo this line:\n\n```sql\n date_trunc(pickup_datetime, month) as revenue_month,\n```\n\nMake sure that \"month\" isn’t surrounded by quotes!"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-4/027_2e2f27a6c0_replace-dbt_utilssurrogate_key-field_a-field_b-fie.md", "metadata": {"id": "2e2f27a6c0", "question": "Replace: {{ dbt_utils.surrogate_key([ field_a, field_b, field_c, …, field_z ]) }}", "sort_order": 27}, "content": "For this use:\n\n```sql\n{{ dbt_utils.generate_surrogate_key([ field_a, field_b, field_c, …, field_z ]) }}\n```\n\nAdditionally, add a global variable in `dbt_project.yml`. (...)"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-4/028_ba210ca8ef_i-changed-location-in-dbt-but-dbt-run-still-gives.md", "metadata": {"id": "ba210ca8ef", "question": "I changed location in dbt, but dbt run still gives me an error", "sort_order": 28}, "content": "- Remove the dataset from BigQuery that was created by dbt.\n- Run `dbt run` again so that it will recreate the dataset in BigQuery with the correct location."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-4/029_3cc457b0de_dbt-i-ran-dbt-run-without-specifying-variable-whic.md", "metadata": {"id": "3cc457b0de", "images": [{"description": "image #1", "id": "image_1", "path": "images/data-engineering-zoomcamp/image_71e1a1ed.png"}], "question": "DBT: I ran dbt run without specifying variable which gave me a table of 100 rows. I ran again with the variable value specified but my table still has 100 rows in BQ.", "sort_order": 29}, "content": "Remove the dataset from BigQuery created by dbt and run again (with test disabled) to ensure the dataset created has all the rows.\n\nDBT: Why am I getting a new dataset after running my CI/CD Job? / What is this new dbt dataset in BigQuery?\n\nWhen you create the CI/CD job, under 'Compare Changes against an environment (Deferral)' make sure that you select 'No; do not defer to another environment'. Otherwise, dbt won’t merge your dev models into production models; it will create a new environment called ‘dbt_cloud_pr_number of pull request’\n\n<{IMAGE:image_1}>"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-4/030_a31defac51_why-do-we-need-the-staging-dataset.md", "metadata": {"id": "a31defac51", "images": [{"description": "image #1", "id": "image_1", "path": "images/data-engineering-zoomcamp/image_d2b29adb.png"}], "question": "Why do we need the Staging dataset?", "sort_order": 30}, "content": "<{IMAGE:image_1}>\n\nStaging, as the name suggests, acts as an intermediary between raw datasets and the final fact and dimension tables. It helps in transforming raw data into a more usable format. In staging, datasets are typically materialized as views rather than tables.\n\nIn the project, you focus on creating production and `dbt_name + trips_data_all`; the staging dataset serves its role behind the scenes."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-4/031_53afe4bee2_dbt-docs-served-but-not-accessible-via-browser.md", "metadata": {"id": "53afe4bee2", "question": "DBT: Docs Served but Not Accessible via Browser", "sort_order": 31}, "content": "Try removing the `network: host` line in `docker-compose`."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-4/032_04f55d5846_bigquery-adapter-404-not-found-dataset-was-not-fou.md", "metadata": {"id": "04f55d5846", "question": "BigQuery adapter: 404 Not found: Dataset was not found in location europe-west6", "sort_order": 32}, "content": "1. Go to **Account settings** > **Project** > **Analytics**.\n2. Click on your connection.\n3. Scroll down to **Location** and type in the GCP location exactly as displayed in GCP (e.g., `europe-west6`). You might need to reupload your GCP key.\n\n4. Delete your dataset in Google BigQuery (GBQ).\n5. Rebuild the project using the command:\n   \n   ```bash\n   dbt build\n   ```\n\n6. The newly built dataset should be in the correct location."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-4/033_f0ad880ffb_dbtgit-main-branch-is-read-only.md", "metadata": {"id": "f0ad880ffb", "question": "Dbt+git - Main branch is “read-only”", "sort_order": 33}, "content": "Create a new branch to edit. More on this can be found [here in the dbt docs](https://docs.getdbt.com/docs/collaborate/git/version-control-basics)."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-4/034_c1acd7331a_dbtgit-it-appears-that-i-cant-edit-the-files-becau.md", "metadata": {"id": "c1acd7331a", "question": "Dbt+git: It appears that I can't edit the files because I'm in read-only mode. Does anyone know how I can change that?", "sort_order": 34}, "content": "1. **Create a New Branch:**\n   - You need to create a new branch for development to make changes.\n   \n   ```bash\n   git checkout -b your-feature-branch\n   ```\n   \n2. **Switch to the New Branch:**\n   - This allows you to make edits.\n\n   ```bash\n   git checkout your-feature-branch\n   ```\n\n3. **Commit and Push Changes:**\n   - Once you've made your changes, commit and push them to the main branch.\n\n   ```bash\n   git add .\n   git commit -m \"Your commit message\"\n   git push origin your-feature-branch\n   ```\n\n4. **Merge Changes to Main:**\n   - Finally, merge your changes from your feature branch to the main branch."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-4/035_3d51ece7b9_dbt-deploy-git-ci-cannot-create-ci-checks-job-for.md", "metadata": {"id": "3d51ece7b9", "images": [{"description": "image #1", "id": "image_1", "path": "images/data-engineering-zoomcamp/image_c35c101c.png"}], "question": "Dbt deploy + Git CI: cannot create CI checks job for deployment to Production", "sort_order": 35}, "content": "**Error:**\n\n```\nTriggered by pull requests\n\nThis feature is only available for dbt repositories connected through dbt Cloud's native integration with Github, Gitlab, or Azure DevOps\n```\n\n**Solution:**\n\nContrary to the [guide on DTC repo](https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/04-analytics-engineering/dbt_cloud_setup.md), don’t use the Git Clone option. Use the Github one instead. A step-by-step guide to unlink Git Clone and relink with Github is available in the next entry.\n\n<{IMAGE:image_1}>"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-4/036_1c4ac5d626_dbt-deploy-git-ci-unable-to-configure-continuous-i.md", "metadata": {"id": "1c4ac5d626", "images": [{"description": "image #1", "id": "image_1", "path": "images/data-engineering-zoomcamp/image_7800f401.png"}, {"description": "image #2", "id": "image_2", "path": "images/data-engineering-zoomcamp/image_8efd4f76.png"}, {"description": "image #3", "id": "image_3", "path": "images/data-engineering-zoomcamp/image_4e68416b.png"}], "question": "Dbt deploy + Git CI - Unable to configure Continuous Integration (CI) with Github", "sort_order": 36}, "content": "If you’re trying to configure CI with Github and on the job’s options you can’t see `Run on Pull Requests?` on triggers, follow these steps to reconnect using a native connection instead of cloning by SSH:\n\n1. **Connect Your Github Account**  \n   - Go to `Profile Settings > Linked Accounts`.\n   - Connect your Github account with the dbt project, allowing the requested permissions.  \n   - More information can be found [here](https://docs.getdbt.com/docs/collaborate/git/connect-github).\n   \n   <{IMAGE:image_1}>\n   \n2. **Disconnect Current Configuration**  \n   - Navigate to `Account Settings > Projects (analytics) > Github connection`.\n   - Click the `Disconnect` button at the bottom left.\n\n3. **Reconfigure Github Connection**  \n   - After disconnecting, configure again by choosing Github.\n   - Select your repository from all allowed repositories to work with dbt. Your setup will now be ready.\n   \n   <{IMAGE:image_2}>\n   \n4. **Configure Triggers**  \n   - Go to `Deploy > Job Configuration`.\n   - Scroll down to `Triggers` where you can see the option `Run on Pull Requests:`\n   \n   <{IMAGE:image_3}>"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-4/037_c37b2ed3a5_compilation-error-model-modelmy_new_projectstg_gre.md", "metadata": {"id": "c37b2ed3a5", "images": [{"description": "image #1", "id": "image_1", "path": "images/data-engineering-zoomcamp/image_ea2e4bf2.png"}], "question": "Compilation Error: Model 'model.my_new_project.stg_green_tripdata' (models/staging/stg_green_tripdata.sql) depends on a source named 'staging.green_trip_external' which was not found", "sort_order": 37}, "content": "If you're following video DE Zoomcamp 4.3.1 - Building the First DBT Models, you may encounter an issue at 14:25 where the Lineage graph isn't displayed and a Compilation Error occurs, as shown in the attached image.\n\nDon't worry—a quick fix for this is to simply save your `schema.yml` file. Once you've done this, you should be able to view your Lineage graph without any further issues.\n\n<{IMAGE:image_1}>"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-4/038_8bfd724e4f_compilation-error-in-test-accepted_values_stg_gree.md", "metadata": {"id": "8bfd724e4f", "question": "Compilation Error in test accepted_values_stg_green_tripdata_Payment_type__False___var_payment_type_values_ (models/staging/schema.yml)  'NoneType' object is not iterable", "sort_order": 38}, "content": "In the macro `test_accepted_values` (found in `tests/generic/builtin.sql`), an error was triggered by the test `accepted_values_stg_green_tripdata_Payment_type__False___var_payment_type_values_` located in `models/staging/schema.yml`.\n\nTo resolve this issue, ensure the following variable is added to your `dbt_project.yml` file:\n\n```yaml\nvars:\n  payment_type_values: [1, 2, 3, 4, 5, 6]\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-4/039_89733da275_dbt-macro-errors-with-get_payment_type_description.md", "metadata": {"id": "89733da275", "images": [{"description": "image #1", "id": "image_1", "path": "images/data-engineering-zoomcamp/image_c3c0865e.png"}], "question": "dbt: macro errors with get_payment_type_description(payment_type)", "sort_order": 39}, "content": "You will face this issue if you copied and pasted the exact macro directly from the data-engineering-zoomcamp repo.\n\n### Error Message\n\n```\nBigQuery adapter: Retry attempt 1 of 1 after error: BadRequest('No matching signature for operator CASE for argument types: STRING, INT64, STRING, INT64, STRING, INT64, STRING, INT64, STRING, INT64, STRING, INT64, STRING, NULL at [35:5]; reason: invalidQuery, location: query, message: No matching signature for operator CASE for argument types: STRING, INT64, STRING, INT64, STRING, INT64, STRING, INT64, STRING, INT64, STRING, INT64, STRING, NULL at [35:5]')\n```\n\n### Solution\n\nTo resolve this issue, change the data type of the numbers (1, 2, 3, etc.) to text by enclosing them in quotes. The `payment_type` data type should be a string.\n\n#### Updated Macro\n\n```jinja\n{#\nThis macro returns the description of the payment_type\n#}\n\n{% macro get_payment_type_description(payment_type) -%}\n\ncase {{ payment_type }}\n\nwhen '1' then 'Credit card'\n\nwhen '2' then 'Cash'\n\nwhen '3' then 'No charge'\n\nwhen '4' then 'Dispute'\n\nwhen '5' then 'Unknown'\n\nwhen '6' then 'Voided trip'\n\nend\n\n{%- endmacro %}\n```\n\n<{IMAGE:image_1}>"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-4/040_2d05496ddf_troubleshooting-in-dbt.md", "metadata": {"id": "2d05496ddf", "question": "Troubleshooting in dbt:", "sort_order": 40}, "content": "The dbt error log contains a link to BigQuery. When you follow it, you will see your query, and the problematic line will be highlighted."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-4/041_58586a16a6_dbt-why-changing-the-target-schema-to-marts-actual.md", "metadata": {"id": "58586a16a6", "question": "DBT: Why changing the target schema to “marts” actually creates a schema named “dbt_marts” instead?", "sort_order": 41}, "content": "It is a default behavior of dbt to [append custom schema to the initial schema](https://docs.getdbt.com/docs/build/custom-schemas#why-does-dbt-concatenate-the-custom-schema-to-the-target-schema). To override this behavior, create a macro named `generate_schema_name.sql`:\n\n```sql\n{% macro generate_schema_name(custom_schema_name, node) -%}\n\n{%- set default_schema = target.schema -%}\n\n{%- if custom_schema_name is none -%}\n\n{{ default_schema }}\n\n{%- else -%}\n\n{{ custom_schema_name | trim }}\n\n{%- endif -%}\n\n{%- endmacro %}\n```\n\nNow you can override the default custom schema in `dbt_project.yml`."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-4/042_c7ca760fed_how-to-set-subdirectory-of-the-github-repository-a.md", "metadata": {"id": "c7ca760fed", "images": [{"description": "image #1", "id": "image_1", "path": "images/data-engineering-zoomcamp/image_25bb53c3.png"}], "question": "How to set subdirectory of the github repository as the dbt project root", "sort_order": 42}, "content": "There is a project setting which allows you to set `Project subdirectory` in dbt cloud:\n\n<{IMAGE:image_1}>"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-4/043_3ce6d0445f_compilation-error-model-modelxxx-modelsmodel_pathx.md", "metadata": {"id": "3ce6d0445f", "question": "Compilation Error: Model 'model.XXX' (models/<model_path>/XXX.sql) depends on a source named '<a table name>' which was not found", "sort_order": 43}, "content": "Remember to modify your `.sql` models to read from existing table names in BigQuery/Postgres DB.\n\nExample:\n\n```sql\nselect * from {{ source('staging', '<your table name in the database>') }}\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-4/044_2d17397995_compilation-error-model-model_name-model_path-depe.md", "metadata": {"id": "2d17397995", "question": "Compilation Error: Model '<model_name>' (<model_path>) depends on a node named '<seed_name>' which was not found (Production Environment)", "sort_order": 44}, "content": "Make sure that you create a pull request from your Development branch to the Production branch (main by default). After that, check in your `seeds` folder to ensure the seed file is inside it. Another thing to check is your `.gitignore` file. Make sure that the `.csv` extension is not included."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-4/045_a17cb536b1_when-executing-dbt-run-after-using-fhv_tripdata-as.md", "metadata": {"id": "a17cb536b1", "question": "When executing dbt run after using fhv_tripdata as an external table: you get \"Access Denied: BigQuery BigQuery: Permission denied\"", "sort_order": 45}, "content": "1. Go to your dbt cloud service account.\n\n2. Add the `Storage Object Admin` and `Storage Admin` roles in addition to `BigQuery Admin`."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-4/046_b26658a658_how-to-automatically-infer-the-column-data-type-pa.md", "metadata": {"id": "b26658a658", "question": "How to automatically infer the column data type (pandas missing value issues)?", "sort_order": 46}, "content": "Problem: When injecting data to BigQuery, you may face a type error. This is because pandas by default will parse integer columns with missing values as float type.\n\nSolution:\n\nOne way to solve this problem is to specify or cast the data type as `Int64` during the data transformation stage.\n\nIf specifying all the integer columns is inconvenient, you can use `convert_dtypes` to infer the data type automatically.\n\n- Make pandas infer the correct data type (as pandas parse int with missing as float):\n\n```python\n# Fill missing values with a placeholder\n df.fillna(-999999, inplace=True)\n\n# Infer data types\n df = df.convert_dtypes()\n\n# Replace placeholder with None\n df = df.replace(-999999, None)\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-4/047_fd607affe5_when-loading-github-repo-raise-exception-that-taxi.md", "metadata": {"id": "fd607affe5", "question": "When loading GitHub repo raise exception that 'taxi_zone_lookup' not found", "sort_order": 47}, "content": "Seed files are loaded from a directory named 'seed'. Therefore, you should rename the directory currently named 'data' to 'seed'."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-4/048_9c40a115f8_taxi_zone_lookup-not-found.md", "metadata": {"id": "9c40a115f8", "question": "‘taxi_zone_lookup’ not found", "sort_order": 48}, "content": "Check the `.gitignore` file and make sure you don’t have `*.csv` in it.\n\nIf you're encountering a dbt error with the following message:\n\n```\nRuntime Error in rpc request (from remote system.sql)\n404 Not found: Table dtc-de-0315:trips_data_all.green_tripdata_partitioned was not found in location europe-west6\nLocation: europe-west6\nJob ID: 168ee9bd-07cd-4ca4-9ee0-4f6b0f33897c\n```\n\n- Verify that all your datasets are configured with the correct region. For example, use `europe-west6` instead of a general region like `EU`.\n- To update the region in dbt settings:\n  - Go to `dbt -> projects -> optional settings` \n  - Manually set the location to match the required region."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-4/049_1f4fd984f5_data-type-errors-when-ingesting-with-parquet-files.md", "metadata": {"id": "1f4fd984f5", "question": "Data type errors when ingesting with parquet files", "sort_order": 49}, "content": "The easiest way to avoid these errors is by ingesting the relevant data in a `.csv.gz` file type. Then, do:\n\n```sql\nCREATE OR REPLACE EXTERNAL TABLE `dtc-de.trips_data_all.fhv_tripdata`\nOPTIONS (\n  format = 'CSV',\n  uris = ['gs://dtc_data_lake_dtc-de-updated/data/fhv_all/fhv_tripdata_2019-*.csv.gz']\n);\n```\n\nThis example should help you avoid data type issues for week 4."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-4/050_d9072c7279_inconsistent-number-of-rows-when-re-running-fact_t.md", "metadata": {"id": "d9072c7279", "question": "Inconsistent number of rows when re-running fact_trips model", "sort_order": 50}, "content": "This issue arises from the way deduplication is handled in two staging files.\n\n**Solution:**\n\n- Add an `ORDER BY` clause in the `PARTITION BY` section of both staging files.\n- Continue adding columns to the `ORDER BY` clause until the row count in the `fact_trips` table is consistent upon re-running the model.\n\n**Explanation:**\n\nWe partition by `vendor_id` and `pickup_datetime`, selecting the first row (`rn=1`) from these partitions. These partitions lack an order, so every execution might yield a different first row. The inconsistency leads to different rows being processed, possibly with or without an unknown borough. Consequently, the `fact_trips` model discards a varying number of rows based on the presence of unknown boroughs."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-4/051_cbaecfefaf_data-type-error-when-running-fact-table.md", "metadata": {"id": "cbaecfefaf", "question": "Data Type Error when running fact table", "sort_order": 51}, "content": "If you encounter a data type error on the `trip_type` column, it may be due to some `nan` values that aren't null in BigQuery.\n\n**Solution:** Try casting it to `FLOAT` datatype instead of `NUMERIC`. \n\n```sql\nSELECT CAST(trip_type AS FLOAT) FROM your_table;\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-4/052_1d2222e38a_create-table-has-columns-with-duplicate-name-locat.md", "metadata": {"id": "1d2222e38a", "question": "CREATE TABLE has columns with duplicate name locationid.", "sort_order": 52}, "content": "This error could result if you are using a `SELECT *` query without specifying the table names.\n\nExample:\n\n```sql\nWITH dim_zones AS (\n  SELECT * FROM `engaged-cosine-374921`.`dbt_victoria_mola`.`dim_zones`\n  WHERE borough != 'Unknown'\n),\nfhv AS (\n  SELECT * FROM `engaged-cosine-374921`.`dbt_victoria_mola`.`stg_fhv_tripdata`\n)\nSELECT * FROM fhv\nINNER JOIN dim_zones AS pickup_zone\nON fhv.PUlocationID = pickup_zone.locationid\nINNER JOIN dim_zones AS dropoff_zone\nON fhv.DOlocationID = dropoff_zone.locationid;\n```\n\nTo resolve, replace with:\n\n```sql\nSELECT fhv.* FROM fhv\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-4/053_9f2048fc73_bad-int64-value-00-error.md", "metadata": {"id": "9f2048fc73", "question": "Bad int64 value: 0.0 error", "sort_order": 53}, "content": "Some ehail fees are null, causing a `Bad int64 value: 0.0` error when casting them to integer.\n\nSolution:\n\n- Use `safe_cast`, which returns NULL instead of throwing an error. Implement `safe_cast` from the `dbt_utils` function in Jinja code for casting into integer as follows:\n\n  ```jinja\n  {{ dbt_utils.safe_cast('ehail_fee', api.Column.translate_type(\"integer\")) }} as ehail_fee,\n  ```\n\n- Alternatively, use `safe_cast(ehail_fee as integer)` without relying on `dbt_utils`."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-4/054_0c2badaa56_bad-int64-value-2010-error.md", "metadata": {"id": "0c2badaa56", "question": "Bad int64 value: 2.0/1.0 error", "sort_order": 54}, "content": "You might encounter this when building the `fact_trips.sql` model. The issue may be with the `payment_type_description` field.\n\nUsing `safe_cast` as above would cause the entire field to become null. A better approach is to drop the offending decimal place, then cast to integer:\n\n```sql\ncast(replace({{ payment_type }},'.0','') as integer)\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-4/055_583d28267f_bad-int64-value-10-error-again.md", "metadata": {"id": "583d28267f", "question": "Bad int64 value: 1.0 error (again)", "sort_order": 55}, "content": "I found that there are more columns causing the bad INT64: `ratecodeid` and `trip_type` on `Green_tripdata` table. You can use the queries below to address them:\n\n```sql\nCAST(\n    REGEXP_REPLACE(CAST(rate_code AS STRING), r'\\.0', '') AS INT64\n) AS ratecodeid,\n\nCAST(\n    CASE\n        WHEN REGEXP_CONTAINS(CAST(trip_type AS STRING), r'\\.\\d+') THEN NULL\n        ELSE CAST(trip_type AS INT64)\n    END AS INT64\n) AS trip_type\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-4/056_0fbc18e0f2_dbt-error-on-building-fact_tripssql-parquet-column.md", "metadata": {"id": "0fbc18e0f2", "question": "DBT: Error on building fact_trips.sql - Parquet column 'ehail_fee' has type DOUBLE which does not match the target cpp_type INT64.", "sort_order": 56}, "content": "To resolve the error regarding the 'ehail_fee' column type mismatch, you can use the following line in `stg_green_trips.sql` to replace the original `ehail_fee` line:\n\n```sql\n{{ dbt.safe_cast('ehail_fee', api.Column.translate_type(\"numeric\")) }} as ehail_fee,\n```\n\nThis ensures that the column type is correctly converted to match the expected type."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-4/057_57767e983b_the-vars-argument-must-be-a-yaml-dictionary-but-wa.md", "metadata": {"id": "57767e983b", "question": "The - vars argument must be a YAML dictionary, but was of type str", "sort_order": 57}, "content": "Remember to add a space between the variable and the value. Otherwise, it won't be interpreted as a dictionary.\n\nCorrect usage:\n\n```bash\ndbt run --var 'is_test_run: false'\n```\n\nNot able to change Environment Type as it is greyed out and inaccessible. You don't need to change the environment type. If you are following the videos, you are creating a Production Deployment, so the only available option is the correct one."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-4/058_ec6d89a462_access-denied-table-yellow_tripdata-user-does-not.md", "metadata": {"id": "ec6d89a462", "images": [{"description": "image #1", "id": "image_1", "path": "images/data-engineering-zoomcamp/image_ee3efac5.png"}], "question": "Access Denied: Table yellow_tripdata: User does not have permission to query table yellow_tripdata, or perhaps it does not exist in location US.", "sort_order": 58}, "content": "<{IMAGE:image_1}>\n\n### Error Details\n\n```\nDatabase Error in model stg_yellow_tripdata (models/staging/stg_yellow_tripdata.sql)\n\nAccess Denied: Table taxi-rides-ny-339813-412521:trips_data_all.yellow_tripdata: User does not have permission to query table taxi-rides-ny-339813-412521:trips_data_all.yellow_tripdata, or perhaps it does not exist in location US.\n\ncompiled Code at target/run/taxi_rides_ny/models/staging/stg_yellow_tripdata.sql\n```\n\n### Solution\n\n1. **Branch Verification**:\n   - Ensure you are working on the correct branch. If not, switch to the appropriate branch.\n   \n2. **Schema Configuration**:\n   - Edit the `04-analytics-engineering/taxi_rides_ny/models/staging/schema.yml` file.\n   - Ensure the configuration is correct:\n     \n     ```yaml\n     sources:\n     - name: staging\n       database: your_database_name\n     ```\n\n3. **Custom Branch Setup in dbt Cloud**:\n   - If the error persists, consider running the dbt job on a custom branch:\n     \n     - Navigate to the environment settings in dbt Cloud.\n     - In General settings, select \"Only run on a custom branch\".\n     - Enter the name of your custom branch (e.g., HW).\n     - Click Save."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-4/059_8ab8329fea_could-not-parse-the-dbt-project-please-check-that.md", "metadata": {"id": "8ab8329fea", "images": [{"description": "image #1", "id": "image_1", "path": "images/data-engineering-zoomcamp/image_403eb7c5.png"}], "question": "Could not parse the dbt project. Please check that the repository contains a valid dbt project.", "sort_order": 59}, "content": "Running the Environment on the master branch causes this error. You must activate the “Only run on a custom branch” checkbox and specify the branch you are working on when the Environment is set up.\n\n<{IMAGE:image_1}>"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-4/060_ee3f7f89ba_made-change-to-your-modeling-files-and-commit-to-y.md", "metadata": {"id": "ee3f7f89ba", "question": "Made change to your modeling files and commit to your development branch, but Job still runs on old file?", "sort_order": 60}, "content": "Switch to the main branch and make a pull request from the development branch. This will take you to GitHub. Approve the merge and rerun your job; it should work as planned now."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-4/061_dfd4ddbf39_setup-ive-set-github-and-bigquery-to-dbt-successfu.md", "metadata": {"id": "dfd4ddbf39", "question": "Setup: I’ve set Github and Bigquery to dbt successfully. Why nothing showed in my Develop tab?", "sort_order": 61}, "content": "Before you can develop some data model on dbt, you should:\n\n1. **Create a Development Environment:** Ensure that your development environment is properly configured.\n   \n2. **Set Parameters:** Specify necessary parameters within the environment.\n   \nOnce the model has been developed, also create a deployment environment to create and run jobs."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-4/062_5b7577406d_bigquery-returns-an-error-when-i-try-to-run-dbt-ru.md", "metadata": {"id": "5b7577406d", "question": "BigQuery returns an error when I try to run ‘dbt run’:", "sort_order": 62}, "content": "My taxi data was loaded into GCS with `etl_web_to_gcs.py`, which converts CSV data into Parquet. Then I placed raw data trips into external tables, and when I executed `dbt run`, I got an error message:\n\n```plaintext\nParquet column 'passenger_count' has type INT64 which does not match the target cpp_type DOUBLE.\n```\n\nThis is because several columns in the files have different data formats.\n\nTo resolve this error, I added the transformation:\n\n```python\ndf[col] = df[col].astype('Int64')\n```\n\nApply this transformation to the following columns:\n\n- `passenger_count`\n- `payment_type`\n- `RatecodeID`\n- `VendorID`\n- `trip_type`\n\nAfter making these changes, the process completed successfully."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-4/063_778635395f_dbt-running-dbt-run-models-stg_green_tripdata-var.md", "metadata": {"id": "778635395f", "question": "DBT: Running `dbt run --models stg_green_tripdata --var 'is_test_run: false'` is not returning anything:", "sort_order": 63}, "content": "Use the syntax below instead if the code in the tutorial is not working.\n\n```bash\ndbt run --select stg_green_tripdata --vars '{\"is_test_run\": false}'\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-4/064_e5b8322bee_dbt-error-no-module-named-pytz-while-setting-up-db.md", "metadata": {"id": "e5b8322bee", "question": "DBT: Error: No module named 'pytz' while setting up dbt with docker", "sort_order": 64}, "content": "Following dbt with [BigQuery on Docker readme.md](https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/04-analytics-engineering/docker_setup/README.md), after running `docker-compose build` and `docker-compose run dbt-bq-dtc init`, you might encounter the error:\n\n```\nModuleNotFoundError: No module named 'pytz'\n```\n\nSolution:\n\nAdd the following line in the Dockerfile under `FROM --platform=$build_for python:3.9.9-slim-bullseye as base`:\n\n```bash\nRUN python -m pip install --no-cache pytz\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-4/065_eda58f442a_vs-code-nopermissions-filesystemerror-error-eacces.md", "metadata": {"id": "eda58f442a", "question": "VS Code: NoPermissions (FileSystemError): Error: EACCES: permission denied (linux)", "sort_order": 65}, "content": "If you encounter problems editing `dbt_project.yml` when using Docker after running `docker-compose run dbt-bq-dtc init`, to change the profile ‘taxi_rides_ny’ to 'bq-dbt-workshop’, execute the following command:\n\n```bash\nsudo chown -R username path\n```\n\nIf you see the error \"DBT - Internal Error: Profile should not be None if loading is completed\" when running `dbt debug`, change the directory to the newly created subdirectory, such as the `taxi_rides_ny` directory, which contains the dbt project."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-4/066_9179edaff3_google-cloud-bigquery-location-problems.md", "metadata": {"id": "9179edaff3", "question": "Google Cloud BigQuery: Location Problems", "sort_order": 66}, "content": "When running a query on BigQuery, you might encounter the error: **\"This table is not on the specified location\"**.\n\nTo resolve this issue, consider the following steps:\n\n- **Check the Locations**: Ensure the locations of your bucket, datasets, and tables are consistent. They should all reside in the same location.\n\n- **Modify Query Settings**:\n  1. Go to the query window.\n  2. Select **More** -> **Query Settings**.\n  3. Select the correct location.\n\n- **Verify Table Paths**: Double-check the paths in your query:\n  1. Click on the table.\n  2. Go to **Details**.\n  3. Copy the correct path."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-4/067_dc51ef9830_dbt-deploy-this-dbt-cloud-run-was-cancelled-becaus.md", "metadata": {"id": "dc51ef9830", "images": [{"description": "image #1", "id": "image_1", "path": "images/data-engineering-zoomcamp/image_c27ecb8e.png"}, {"description": "image #2", "id": "image_2", "path": "images/data-engineering-zoomcamp/image_8b6478c1.png"}, {"description": "image #3", "id": "image_3", "path": "images/data-engineering-zoomcamp/image_80992235.png"}, {"description": "image #4", "id": "image_4", "path": "images/data-engineering-zoomcamp/image_cd924928.png"}], "question": "DBT Deploy: This dbt Cloud run was cancelled because a valid dbt project was not found.", "sort_order": 67}, "content": "This issue occurs when the dbt project is moved to another directory in the repository or if you're on a different branch than expected.\n\n**Solution:**\n\n1. Navigate to the projects window on dbt Cloud.\n2. Go to Settings -> Edit.\n3. Add the directory path where the dbt project is located. Ensure that this path matches your file explorer path. For example:\n   \n   ```\n   /week5/taxi_rides_ny\n   ```\n\n4. Check that there are no files waiting to be committed to GitHub if you’re running the job to deploy to PROD.\n\n<{IMAGE:image_1}>\n\n<{IMAGE:image_2}>\n\n5. Ensure the PROD environment is set up to check the main branch, or the specified branch.\n\nIn the image below, the branch \"ella2024\" is set to be checked as \"production-ready\" by the \"freshness\" check mark in PROD environment settings. Each time a branch is merged into \"ella2024\" and a PR is triggered, the CI check job initiates. Note that merging and closing the PR must be done manually.\n\n<{IMAGE:image_3}>\n\n6. Set up the PROD custom branch (if not the default main) in the Environment setup screen.\n\n<{IMAGE:image_4}>"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-4/068_5dd0cdae58_dbt-deploy-ci-location-problems-on-bigquery.md", "metadata": {"id": "5dd0cdae58", "question": "DBT Deploy + CI: Location Problems on BigQuery", "sort_order": 68}, "content": "When creating a pull request and running the CI, dbt creates a new schema on BigQuery. By default, this new schema is created with the 'US' location. If your datasets, schemas, and tables are in the 'EU', this will cause an error and the pull request will not be accepted. \n\nTo change the location to 'EU' in the connection to BigQuery from dbt, follow these steps:\n\n- Navigate to **Dbt -> Project -> Settings -> Connection BigQuery**\n- Open **Optional Settings**\n- Set **Location** to `EU`"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-4/069_fee87c684a_dbt-deploy-error-when-trying-to-run-the-dbt-projec.md", "metadata": {"id": "fee87c684a", "question": "DBT Deploy: Error When trying to run the dbt project on Prod", "sort_order": 69}, "content": "When running the dbt project on production, ensure the following steps:\n\n1. **Pull Request and Merge**\n   - Make the pull request and merge the branch into the main.\n\n2. **Version Check**\n   - Ensure you have the latest version if changes were made to the repository elsewhere.\n\n3. **Project File Accessibility**\n   - Verify that the `dbt_project.yml` file is accessible to the project. If not, refer to the solution for the error: \"Dbt: This dbt Cloud run was cancelled because a valid dbt project was not found.\"\n\n4. **Dataset Consistency**\n   - Confirm that the name assigned to the dataset on BigQuery matches the name specified in the production environment configuration on dbt Cloud."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-4/070_b209edb213_dbt-error-404-not-found-dataset-dataset_namedbt_sc.md", "metadata": {"id": "b209edb213", "question": "DBT: Error: “404 Not found: Dataset <dataset_name>:<dbt_schema_name> was not found in location EU” after building from stg_green_tripdata.sql", "sort_order": 70}, "content": "In the step in [this video](https://www.youtube.com/watch?v=ueVy2N54lyc&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=44) (DE Zoomcamp 4.3.1 - Build the First dbt Models), after creating `stg_green_tripdata.sql` and clicking `build`, I encountered an error saying the dataset was not found in location EU. The default location for dbt Bigquery is the US, so when generating the new Bigquery schema for dbt, unless specified, the schema locates in the US.\n\nSolution:\n\n- Ensure the location is set to `EU` when adding connection details:\n  - Navigate to **Develop** -> **Configure Cloud CLI** -> **Projects** -> **taxi_rides_ny** -> (connection) **Bigquery** -> **Edit**\n  - Under **Location (Optional)**, type `EU`\n  - Save the changes."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-4/071_6783a9bddb_homework-ingesting-fhv_20-data.md", "metadata": {"id": "6783a9bddb", "question": "Homework: Ingesting FHV_20?? data", "sort_order": 71}, "content": "### Issue\n\nIf you’re having problems loading the FHV_20?? data from the GitHub repo into GCS and then into BigQuery (input file not of type parquet), follow these steps:\n\n1. **Append URL Template**\n   \n   Add `?raw=true` to the `URL_TEMPLATE` link:\n   \n   ```python\n   URL_TEMPLATE = URL_PREFIX + \"/fhv_tripdata_{{ execution_date.strftime('%Y-%m') }}.parquet?raw=true\"\n   ```\n   \n2. **Update URL Prefix**\n   \n   Ensure `URL_PREFIX` is set to the following value:\n   \n   ```\n   URL_PREFIX = \"https://github.com/alexeygrigorev/datasets/blob/master/nyc-tlc/fhv\"\n   ```\n   \n   It is critical to use this link with the keyword `blob`. If the link contains `tree`, replace it with `blob`. Everything else, including the `curl -sSLf` command, can remain unchanged."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-4/072_b459de9135_ingesting-fhv-alternative-with-kestra.md", "metadata": {"id": "b459de9135", "question": "Ingesting FHV: alternative with kestra", "sort_order": 72}, "content": "Add this task based on the previous ones:\n\n- id: if_fhv_taxi\n  \n  type: io.kestra.plugin.core.flow.If\n  \n  condition: \"{{inputs.taxi == 'fhv'}}\"\n  \n  then:\n  \n  - id: bq_fhv_tripdata\n  \n    type: io.kestra.plugin.gcp.bigquery.Query\n  \n    sql: |\n      \n      ```sql\n      CREATE TABLE IF NOT EXISTS `{{kv('GCP_PROJECT_ID')}}.{{kv('GCP_DATASET')}}.fhv_tripdata`\n      \n      (\n      \n      unique_row_id BYTES OPTIONS (description = 'A unique identifier for the trip, generated by hashing key trip attributes.'),\n      \n      filename STRING OPTIONS (description = 'The source filename from which the trip data was loaded.'),\n      \n      dispatching_base_num STRING,\n      \n      pickup_datetime TIMESTAMP,\n      \n      dropoff_datetime TIMESTAMP,\n      \n      PUlocationID NUMERIC,\n      \n      DOlocationID NUMERIC,\n      \n      SR_Flag STRING,\n      \n      Affiliated_base_number STRING\n      \n      )\n      \n      PARTITION BY DATE(pickup_datetime);\n      ```  \n\n- id: bq_fhv_table_ext\n  \n  type: io.kestra.plugin.gcp.bigquery.Query\n  \n  sql: |\n    \n    ```sql\n    CREATE OR REPLACE EXTERNAL TABLE `{{kv('GCP_PROJECT_ID')}}.{{render(vars.table)}}_ext`\n    \n    (\n    \n    dispatching_base_num STRING,\n    \n    pickup_datetime TIMESTAMP,\n    \n    dropoff_datetime TIMESTAMP,\n    \n    PUlocationID NUMERIC,\n    \n    DOlocationID NUMERIC,\n    \n    SR_Flag STRING,\n    \n    Affiliated_base_number STRING\n    \n    )\n    \n    OPTIONS (\n    \n    format = 'CSV',\n    \n    uris = ['{{render(vars.gcs_file)}}'],\n    \n    skip_leading_rows = 1,\n    \n    ignore_unknown_values = TRUE\n    \n    );\n    ```\n\n- id: bq_fhv_table_tmp\n  \n  type: io.kestra.plugin.gcp.bigquery.Query\n  \n  sql: |\n    \n    ```sql\n    CREATE OR REPLACE TABLE `{{kv('GCP_PROJECT_ID')}}.{{render(vars.table)}}`\n    \n    AS\n    \n    SELECT\n    \n    MD5(CONCAT(\n    \n    COALESCE(CAST(pickup_datetime AS STRING), \"\"),\n    \n    COALESCE(CAST(dropoff_datetime AS STRING), \"\"),\n    \n    COALESCE(CAST(PUlocationID AS STRING), \"\"),\n    \n    COALESCE(CAST(DOLocationID AS STRING), \"\")\n    \n    )) AS unique_row_id,\n    \n    \"{{render(vars.file)}}\" AS filename,\n    \n    *\n    \n    FROM `{{kv('GCP_PROJECT_ID')}}.{{render(vars.table)}}_ext`;\n    ```\n\n- id: bq_fhv_merge\n  \n  type: io.kestra.plugin.gcp.bigquery.Query\n  \n  sql: |\n    \n    ```sql\n    MERGE INTO `{{kv('GCP_PROJECT_ID')}}.{{kv('GCP_DATASET')}}.fhv_tripdata` T\n    \n    USING `{{kv('GCP_PROJECT_ID')}}.{{render(vars.table)}}` S\n    \n    ON T.unique_row_id = S.unique_row_id\n    \n    WHEN NOT MATCHED THEN\n    \n    INSERT (unique_row_id, filename, dispatching_base_num, pickup_datetime, dropoff_datetime, PUlocationID, DOlocationID, SR_Flag, Affiliated_base_number)\n    \n    VALUES (S.unique_row_id, S.filename, S.dispatching_base_num, S.pickup_datetime, S.dropoff_datetime, S.PUlocationID, S.DOlocationID, S.SR_Flag, S.Affiliated_base_number);\n    ```\n\nAdd a trigger too:\n\n- id: fhv_schedule\n\n  type: io.kestra.plugin.core.trigger.Schedule\n\n  cron: \"0 11 1 * *\"\n\n  inputs:\n\n  taxi: fhv\n\nAnd modify inputs:\n\n```yaml\ninputs:\n\n- id: taxi\n\n  type: SELECT\n\n  displayName: Select taxi type\n\n  values: [yellow, green, fhv]\n\n  defaults: green\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-4/073_c549d24645_homework-ingesting-nyc-tlc-data.md", "metadata": {"id": "c549d24645", "question": "Homework: Ingesting NYC TLC Data", "sort_order": 73}, "content": "The easiest way to upload datasets from GitHub for the homework is by utilizing this script: [git_csv_to_gcs.py](https://github.com/inner-outer-space/de-zoomcamp-2024/blob/main/4-analytics-engineering/git_csv_to_gcs.py). It is similar to a script provided in `03-data-warehouse/extras/web_to_gcs.py`. <{IMAGE:image_id}>"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-4/074_fb43c209d9_how-to-set-environment-variable-easily-for-any-cre.md", "metadata": {"id": "fb43c209d9", "question": "How to set environment variable easily for any credentials", "sort_order": 74}, "content": "If you need to securely set up credentials for a project and intend to push it to a git repository, using an environment variable is a recommended option.\n\nFor example, for scripts like `web_to_gcs.py` or `git_csv_to_gcs.py`, you may need to set these variables:\n\n- `GOOGLE_APPLICATION_CREDENTIALS`\n- `GCP_GCS_BUCKET`\n\nThe easiest option to manage this is to use a `.env` file with [dotenv](https://pypi.org/project/python-dotenv/).\n\nTo install and utilize this package, follow these steps:\n\n1. Install `python-dotenv`:\n\n   ```bash\n   pip install python-dotenv\n   ```\n\n2. Add the following code to inject these variables into your project:\n\n   ```python\n   from dotenv import load_dotenv\n   import os\n\n   # Load environment variables from .env file\n   load_dotenv()\n\n   # Now you can access environment variables like GCP_GCS_BUCKET and GOOGLE_APPLICATION_CREDENTIALS\n   credentials_path = os.getenv(\"GOOGLE_APPLICATION_CREDENTIALS\")\n   BUCKET = os.environ.get(\"GCP_GCS_BUCKET\")\n   ```"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-4/075_8c9b0fdaa5_invalid-date-types-after-ingesting-fhv-data-throug.md", "metadata": {"id": "8c9b0fdaa5", "question": "Invalid date types after Ingesting FHV data through CSV files: Could not parse 'pickup_datetime' as a timestamp", "sort_order": 75}, "content": "If you uploaded manually the FHV 2019 CSV files, you may face errors regarding date types. Try to create an external table in BigQuery but define the `pickup_datetime` and `dropoff_datetime` to be strings:\n\n```sql\nCREATE OR REPLACE EXTERNAL TABLE `gcp_project.trips_data_all.fhv_tripdata`  (\n\n    dispatching_base_num STRING,\n\n    pickup_datetime STRING,\n\n    dropoff_datetime STRING,\n\n    PUlocationID STRING,\n\n    DOlocationID STRING,\n\n    SR_Flag STRING,\n\n    Affiliated_base_number STRING\n\n)\n\nOPTIONS (\n\n    format = 'csv',\n\n    uris = ['gs://bucket/*.csv']\n\n);\n```\n\nThen, when creating the FHV core model in dbt, use `TIMESTAMP(CAST(())` to ensure it first parses as a string and then converts it to a timestamp:\n\n```sql\nWITH fhv_tripdata AS (\n\n    SELECT * FROM {{ ref('stg_fhv_tripdata') }}\n\n),\n\ndim_zones AS (\n\n    SELECT * FROM {{ ref('dim_zones') }}\n\n    WHERE borough != 'Unknown'\n\n)\n\nSELECT fhv_tripdata.dispatching_base_num,\n\n    TIMESTAMP(CAST(fhv_tripdata.pickup_datetime AS STRING)) AS pickup_datetime,\n\n    TIMESTAMP(CAST(fhv_tripdata.dropoff_datetime AS STRING)) AS dropoff_datetime,\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-4/076_05aad03ef3_invalid-data-types-after-ingesting-fhv-data-throug.md", "metadata": {"id": "05aad03ef3", "question": "Invalid data types after Ingesting FHV data through parquet files: Could not parse SR_Flag as Float64, Couldn’t parse datetime column as timestamp, couldn’t handle NULL values in PULocationID, DOLocationID", "sort_order": 76}, "content": "If you uploaded the FHV 2019 parquet files manually after downloading from [this source](https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2019-*.parquet), you may face errors regarding data types while loading the data into a landing table (e.g., `fhv_tripdata`). To avoid these errors, create an external table with the schema defined as follows and load each month in a loop:\n\n```sql\nCREATE OR REPLACE EXTERNAL TABLE `dw-bigquery-week-3.trips_data_all.external_tlc_fhv_trips_2019` (\n\n  dispatching_base_num STRING,\n\n  pickup_datetime TIMESTAMP,\n\n  dropoff_datetime TIMESTAMP,\n\n  PUlocationID FLOAT64,\n\n  DOlocationID FLOAT64,\n\n  SR_Flag FLOAT64,\n\n  Affiliated_base_number STRING\n\n)\n\nOPTIONS (\n\n  format = 'PARQUET',\n\n  uris = ['gs://project id/fhv_2019_8.parquet']\n\n);\n```\n\nYou can also use:\n\n```sql\nuris = ['gs://project id/fhv_2019_*.parquet']\n```\n\nThis approach removes the need for a loop and allows you to process all months in a single run."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-4/077_faddbcb675_join-error-on-locationid-unable-to-find-common-sup.md", "metadata": {"id": "faddbcb675", "question": "Join Error on LocationID: \"Unable to find common supertype for templated argument\"", "sort_order": 77}, "content": "No matching signature for operator `=` for argument types: `STRING`, `INT64`\n\n**Signature**: `T1 = T1`\n\n**Error:** Unable to find common supertype for templated argument.\n\n**Solution:**\n\nMake sure the `LocationID` field is of the same type in both tables. If it is in string format in one table, use the following dbt code to convert it to an integer:\n\n```sql\n{{ dbt.safe_cast(\"PULocationID\", api.Column.translate_type(\"integer\")) }} as pickup_locationid\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-4/078_17be084780_google-looker-studio-you-have-used-up-your-30-day.md", "metadata": {"id": "17be084780", "images": [{"description": "image #1", "id": "image_1", "path": "images/data-engineering-zoomcamp/image_2c1e7560.png"}], "question": "Google Looker Studio: you have used up your 30-day trial", "sort_order": 78}, "content": "When accessing Looker Studio through the Google Cloud Project console, you may be prompted to subscribe to the Pro version and receive the following errors:\n\n<{IMAGE:image_1}>\n\nInstead, navigate to [https://lookerstudio.google.com/navigation/reporting](https://lookerstudio.google.com/navigation/reporting) which will take you to the free version."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-4/079_78fda262c6_how-does-dbt-handle-dependencies-between-models.md", "metadata": {"id": "78fda262c6", "question": "How does dbt handle dependencies between models?", "sort_order": 79}, "content": "Dbt provides a mechanism called `ref` to manage dependencies between models. By referencing other models using the `ref` keyword in SQL, dbt automatically understands the dependencies and ensures the correct execution order."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-4/080_41463c387b_loading-fhv-data-goes-into-slumber-using-mage.md", "metadata": {"id": "41463c387b", "question": "Loading FHV Data goes into slumber using Mage?", "sort_order": 80}, "content": "Try loading the data using Jupyter notebooks in a local environment. There might be bandwidth issues with Mage.\n\nLoad the data into a pandas DataFrame using the URLs, make necessary transformations, upload to the GCP bucket, or alternatively download the Parquet/CSV files locally and then upload to GCP manually."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-4/081_30da3ec9c5_region-mismatch-in-dbt-and-bigquery.md", "metadata": {"id": "30da3ec9c5", "images": [{"description": "image #1", "id": "image_1", "path": "images/data-engineering-zoomcamp/image_6e795821.png"}, {"description": "image #2", "id": "image_2", "path": "images/data-engineering-zoomcamp/image_522c20d6.png"}], "question": "Region Mismatch in DBT and BigQuery", "sort_order": 81}, "content": "If you are using the datasets copied into BigQuery from BigQuery public datasets, the region will be set as `US` by default. It is much easier to set your dbt profile location as `US` while transforming the tables and views. You can change the location as follows:\n\n<{IMAGE:image_1}>\n\n<{IMAGE:image_2}>"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-4/082_a250f11737_what-is-the-fastest-way-to-upload-taxi-data-to-dbt.md", "metadata": {"id": "a250f11737", "question": "What is the fastest way to upload taxi data to dbt-postgres?", "sort_order": 82}, "content": "Use the PostgreSQL `COPY FROM` feature, which is compatible with CSV files.\n\n### Steps:\n\n1. **Create the Table**\n   \n   First, create your table (example):\n   \n   ```sql\n   CREATE TABLE taxis (\n   \n   …\n   \n   );\n   ```\n\n2. **Use COPY Functionality**\n\n   Use the `COPY` command (example):\n\n   ```sql\n   COPY taxis FROM PROGRAM\n   'url'\n   WITH (\n   FORMAT csv,\n   HEADER true,\n   ENCODING utf8\n   );\n   ```\n\n   - Syntax for `COPY`:\n\n   ```sql\n   COPY table_name [ ( column_name [, ...] ) ]\n   FROM { 'filename' | PROGRAM 'command' | STDIN }\n   [ [ WITH ] ( option [, ...] ) ]\n   [ WHERE condition ]\n   ```"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-4/083_49e1ec9d84_dbt-where-should-we-create-profilesyml.md", "metadata": {"id": "49e1ec9d84", "question": "dbt: Where should we create `profiles.yml`?", "sort_order": 83}, "content": "For local environments using dbt-core, the profile configuration is valid for all projects. Note: dbt Cloud doesn’t require it.\n\nThe `~/.dbt/profiles.yml` file should be located in your user's home directory. On Windows, this would typically be:\n\n```\nC:\\Users\\<YourUsername>\\.dbt\\profiles.yml\n```\n\nReplace `<YourUsername>` with your actual Windows username. This file is used by dbt to store connection profiles for different projects.\n\nHere's how you can create the `profiles.yml` file in the appropriate directory:\n\n1. Open File Explorer and navigate to `C:\\Users\\<YourUsername>\\`.\n2. Create a new folder named `.dbt` if it doesn't already exist.\n3. Inside the `.dbt` folder, create a new file named `profiles.yml`.\n\nUsage example can be found [here](https://gist.github.com/pizofreude/ff4d0601f1eb353683d8af8f4b5aac27?permalink_comment_id=5457712#gistcomment-5457712)."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-4/084_a8e992c143_dbt-are-there-ui-options-for-dbt-core-like-dbt-clo.md", "metadata": {"id": "a8e992c143", "question": "dbt: Are there UI options for dbt Core like dbt Cloud?", "sort_order": 84}, "content": "While dbt Core does not have an official UI like dbt Cloud, there are several tools available that provide UI functionality:\n\n- **Altimate's VS Code Extension**: \n  - Use the [VS Code dbt Power User extension](https://github.com/AltimateAI/vscode-dbt-power-user).\n  - Sign up for the community plan for free usage at [Altimate](https://app.myaltimate.com/register) and add the API into your VS Code extension.\n\n- **VSCode Snippets for dbt and Jinja**: \n  - Access the snippets package for SQL, YAML, and Markdown [here](https://github.com/bastienboutonnet/vscode-dbt).\n\n- **Monitoring with Elementary**:\n  - Monitor dbt projects using [Elementary](https://github.com/elementary-data/elementary).\n  - Learn more about its setup and features in this [Medium article](https://medium.com/@srinivas.dataengineer/supercharge-your-dbt-monitoring-with-elementary-data-0fac140a6f60)."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-4/085_5d92a0583a_when-configuring-the-profilesyml-file-for-dbt-post.md", "metadata": {"id": "5d92a0583a", "question": "When configuring the profiles.yml file for dbt-postgres with jinja templates with environment variables, I'm getting \"Credentials in profile \\\"PROFILE_NAME\\\", target: 'dev', invalid: '5432' is not of type 'integer'\"", "sort_order": 85}, "content": "Make sure that the port number is set as an integer in your `profiles.yml` file. Environment variables are usually strings, so you need to explicitly convert them to integers in Jinja. Update the line that sets the port with something like:\n\n```yaml\nport: {{ env_var('DB_PORT') | int }}\n```\n\nThis will ensure that the value is treated as an integer."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-4/086_23687d0f93_dbt-the-database-is-correct-but-i-get-error-with-i.md", "metadata": {"id": "23687d0f93", "images": [{"description": "image #1", "id": "image_1", "path": "images/data-engineering-zoomcamp/image_48537290.png"}], "question": "DBT: The database is correct but I get Error with Incorrect Schema in Models", "sort_order": 86}, "content": "What to do if your dbt model fails with an error similar to:\n\n```\nDBT-CORE\n```\n\nCheck **profiles.yml**:\n\n- Ensure your `profiles.yml` file is correctly configured with the correct schema and database under your target. This file is typically located in `~/.dbt/`.\n\nExample configuration:\n\n**DBT-CLOUD-IDE**\n\nCheck Credentials in dbt Cloud UI:\n\n- Navigate to the Credentials section in the dbt Cloud project settings.\n- Ensure the correct database and schema are set (e.g., ‘my_dataset’).\n\n<{IMAGE:image_1}>\n\nVerify Environment Settings:\n\n- Double-check that you are working in the correct environment (dev, prod, etc.), as dbt Cloud allows different settings for different environments.\n\n**No Need for profiles.yml**:\n\n- In dbt Cloud, you don’t need to configure `profiles.yml` manually. All connection settings are handled via the UI."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-4/087_7cec1857a9_dbt-dbt-allows-only-1-project-in-free-developer-ve.md", "metadata": {"id": "7cec1857a9", "images": [{"description": "image #1", "id": "image_1", "path": "images/data-engineering-zoomcamp/image_789bb06f.png"}], "question": "DBT: DBT allows only 1 project in free developer version.", "sort_order": 87}, "content": "Yes, DBT allows only 1 project under one account. But you can create multiple accounts as shown below:\n\n<{IMAGE:image_1}>"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-5/001_98b6a15ece_documentation-or-book-sign-not-shown-even-after-do.md", "metadata": {"id": "98b6a15ece", "question": "Documentation or book sign not shown even after doing `dbt docs generate`.", "sort_order": 1}, "content": "In the free version, it does not show the docs when models are run in the development environment. Create a production job and tick the 'generate docs' section. Execute it, and it will generate the documentation."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-5/002_d327cff06d_setting-up-java-and-spark-with-pyspark-on-linux-al.md", "metadata": {"id": "d327cff06d", "question": "Setting up Java and Spark (with PySpark) on Linux (Alternative option using SDKMAN):", "sort_order": 2}, "content": "1. **Install SDKMAN:**\n\n   ```bash\n   curl -s \"https://get.sdkman.io\" | bash\n   \n   source \"$HOME/.sdkman/bin/sdkman-init.sh\"\n   ```\n\n2. **Using SDKMAN, install Java 11 and Spark 3.3.2:**\n\n   ```bash\n   sdk install java 11.0.22-tem\n   \n   sdk install spark 3.3.2\n   ```\n\n3. **Open a new terminal or run the following in the same shell:**\n\n   ```bash\n   source \"$HOME/.sdkman/bin/sdkman-init.sh\"\n   ```\n\n4. **Verify the locations and versions of Java and Spark that were installed:**\n\n   ```bash\n   echo $JAVA_HOME\n   \n   java -version\n   \n   echo $SPARK_HOME\n   \n   spark-submit --version\n   ```"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-5/003_83b9009569_pyspark-setting-spark-up-in-google-colab.md", "metadata": {"id": "83b9009569", "question": "PySpark: Setting Spark up in Google Colab", "sort_order": 3}, "content": "If you’re struggling to set things up \"locally\" (meaning non-managed environments like your laptop, a VM, or Codespaces), you can follow this guide to use Spark in Google Colab:\n\n[Launch Spark on Google Colab and Connect to SparkUI](https://medium.com/gitconnected/launch-spark-on-google-colab-and-connect-to-sparkui-342cad19b304)\n\nStarter notebook:\n\n[GitHub Repository - Spark in Colab](https://github.com/aaalexlit/medium_articles/blob/main/Spark_in_Colab.ipynb)\n\nIt’s advisable to spend some time setting up locally rather than using this solution immediately."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-5/004_a3790900cc_spark-shell-unable-to-load-native-hadoop-library-f.md", "metadata": {"id": "a3790900cc", "question": "Spark-shell: unable to load native-hadoop library for platform - Windows", "sort_order": 4}, "content": "If after installing Java (either JDK or OpenJDK), Hadoop, and Spark, and setting the corresponding environment variables, you encounter the following error when running `spark-shell` in CMD:\n\n```java\njava.lang.IllegalAccessError: class org.apache.spark.storage.StorageUtils$ (in unnamed module @0x3c947bc5) cannot access class sun.nio.ch.DirectBuffer (in module java.base) because module java.base does not export sun.nio.ch to unnamed module @0x3c947bc5\n```\n\nSolution:\n- Java 17 or 19 is not supported by Spark. \n- Spark 3.x requires Java 8, 11, or 16.\n- Install Java 11 from the website provided in the windows.md setup file."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-5/005_59ad389756_pyspark-python-was-not-found-run-without-arguments.md", "metadata": {"id": "59ad389756", "question": "PySpark: Python was not found; run without arguments to install from the Microsoft Store, or disable this shortcut from Settings > Manage App Execution Aliases.", "sort_order": 5}, "content": "I encountered this error while executing a user-defined function in Spark (`crazy_stuff_udf`). I am working on Windows and using conda. After following the setup instructions, I discovered that the `PYSPARK_PYTHON` environment variable was not set correctly, as conda has different Python paths for each environment.\n\n**Solution:**\n\n1. Run the following command inside the appropriate environment:\n   \n   ```bash\n   pip install findspark\n   ```\n\n2. Add the following to the top of your script:\n   \n   ```python\n   import findspark\n   \n   findspark.init()\n   ```"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-5/006_d5d38534f6_pyspark-typeerror-code-argument-13-must-be-str-not.md", "metadata": {"id": "d5d38534f6", "question": "PySpark: TypeError: code() argument 13 must be str, not int, while executing `import pyspark`  (Windows/ Spark 3.0.3 - Python 3.11)", "sort_order": 6}, "content": "This error occurs because Python 3.11 has some inconsistencies with the older Spark 3.0.3 version.\n\n### Solutions:\n\n1. **Downgrade Python Version:**\n   - Use Python 3.9. A conda environment can help in managing different Python versions.\n\n2. **Upgrade PySpark Version:**\n   - Install a newer PySpark version, such as 3.5.1 or above, which is compatible with Python 3.11.\n\n```bash\n# Example commands\nconda create -n pyspark_env python=3.9\nconda activate pyspark_env\npip install pyspark==3.5.1\n```\n\nEnsure that your environment is set up correctly to avoid version mismatches."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-5/007_84f10086ab_import-pyspark-error-no-module-named-pyspark.md", "metadata": {"id": "84f10086ab", "question": "Import pyspark - Error: No Module named ‘pyspark’", "sort_order": 7}, "content": "Ensure that your `PYTHONPATH` is set correctly to include the PySpark library. You can check if PySpark is pointing to the correct location by running:\n\n```python\nimport pyspark\n\nprint(pyspark.__file__)\n```\n\nIt should point to the location where PySpark is installed (e.g., `/home/<your username>/spark/spark-3.x.x-bin-hadoop3.x/python/pyspark/__init__.py`)."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-5/008_14b039801e_cannot-find-spark-jobs-ui-at-localhost.md", "metadata": {"id": "14b039801e", "question": "Cannot find Spark jobs UI at localhost", "sort_order": 8}, "content": "This is because the current port is in use, so the Spark UI will run on a different port. You can check which port Spark is using by running this command:\n\n```\nspark.sparkContext.uiWebUrl\n```\n\nIf it indicates a different port, you should access that specific port instead. Additionally, ensure that there are no other notebooks or processes that might be using the same port. Clean up unused resources to avoid port conflicts."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-5/009_0de998a04e_javaspark-easy-setup-with-miniconda-env-worked-on.md", "metadata": {"id": "0de998a04e", "question": "Java+Spark - Easy setup with miniconda env (worked on MacOS)", "sort_order": 9}, "content": "If you want to manage all Python dependencies within the same virtual environment (e.g., conda), along with Java and Spark, follow these steps:\n\n1. **Install OpenJDK 11:**\n   \n   On MacOS, run:\n   \n   ```bash\n   brew install java11\n   ```\n\n   Add the following line to your `~/.bashrc` or `~/zshrc`:\n   \n   ```bash\n   export PATH=\"/opt/homebrew/opt/openjdk@11/bin:$PATH\"\n   ```\n\n2. **Activate Your Environment:**\n\n   Use your preferred tool (pipenv, poetry, or conda) to activate your working environment.\n\n3. **Install PySpark:**\n\n   Run:\n   \n   ```bash\n   pip install pyspark\n   ```\n\n4. **Proceed with Exercises:**\n\n   Work with your Spark exercises as usual. All default Spark commands will be available in the shell session under the activated environment.\n\n   Note: You don’t need `findspark` for initialization.\n\n5. **Resolving Py4J Errors:**\n\n   If you encounter an error like:\n   \n   ```plaintext\n   Py4JJavaError: An error occurred while calling (...)  java.net.ConnectException: Connection refused: no further information;\n   ```\n\n   Ensure you're using compatible versions of JDK or Python with Spark. Spark 3.5.0 supports JDK 8/11/17 and Python 3.8+.\n\n   Use SDKMan! to install:\n   \n   ```bash\n   sdk install java 17.0.10-librca\n   sdk install spark 3.5.0\n   sdk install hadoop 3.3.5\n   ```\n\n   Create and activate a conda environment with:\n\n   ```bash\n   conda create -n ENV_NAME python=3.11\n   conda activate ENV_NAME\n   pip install pyspark==3.5.0\n   ```\n\n6. **Windows Py4J Setup:**\n\n   Ensure correct PATH settings in your `~/.bashrc`:\n\n   ```bash\n   export JAVA_HOME=\"/c/tools/jdk-11.0.21\"\n   export PATH=\"${JAVA_HOME}/bin:${PATH}\"\n   export HADOOP_HOME=\"/c/tools/hadoop-3.2.0\"\n   export PATH=\"${HADOOP_HOME}/bin:${PATH}\"\n   export SPARK_HOME=\"/c/tools/spark-3.3.2-bin-hadoop3\"\n   export PATH=\"${SPARK_HOME}/bin:${PATH}\"\n   export PYTHONPATH=\"${SPARK_HOME}/python/:$PYTHONPATH\"\n   \n   export PYTHONPATH=\"${SPARK_HOME}spark-3.5.1-bin-hadoop3py4j-0.10.9.5-src.zip:$PYTHONPATH\"\n   ```\n\n   Download `winutils` from [Stephenlaye2/winutils3.3.0](https://github.com/Stephenlaye2/winutils3.3.0) and place them in `/c/tools/hadoop-3.2.0/bin`.\n\n   Check out this video for a solution: [How To Resolve Issue with Writing DataFrame to Local File](https://www.youtube.com/watch?v=yFem0Pu0gC8)\n\n   Restart your IDE and computer to apply the changes. Be aware that fixing one error might result in new ones like `o31.parquet`. Address these as needed."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-5/010_0f8512fc2a_spark-installation-error-code-1603.md", "metadata": {"id": "0f8512fc2a", "question": "Spark - Installation Error Code 1603", "sort_order": 10}, "content": "**Issue:** Spark installation on Windows completed but failed to run.\n\nThis is a common Windows Installer error code indicating that there was a fatal error during installation. It often occurs due to issues like insufficient permissions, conflicts with other software, or problems with the installer package.\n\n### Step to Solve the Issue:\n\n#### Installing Chocolatey\n\nChocolatey is a package manager for Windows, which makes it easy to install, update, and manage software.\n\n**Installation Steps**\n\n1. **Open PowerShell as an Administrator**\n   - Press `Win + X` and select **Windows PowerShell (Admin)** or search for PowerShell, right-click, and select **Run as administrator**.\n\n2. **Run the following command to install Chocolatey:**\n   \n   ```bash\n   Set-ExecutionPolicy Bypass -Scope Process -Force; \\\n   [System.Net.ServicePointManager]::SecurityProtocol = [System.Net.ServicePointManager]::SecurityProtocol -bor 3072; \\\n   iex ((New-Object System.Net.WebClient).DownloadString('https://community.chocolatey.org/install.ps1'))\n   ```\n\n3. **Verify the installation**\n   - Close and reopen PowerShell as an administrator and run:\n   \n   ```bash\n   choco -v\n   ```\n   - You should see the Chocolatey version number indicating that it has been installed successfully.\n\n4. **Command for Global Acceptance**\n   - To globally accept all licenses for all packages installed using Chocolatey, run the following command:\n   \n   ```bash\n   choco feature enable -n allowGlobalConfirmation\n   ```\n   - This command configures Chocolatey to automatically accept license agreements for all packages, streamlining the installation process and avoiding prompts for each package."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-5/011_ce188e5db2_runtimeerror-java-gateway-process-exited-before-se.md", "metadata": {"id": "ce188e5db2", "question": "RuntimeError: Java gateway process exited before sending its port number", "sort_order": 11}, "content": "After installing everything, including PySpark, when running this script in a Jupyter Notebook:\n\n```python\nimport pyspark\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder \\\n    .master(\"local[*]\") \\\n    .appName('test') \\\n    .getOrCreate()\n\n# Read the CSV file\ndf = spark.read \\\n    .option(\"header\", \"true\") \\\n    .csv('taxi+_zone_lookup.csv')\n\ndf.show()\n```\n\nYou might encounter the error:\n\n```\nRuntimeError: Java gateway process exited before sending its port number\n```\n\n### Solutions\n\n- **Solution 1:**\n  1. Install `findspark` by running:\n     \n     ```bash\n     pip install findspark\n     ```\n  2. Add the following lines to the top of your script:\n\n     ```python\n     import findspark\n     findspark.init()\n     ```\n\n- **Solution 2:**\n  1. Ensure that PySpark points to the correct location. Run:\n     \n     ```python\n     pyspark.__file__\n     ```\n     \n     It should list a path like `/home/<your user name>/spark/spark-3.0.3-bin-hadoop3.2/python/pyspark/__init__.py` if you followed the setup correctly.\n  2. If it points to your Python site-packages, remove the PySpark directory there.\n  3. Verify `.bashrc` for correct exports, ensuring no conflicting variables are present.\n\n- **Alternative Solution:**\n  - Set environment variables permanently at the system and user levels by following a tutorial.\n\n     Once installed, skip to 7:14 in the tutorial to help set up environment variables."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-5/012_675d60dadd_module-not-found-error-in-jupyter-notebook.md", "metadata": {"id": "675d60dadd", "question": "Module Not Found Error in Jupyter Notebook.", "sort_order": 12}, "content": "Even after installing `pyspark` correctly on a Linux machine (VM) as instructed in the course, a module not found error was encountered in the Jupyter Notebook.\n\nThe solution that worked:\n\n1. Run the following in the Jupyter Notebook:\n\n   ```bash\n   !pip install findspark\n   ```\n\n2. Import and initialize `findspark`:\n\n   ```python\n   import findspark\n   findspark.init()\n   ```\n\n3. Thereafter, import `pyspark` and create the Spark context as usual.\n\nIf the above solution does not work, try:\n\n- Using `!pip3 install pyspark` instead of `!pip install pyspark`.\n\nTo filter based on conditions across multiple columns:\n\n```python\nfrom pyspark.sql.functions import col\n\nnew_final.filter((new_final.a_zone==\"Murray Hill\") & (new_final.b_zone==\"Midwood\")).show()\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-5/013_d87bb0e38f_py4jjavaerror-modulenotfounderror-no-module-named.md", "metadata": {"id": "d87bb0e38f", "question": "Py4JJavaError - ModuleNotFoundError: No module named 'py4j' while executing `import pyspark`", "sort_order": 13}, "content": "To resolve the `ModuleNotFoundError: No module named 'py4j'` when executing `import pyspark`, follow these steps:\n\n1. **Check for Py4J File Version:**\n   - Run the command:\n     ```bash\n     ls ${SPARK_HOME}/python/lib/\n     ```\n   - Note the version of the `py4j` file.\n\n2. **Update the `PYTHONPATH`:**\n   - Use the version identified above to update the `PYTHONPATH` accordingly. For example:\n     ```bash\n     export PYTHONPATH=\"${SPARK_HOME}/python/lib/py4j-0.10.9.3-src.zip:$PYTHONPATH\"\n     ```\n   - Ensure that the version matches the filename under `${SPARK_HOME}/python/lib/`.\n\n3. **Verify Spark's Py4J Version:**\n   - Check the Py4J version of the Spark you're using from the [Apache Spark documentation](https://spark.apache.org/docs/latest/api/python/getting_started/install.html).\n\n4. **Install Py4J (if needed):**\n   - If the above steps do not resolve the issue, run:\n     ```bash\n     pip install py4j\n     ```\n   - This can resolve version mismatches or missing installations.\n\nThis should address the `ModuleNotFoundError` for Py4J when using PySpark. Ensure all versions are consistent and correct."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-5/014_590e5d8c20_py4j-error-modulenotfounderror-no-module-named-py4.md", "metadata": {"id": "590e5d8c20", "images": [{"description": "image #1", "id": "image_1", "path": "images/data-engineering-zoomcamp/image_be1c5cff.png"}], "question": "Py4J Error - ModuleNotFoundError: No module named 'py4j' (Solve with latest version)", "sort_order": 14}, "content": "To resolve the Py4J error, follow these steps:\n\n1. Install the latest available Py4J version using conda:\n   \n   ```bash\n   conda install -c conda-forge py4j\n   ```\n   \n   Make sure to replace with the latest version number as found on the website.\n\n   <{IMAGE:image_1}>\n\n2. Add the following lines to your `.bashrc` file:\n   \n   ```bash\n   export PYTHONPATH=\"${SPARK_HOME}/python/:$PYTHONPATH\"\n   export PYTHONPATH=\"${SPARK_HOME}/python/lib/py4j-0.10.9.7-src.zip:$PYTHONPATH\"\n   ```"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-5/015_e269cf9e38_exception-jupyter-command-jupyter-notebook-not-fou.md", "metadata": {"id": "e269cf9e38", "question": "Exception: Jupyter command `jupyter-notebook` not found.", "sort_order": 15}, "content": "Even after exporting paths correctly, you may find that Jupyter is installed but `jupyter-notebook` is not found. Follow these steps to resolve the issue:\n\nFull steps:\n\n1. **Update and Upgrade Packages:**\n   ```bash\n   sudo apt update && sudo apt -y upgrade\n   ```\n\n2. **Install Python:**\n   ```bash\n   sudo apt install python3-pip python3-dev\n   ```\n\n3. **Install Python Virtualenv:**\n   ```bash\n   sudo -H pip3 install --upgrade pip\n   sudo -H pip3 install virtualenv\n   ```\n\n4. **Create a Python Virtual Environment:**\n   ```bash\n   mkdir notebook\n   cd notebook\n   virtualenv jupyterenv\n   source jupyterenv/bin/activate\n   ```\n\n5. **Install Jupyter Notebook:**\n   ```bash\n   pip install jupyter\n   ```\n\n6. **Run Jupyter Notebook:**\n   ```bash\n   jupyter notebook\n   ```\n   \nFor full instructions, refer to [this guide](https://learningdataengineering540969211.wordpress.com/2022/02/24/week-5-de-zoomcamp-5-2-1-installing-spark-on-linux/) or the original instructions [here](https://speedysense.com/install-jupyter-notebook-on-ubuntu-20-04/)."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-5/016_6cab9c338e_following-521-i-am-getting-an-error-headcannot-ope.md", "metadata": {"id": "6cab9c338e", "question": "Following 5.2.1, I am getting an error - Head:cannot open ‘taxi+_zone_lookup.csv’ for reading: No such file or directory", "sort_order": 16}, "content": "The latest filename is just `taxi_zone_lookup.csv`, so it should work after removing the `+` now."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-5/017_ef01c39d63_error-javaiofilenotfoundexception.md", "metadata": {"id": "ef01c39d63", "question": "Error: java.io.FileNotFoundException", "sort_order": 17}, "content": "```python\n# Code executed:\ndf = spark.read.parquet(pq_path)\n\n# … some operations on df …\n\ndf.write.parquet(pq_path, mode=\"overwrite\")\n\n# Error:\n# java.io.FileNotFoundException: File file:/home/xxx/code/data/pq/fhvhv/2021/02/part-00021-523f9ad5-14af-4332-9434-bdcb0831f2b7-c000.snappy.parquet does not exist\n```\n\nThe problem is that Spark performs lazy transformations, so the actual action that triggers the job is `df.write`, which deletes the parquet files it tries to read (mode=\"overwrite\").\n\n**Solution:**\n\n- Write to a different directory:\n\n  ```python\n  df.write.parquet(pq_path_temp, mode=\"overwrite\")\n  ```"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-5/018_cbf0755be8_hadoop-filenotfoundexception-hadoop-bin-directory.md", "metadata": {"id": "cbf0755be8", "question": "Hadoop: FileNotFoundException: Hadoop bin directory does not exist, when trying to write (Windows)", "sort_order": 18}, "content": "Create the Hadoop `/bin` directory manually and add the downloaded files there. The shell script for Windows installation typically places them in `/c/tools/hadoop-3.2.0/`."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-5/019_b024daf322_which-type-of-sql-is-used-in-spark-postgres-mysql.md", "metadata": {"id": "b024daf322", "question": "Which type of SQL is used in Spark? Postgres? MySQL? SQL Server?", "sort_order": 19}, "content": "Spark uses its own type of SQL, known as Spark SQL.\n\nThe SQL syntax across various providers is generally similar, as shown below:\n\n```sql\nSELECT [attributes]\nFROM [table]\nWHERE [filter]\nGROUP BY [grouping attributes]\nHAVING [filtering the groups]\nORDER BY [attribute to order]\n(INNER/FULL/LEFT/RIGHT) JOIN [table2]\nON [attributes table joining table2] (...)\n```\n\nWhat differs most between SQL providers are the built-in functions.\n\nFor built-in Spark SQL functions, check this link: [Spark SQL Functions](https://spark.apache.org/docs/latest/api/sql/index.html)\n\nExtra information on Spark SQL:\n\n[What is Spark SQL?](https://databricks.com/glossary/what-is-spark-sql#:~:text=Spark%20SQL%20is%20a%20Spark,on%20existing%20deployments%20and%20data)"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-5/020_772aea0210_the-spark-viewer-on-localhost4040-was-not-showing.md", "metadata": {"id": "772aea0210", "question": "The spark viewer on localhost:4040 was not showing the current run", "sort_order": 20}, "content": "**Solution:**\n\n- Ensure you have identified all running Spark notebooks. If multiple notebooks are running, each will attempt to use available ports starting from 4040.\n- If a port is in use, Spark automatically assigns the next available port (e.g., 4041, 4042, etc.).\n- To find the exact port being used by your current Spark application, run the following command:\n\n  ```python\n  spark.sparkContext.uiWebUrl\n  ```\n\n- The result will provide the URL, for example: `[172.19.10.61:4041](http://172.19.10.61:4041)`.\n- If the expected port does not show your current run, verify that cleanup has been performed on closed or non-running containers."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-5/021_11fc890b73_java-javalangnosuchmethoderror-sunniochdirectbuffe.md", "metadata": {"id": "11fc890b73", "question": "Java: java.lang.NoSuchMethodError: sun.nio.ch.DirectBuffer.cleaner()Lsun/misc/Cleaner Error during repartition call (conda pyspark installation)", "sort_order": 21}, "content": "```java\njava.lang.NoSuchMethodError: sun.nio.ch.DirectBuffer.cleaner()Lsun/misc/Cleaner;\n```\n\n**Solution:** Replace Java Developer Kit 11 with Java Developer Kit 8."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-5/022_78c8446eae_java-runtimeerror-java-gateway-process-exited-befo.md", "metadata": {"id": "78c8446eae", "question": "Java - RuntimeError: Java gateway process exited before sending its port number", "sort_order": 22}, "content": "This error indicates that the `JAVA_HOME` environment variable is not set correctly in the notebook log.\n\nFor more information, you can refer to the following resource:\n\n[PySpark Exception - Java gateway process exited before sending the driver its port number](https://sparkbyexamples.com/pyspark/pyspark-exception-java-gateway-process-exited-before-sending-the-driver-its-port-number/)"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-5/023_a340ad48c5_spark-fails-when-reading-from-bigquery-and-using-s.md", "metadata": {"id": "a340ad48c5", "question": "Spark fails when reading from BigQuery and using `.show()` on `SELECT` queries", "sort_order": 23}, "content": "I got it working using `gcs-connector-hadoop-2.2.5-shaded.jar` and Spark 3.1.\n\nI also added the `google_credentials.json` and `.p12` to authenticate with GCS. These files are downloadable from the GCP Service account.\n\nTo create the SparkSession:\n\n```python\nspark = SparkSession.builder.master('local[*]') \\\n    .appName('spark-read-from-bigquery') \\\n    .config('BigQueryProjectId', 'razor-project-xxxxxxx') \\\n    .config('BigQueryDatasetLocation', 'de_final_data') \\\n    .config('parentProject', 'razor-project-xxxxxxx') \\\n    .config(\"google.cloud.auth.service.account.enable\", \"true\") \\\n    .config(\"credentialsFile\", \"google_credentials.json\") \\\n    .config(\"GcpJsonKeyFile\", \"google_credentials.json\") \\\n    .config(\"spark.driver.memory\", \"4g\") \\\n    .config(\"spark.executor.memory\", \"2g\") \\\n    .config(\"spark.memory.offHeap.enabled\", True) \\\n    .config(\"spark.memory.offHeap.size\", \"5g\") \\\n    .config('google.cloud.auth.service.account.json.keyfile', \"google_credentials.json\") \\\n    .config(\"fs.gs.project.id\", \"razor-project-xxxxxxx\") \\\n    .config(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\") \\\n    .config(\"fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\") \\\n    .getOrCreate()\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-5/024_0a431f8863_spark-bigquery-connector-automatic-configuration.md", "metadata": {"id": "0a431f8863", "question": "Spark: BigQuery connector Automatic configuration", "sort_order": 24}, "content": "To automatically configure the Spark BigQuery connector, you can create a `SparkSession` by specifying the `spark.jars.packages` configuration.\n\n```python\nspark = SparkSession.builder \n    .master('local') \n    .appName('bq') \n    .config(\"spark.jars.packages\", \"com.google.cloud.spark:spark-bigquery-with-dependencies_2.12:0.23.2\") \n    .getOrCreate()\n```\n\nThis approach automatically downloads the required dependency jars and configures the connector, eliminating the need to manually manage this dependency. More details are available [here](https://github.com/GoogleCloudDataproc/spark-bigquery-connector)."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-5/025_49ee668514_spark-how-to-read-from-gcp-data-lake-using-pyspark.md", "metadata": {"id": "49ee668514", "question": "Spark: How to read from GCP data lake using PySpark?", "sort_order": 25}, "content": "There are a few steps to read from Google Cloud Storage (GCS) using PySpark:\n\n1. **Download the Cloud Storage connector for Hadoop**\n   - You can download it [here](https://cloud.google.com/dataproc/docs/concepts/connectors/cloud-storage#clusters).\n   - This `.jar` file connects PySpark with GCS.\n\n2. **Move the .jar file to your Spark file directory**\n   - If you installed Spark using Homebrew on MacOS, create a `/jars` directory under your Spark directory, e.g., `/opt/homebrew/Cellar/apache-spark/3.2.1/jars`.\n\n3. **Import necessary classes in your Python script**\n   ```python\n   import pyspark\n   from pyspark.sql import SparkSession\n   from pyspark.conf import SparkConf\n   from pyspark.context import SparkContext\n   ```\n\n4. **Set up your configurations before building the SparkSession**\n   ```python\n   conf = SparkConf() \\\n       .setMaster('local[*]') \\\n       .setAppName('test') \\\n       .set(\"spark.jars\", \"/opt/homebrew/Cellar/apache-spark/3.2.1/jars/gcs-connector-hadoop3-latest.jar\") \\\n       .set(\"spark.hadoop.google.cloud.auth.service.account.enable\", \"true\") \\\n       .set(\"spark.hadoop.google.cloud.auth.service.account.json.keyfile\", \"path/to/google_credentials.json\")\n   \n   sc = SparkContext(conf=conf)\n\n   sc._jsc.hadoopConfiguration().set(\"fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\n   sc._jsc.hadoopConfiguration().set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n   sc._jsc.hadoopConfiguration().set(\"fs.gs.auth.service.account.json.keyfile\", \"path/to/google_credentials.json\")\n   sc._jsc.hadoopConfiguration().set(\"fs.gs.auth.service.account.enable\", \"true\")\n   ```\n\n5. **Build your SparkSession with the new configurations**\n   ```python\n   spark = SparkSession.builder \\\n       .config(conf=sc.getConf()) \\\n       .getOrCreate()\n   ```\n\n6. **You can now read files directly from GCS!**\n\nNote: If you encounter `start-slave.sh: command not found`, ensure Spark is correctly installed and paths are set."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-5/026_6e251d34b6_how-can-i-read-a-small-number-of-rows-from-the-par.md", "metadata": {"id": "6e251d34b6", "question": "How can I read a small number of rows from the parquet file directly?", "sort_order": 26}, "content": "To read a small number of rows from a parquet file, you can use PyArrow or Apache Spark:\n\n### Using PyArrow\n\n```python\nfrom pyarrow.parquet import ParquetFile\n\npf = ParquetFile('fhvhv_tripdata_2021-01.parquet')\n\n# PyArrow builds tables, not dataframes\n\ntbl_small = next(pf.iter_batches(batch_size=1000))\n\n# Convert the table to a DataFrame\n\ndf = tbl_small.to_pandas()\n```\n\n### Alternatively, without PyArrow\n\n```python\ndf = spark.read.parquet('fhvhv_tripdata_2021-01.parquet')\n\ndf1 = df.sort('DOLocationID').limit(1000)\n\npdf = df1.select(\"*\").toPandas()\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-5/027_3e355d74fd_datatype-error-when-creating-spark-dataframe-with.md", "metadata": {"id": "3e355d74fd", "question": "DataType error when creating Spark DataFrame with a specified schema?", "sort_order": 27}, "content": "When defining the schema for a Spark DataFrame, you might encounter a data type error if you're using a Parquet file with the schema definition from the TLC example. The error occurs because the `PULocationID` and `DOLocationID` columns are defined as `IntegerType`, but the Parquet file uses `INT64`.\n\nYou'll get an error like:\n\n```plaintext\nParquet column cannot be converted in file [...] Column [...] Expected: int, Found: INT64\n```\n\nTo resolve this issue:\n\n- Change the schema definition from `IntegerType` to `LongType`. This adjustment should align the expected and actual data types, allowing the DataFrame to be created successfully."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-5/028_7fa4e026d6_remove-white-spaces-from-column-names-in-pyspark.md", "metadata": {"id": "7fa4e026d6", "question": "Remove white spaces from column names in Pyspark", "sort_order": 28}, "content": "```python\ndf_finalx = df_finalw.select([col(x).alias(x.replace(\" \", \"\")) for x in df_finalw.columns])\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-5/029_9b3ee420d4_attributeerror-dataframe-object-has-no-attribute-i.md", "metadata": {"id": "9b3ee420d4", "question": "AttributeError: 'DataFrame' object has no attribute 'iteritems'", "sort_order": 29}, "content": "This error occurs in the Spark video 5.3.1 - First Look at Spark/PySpark because the example utilizes CSV files from 2021, but the current data is in parquet format.\n\nWhen running the command:\n\n```python\nspark.createDataFrame(df1_pandas).show()\n```\n\nYou may encounter the Attribute error due to incompatibility between pandas version 2.0.0 and Spark 3.3.2. To resolve this, you can:\n\n- Downgrade pandas to version 1.5.3 using the following command:\n  \n  ```bash\n  pip install -U pandas==1.5.3\n  ```\n  \n- Alternatively, add the following line after importing pandas if you prefer not to downgrade:\n  \n  ```python\n  pd.DataFrame.iteritems = pd.DataFrame.items\n  ```\n\nThis issue is resolved in Spark versions from 3.4.1 onwards."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-5/030_639151c1d1_attributeerror-dataframe-object-has-no-attribute-i.md", "metadata": {"id": "639151c1d1", "question": "AttributeError: 'DataFrame' object has no attribute 'iteritems'", "sort_order": 30}, "content": "Another alternative is to install pandas 2.0.1 (it worked well at the time of writing this), and it is compatible with Pyspark 3.5.1. Make sure to add or edit your environment variable like this:\n\n```bash\nexport SPARK_HOME=\"${HOME}/spark/spark-3.5.1-bin-hadoop3\"\n\nexport PATH=\"${SPARK_HOME}/bin:${PATH}\"\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-5/031_01092d660a_spark-standalone-mode-on-windows.md", "metadata": {"id": "01092d660a", "question": "Spark Standalone Mode on Windows", "sort_order": 31}, "content": "To set up Spark in standalone mode on Windows, follow these steps:\n\n1. Open a CMD terminal in administrator mode.\n\n2. Navigate to your Spark home directory:\n   \n   ```bash\n   cd %SPARK_HOME%\n   ```\n\n3. Start a master node:\n   \n   ```bash\n   bin\\spark-class org.apache.spark.deploy.master.Master\n   ```\n\n4. Start a worker node:\n   \n   ```bash\n   bin\\spark-class org.apache.spark.deploy.worker.Worker spark://<master_ip>:<port> --host <IP_ADDR>\n   ```\n   \n   Example:\n   \n   ```bash\n   bin\\spark-class org.apache.spark.deploy.worker.Worker spark://localhost:7077 --host <IP_ADDR>\n   ```\n\n   - `spark://<master_ip>:<port>`: Copy the address from the previous command. For example, `spark://localhost:7077`\n   - Use `--host <IP_ADDR>` if you want to run the worker on a different machine. You can leave it empty for local setup.\n\n5. Access the Spark UI through `localhost:8080`."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-5/032_e4654d1d0b_export-pythonpath-command-in-linux-is-temporary.md", "metadata": {"id": "e4654d1d0b", "question": "Export PYTHONPATH command in Linux is temporary", "sort_order": 32}, "content": "To make the `export PYTHONPATH` command permanent, consider the following options:\n\n1. **Add to `.bashrc`:** \n   - Open the `.bashrc` file located in your home directory using a text editor, such as `nano` or `vim`.\n   \n   ```bash\n   nano ~/.bashrc\n   ```\n   \n   - Add the `export PYTHONPATH` line, such as:\n   \n   ```bash\n   export PYTHONPATH=\"/your/custom/path\"\n   ```\n   \n   - Save the changes and source the file to apply the changes:\n   \n   ```bash\n   source ~/.bashrc\n   ```\n\n2. **Run specific commands in Python scripts:**\n   - You can initialize the environment directly in a Python script using:\n   \n   ```python\n   import findspark\n   findspark.init()\n   ```\n\nBy using these methods, you ensure that the `PYTHONPATH` is set up for every session automatically."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-5/033_7bff88dbd7_compression-error-zcat-output-is-gibberish-seems-l.md", "metadata": {"id": "7bff88dbd7", "question": "Compression Error: zcat output is gibberish, seems like still compressed", "sort_order": 33}, "content": "In the code along from Video 5.3.3, Alexey downloads the CSV files from the NYT website and gzips them in their bash script. Currently (2023), if we download the data from the GH course repo, it is already zipped as `csv.gz` files. Following the video exactly would zip them again, leading to gibberish output when using `zcat`, as it only unzips the file once.\n\n**Solution:** Do not gzip the files downloaded from the course repo. Simply use `wget` to download and save them as `csv.gz` files. Then the `zcat` command and the `showSchema` command will work correctly.\n\n```bash\nURL=\"${URL_PREFIX}/${TAXI_TYPE}/${TAXI_TYPE}_tripdata_${YEAR}-${FMONTH}.csv.gz\"\n\nLOCAL_PREFIX=\"data/raw/${TAXI_TYPE}/${YEAR}/${FMONTH}\"\n\nLOCAL_FILE=\"${TAXI_TYPE}_tripdata_${YEAR}_${FMONTH}.csv.gz\"\n\nLOCAL_PATH=\"${LOCAL_PREFIX}/${LOCAL_FILE}\"\n\necho \"downloading ${URL} to ${LOCAL_PATH}\"\n\nmkdir -p ${LOCAL_PREFIX}\n\nwget ${URL} -O ${LOCAL_PATH}\n\necho \"compressing ${LOCAL_PATH}\"\n\n# gzip ${LOCAL_PATH} <- uncomment this line\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-5/034_91df9f5c6f_picklingerror-could-not-serialise-object-indexerro.md", "metadata": {"id": "91df9f5c6f", "question": "PicklingError: Could not serialise object: IndexError: tuple index out of range.", "sort_order": 34}, "content": "This error occurs while running:\n\n```python\nspark.createDataFrame(df_pandas).show()\n```\n\n### Cause\nThis issue is usually related to the Python version. As of March 2, 2023, Spark does not support Python 3.11.\n\n### Solution\n\n1. **Create a New Environment with a Supported Python Version:**\n\n   Using Conda, you can create a virtual environment with Python 3.10:\n   \n   ```bash\n   conda create -n myenv python=3.10 anaconda\n   ```\n\n2. **Activate the Environment:**\n\n   To use Python 3.10, activate the environment:\n   \n   ```bash\n   conda activate myenv\n   ```\n\n3. **Deactivate the Environment:**\n\n   If you need to return to the initial setup, you can deactivate the environment:\n   \n   ```bash\n   conda deactivate\n   ```"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-5/035_497269a77f_connecting-from-local-spark-to-gcs-spark-does-not.md", "metadata": {"id": "497269a77f", "question": "Connecting from local Spark to GCS: Spark does not find my Google credentials as shown in the video?", "sort_order": 35}, "content": "Make sure you have your credentials for your GCP in your VM under the location defined in the script."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-5/036_6d0a7d749a_spark-docker-compose-setup.md", "metadata": {"id": "6d0a7d749a", "question": "Spark docker-compose setup", "sort_order": 36}, "content": "To run Spark in a Docker setup:\n\n1. **Build Bitnami Spark Docker**\n   \n   a. Clone the Bitnami repository using the command:\n   \n   ```bash\n   git clone https://github.com/bitnami/containers.git\n   ```\n   \n   *(Tested on commit 9cef8b892d29c04f8a271a644341c8222790c992)*\n   \n   b. Edit the file `bitnami/spark/3.3/debian-11/Dockerfile` and update the Java and Spark version as follows:\n   \n   ```\n   \"python-3.10.10-2-linux-${OS_ARCH}-debian-11\" \n   \"java-17.0.5-8-3-linux-${OS_ARCH}-debian-11\" \n   ```\n   \n   Reference: [GitHub](https://github.com/bitnami/containers/issues/13409)\n\n   c. Build the Docker image by navigating to the above directory and running the Docker build command:\n   \n   ```bash\n   cd bitnami/spark/3.3/debian-11/\n   docker build -t spark:3.3-java-17 .\n   ```\n\n2. **Run Docker Compose**\n\n   Use the following `docker-compose.yml` file:\n\n   ```yaml\n   version: '2'\n\n   services:\n\n     spark:\n       image: spark:3.3-java-17\n       environment:\n         - SPARK_MODE=master\n         - SPARK_RPC_AUTHENTICATION_ENABLED=no\n         - SPARK_RPC_ENCRYPTION_ENABLED=no\n         - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no\n         - SPARK_SSL_ENABLED=no\n       volumes:\n         - \"./:/home/jovyan/work:rw\"\n       ports:\n         - '8080:8080'\n         - '7077:7077'\n\n     spark-worker:\n       image: spark:3.3-java-17\n       environment:\n         - SPARK_MODE=worker\n         - SPARK_MASTER_URL=spark://spark:7077\n         - SPARK_WORKER_MEMORY=1G\n         - SPARK_WORKER_CORES=1\n         - SPARK_RPC_AUTHENTICATION_ENABLED=no\n         - SPARK_RPC_ENCRYPTION_ENABLED=no\n         - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no\n         - SPARK_SSL_ENABLED=no\n       volumes:\n         - \"./:/home/jovyan/work:rw\"\n       ports:\n         - '8081:8081'\n\n     spark-nb:\n       image: jupyter/pyspark-notebook:java-17.0.5\n       environment:\n         - SPARK_MASTER_URL=spark://spark:7077\n       volumes:\n         - \"./:/home/jovyan/work:rw\"\n       ports:\n         - '8888:8888'\n         - '4040:4040'\n   ```\n\n   Run the command to deploy Docker Compose:\n\n   ```bash\n   docker-compose up\n   ```\n\n   Access the Jupyter notebook using the link logged in Docker Compose logs.\n\n   The Spark master URL is `spark://spark:7077`."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-5/037_e3ce42af1a_how-do-you-read-data-stored-in-gcs-on-pandas-with.md", "metadata": {"id": "e3ce42af1a", "question": "How do you read data stored in GCS on pandas with your local computer?", "sort_order": 37}, "content": "To do this:\n\n1. Install `gcsfs`:\n   \n   ```bash\n   pip install gcsfs\n   ```\n\n2. Copy the URI path to the file and use the following command to read it:\n   \n   ```python\n   df = pandas.read_csv('gs://path/to/your/file.csv')\n   ```"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-5/038_34ebc2c6de_typeerror-when-using-sparkcreatedataframe-function.md", "metadata": {"id": "34ebc2c6de", "question": "TypeError when using spark.createDataFrame function on a pandas df", "sort_order": 38}, "content": "**Error:**\n\n```python\nspark.createDataFrame(df_pandas).schema\n\nTypeError: field Affiliated_base_number: Can not merge type <class 'pyspark.sql.types.StringType'> and <class 'pyspark.sql.types.DoubleType'>\n```\n\n**Solution:**\n\n- **Reason:**\n  - The `Affiliated_base_number` field is a mix of letters and numbers, so it cannot be set to `DoubleType`. The suitable type would be `StringType`. Spark's `inferSchema` is more accurate than Pandas' infer type method in this case. Set `inferSchema` to `true` when reading the CSV to prevent data removal.\n\n- **Implementation:**\n  \n  ```python\n  df = spark.read \\\n    .options(\n      header = \"true\", \\\n      inferSchema = \"true\"\n    ) \\\n    .csv('path/to/your/csv/file/')\n  ````\n\n- **Alternative Solution:**\n  \n  - **Problem:** Some rows in `affiliated_base_number` are null, and therefore are assigned the datatype `String`, which cannot be converted to `Double`.\n  - **Solution:** Only take rows from the pandas df that are not null in the 'Affiliated_base_number' column before converting it to a PySpark DataFrame.\n\n  ```python\n  # Only take rows that have no null values\n  pandas_df = pandas_df[pandas_df.notnull().all(1)]\n  ```"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-5/039_14cc6e2060_memorymanager-total-allocation-exceeds-9500-102005.md", "metadata": {"id": "14cc6e2060", "question": "MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory", "sort_order": 39}, "content": "Default executor memory is 1GB. This error appeared when working with the homework dataset.\n\nError:\n\n```plaintext\nMemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memoryScaling row group sizes to 95.00% for 8 writers\n```\n\nSolution:\n\nIncrease the memory of the executor when creating the Spark session like this:\n\n```python\nspark = SparkSession.builder \\\n    .master(\"local[*]\") \\\n    .appName('test') \\\n    .config(\"spark.executor.memory\", \"4g\") \\\n    .config(\"spark.driver.memory\", \"4g\") \\\n    .getOrCreate()\n```\n\nRemember to restart the Jupyter session (i.e., close the Spark session) or the config won’t take effect."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-5/040_46b39cdb17_how-to-spark-standalone-cluster-is-run-on-windows.md", "metadata": {"id": "46b39cdb17", "question": "How to spark standalone cluster is run on windows OS", "sort_order": 40}, "content": "- Change the working directory to the Spark directory:\n\n  - If you have set up your `SPARK_HOME` variable, use the following:\n    \n    ```bash\n    cd %SPARK_HOME%\n    ```\n\n  - If not, use the following:\n\n    ```bash\n    cd <path to spark installation>\n    ```\n\n- Creating a Local Spark Cluster:\n\n  1. To start Spark Master:\n  \n     ```bash\n     bin\\spark-class org.apache.spark.deploy.master.Master --host localhost\n     ```\n  \n  2. Starting up a cluster:\n     \n     ```bash\n     bin\\spark-class org.apache.spark.deploy.worker.Worker spark://localhost:7077 --host localhost\n     ```"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-5/041_efa85f71b6_env-variables-set-in-bashrc-are-not-loaded-to-jupy.md", "metadata": {"id": "efa85f71b6", "question": "Env variables set in ~/.bashrc are not loaded to Jupyter in VS Code", "sort_order": 41}, "content": "I added `PYTHONPATH`, `JAVA_HOME`, and `SPARK_HOME` to `~/.bashrc`. Importing `pyspark` worked in iPython in the terminal but couldn't be found in a `.ipynb` file opened in VS Code.\n\nAfter adding new lines to `~/.bashrc`, you need to restart the shell to activate the changes. You can do either of the following:\n\n```bash\nsource ~/.bashrc\n```\n\nor\n\n```bash\nexec bash\n```\n\nInstead of configuring paths in `~/.bashrc`, you can create a `.env` file in the root of your workspace with the following content:\n\n```bash\nJAVA_HOME=\"${HOME}/app/java/jdk-11.0.2\"\n\nPATH=\"${JAVA_HOME}/bin:${PATH}\"\n\nSPARK_HOME=\"${HOME}/app/spark/spark-3.3.2-bin-hadoop3\"\n\nPATH=\"${SPARK_HOME}/bin:${PATH}\"\n\nPYTHONPATH=\"${SPARK_HOME}/python/:$PYTHONPATH\"\n\nPYTHONPATH=\"${SPARK_HOME}/python/lib/py4j-0.10.9.5-src.zip:$PYTHONPATH\"\n\nPYTHONPATH=\"${SPARK_HOME}/python/lib/pyspark.zip:$PYTHONPATH\"\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-5/042_cd5d3e8423_hadoop-wc-l-is-giving-a-different-result-than-show.md", "metadata": {"id": "cd5d3e8423", "question": "hadoop “wc -l” is giving a different result than shown in the video", "sort_order": 42}, "content": "If you are using `wc -l fhvhv_tripdata_2021-01.csv.gz` with the gzip file as the file argument, you will get a different result, obviously, since the file is compressed.\n\nUnzip the file and then use:\n\n```bash\nwc -l fhvhv_tripdata_2021-01.csv\n```\n\nto get the right results."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-5/043_05fc1b38f6_hadoop-exception-in-thread-main-javalangunsatisfie.md", "metadata": {"id": "05fc1b38f6", "images": [{"description": "image #1", "id": "image_1", "path": "images/data-engineering-zoomcamp/image_073b1786.png"}, {"description": "image #2", "id": "image_2", "path": "images/data-engineering-zoomcamp/image_57fd99e0.png"}], "question": "Hadoop: Exception in thread \"main\" java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z", "sort_order": 43}, "content": "If you are seeing this (or similar) error when attempting to write to parquet, it is likely an issue with your path variables.\n\nFor Windows, follow these steps:\n\n1. Create a new User Variable \"HADOOP_HOME\" that points to your Hadoop directory.\n2. Add \"%HADOOP_HOME%\\bin\" to the PATH variable.\n\n<{IMAGE:image_1}>\n\n<{IMAGE:image_2}>\n\nAdditional tips can be found here: [Stack Overflow](https://stackoverflow.com/questions/41851066/exception-in-thread-main-java-lang-unsatisfiedlinkerror-org-apache-hadoop-io)"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-5/044_3a1fbd50a0_javaioioexception-cannot-run-program-chadoopbinwin.md", "metadata": {"id": "3a1fbd50a0", "question": "Java.io.IOException. Cannot run program “C:\\hadoop\\bin\\winutils.exe”. CreateProcess error=216, This version of 1% is not compatible with the version of Windows you are using.", "sort_order": 44}, "content": "To resolve the issue, follow these steps:\n\n1. Change the Hadoop version to 3.0.1.\n2. Replace all the files in the local Hadoop `bin` folder with the files from this repository: [winutils/hadoop-3.0.1/bin at master · cdarlint/winutils](https://github.com/cdarlint/winutils/tree/master/hadoop-3.0.1/bin).\n3. If this does not work, try other versions available in the repository.\n\nFor more information, refer to the following issue discussion: [This version of %1 is not compatible with the version of Windows you're running · Issue #20 · cdarlint/winutils](https://github.com/cdarlint/winutils/issues/20)"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-5/045_d01f2fb06b_dataproc-error-gclouddataprocjobssubmitpyspark-the.md", "metadata": {"id": "d01f2fb06b", "question": "Dataproc: ERROR: (gcloud.dataproc.jobs.submit.pyspark) The required property [project] is not currently set. It can be set on a per-command basis by re-running your command with the [--project] flag.", "sort_order": 45}, "content": "Fix is to set the flag like the error states. Get your project ID from your dashboard and set it like so:\n\n```bash\ngcloud dataproc jobs submit pyspark \\\n  --cluster=my_cluster \\\n  --region=us-central1 \\\n  --project=my-dtc-project-1010101 \\\n  gs://my-dtc-bucket-id/code/06_spark_sql.py \\\n  -- \n```\n\n<{IMAGE:image_id}>"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-5/046_61b2ad91aa_run-local-cluster-spark-in-windows-10-with-cmd.md", "metadata": {"id": "61b2ad91aa", "question": "Run Local Cluster Spark in Windows 10 with CMD", "sort_order": 46}, "content": "1. Go to `%SPARK_HOME%\\bin`\n\n2. Run the following command to start the master:\n\n   ```bash\n   spark-class org.apache.spark.deploy.master.Master\n   ```\n   \n   This will give you a URL of the form `spark://ip:port`.\n\n3. Run the following command to start the worker, replacing `spark://ip:port` with the URL obtained from the previous step:\n\n   ```bash\n   spark-class org.apache.spark.deploy.worker.Worker spark://ip:port\n   ```\n\n4. Create a new Jupyter notebook and set up the Spark session:\n\n   ```python\n   spark = SparkSession.builder \\\n       .master(\"spark://{ip}:7077\") \\\n       .appName('test') \\\n       .getOrCreate()\n   ```\n\n5. Check on the Spark UI to see the master, worker, and application running."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-5/047_fdef2dfcc4_lserviceexception-401-anonymous-caller-does-not-ha.md", "metadata": {"id": "fdef2dfcc4", "question": "lServiceException: 401 Anonymous caller does not have storage.objects.list access to the Google Cloud Storage bucket. Permission 'storage.objects.list' denied on resource (or it may not exist).", "sort_order": 47}, "content": "This occurs because you are not logged in with Google Cloud SDK, or the project ID is not set. Follow these steps:\n\n1. Log in using Google Cloud SDK:\n   \n   ```bash\n   gcloud auth login\n   ```\n   \n   This will open a tab in the browser. Accept the terms, then close the tab if needed.\n\n2. Set the project ID:\n   \n   ```bash\n   gcloud config set project <YOUR_PROJECT_ID>\n   ```\n\n3. Upload the `pq` directory to a Google Cloud Storage (GCS) Bucket:\n   \n   ```bash\n   gsutil -m cp -r pq/ <YOUR_URI_from_gsutil>/pq\n   ```"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-5/048_39fc22b5cb_gcp-py4jprotocolpy4jjavaerror.md", "metadata": {"id": "39fc22b5cb", "question": "GCP: py4j.protocol.Py4JJavaError", "sort_order": 48}, "content": "When submitting a job, you might encounter a `py4j.protocol.Py4JJavaError` related to Java in the log panel within Dataproc. \n\nTo address this error, consider the following steps:\n\n1. **Cluster Versioning Control:**\n   - If you've recently changed the versioning settings, ensure that the cluster configuration is compatible with your requirements. For example, switching from **Debian-Hadoop-Spark** to **Ubuntu 20.02-Hadoop3.3-Spark3.3** might resolve issues if you have a similar setup on your local machine.\n\n2. **Consistency with Local Environment:**\n   - Aligning the cluster's OS version and software stack with your local environment can help reduce configuration issues.\n\nAlthough specific documentation may not be available, this approach has proven effective in some scenarios."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-5/049_fa58733a98_repartition-the-dataframe-to-6-partitions-using-df.md", "metadata": {"id": "fa58733a98", "question": "Repartition the Dataframe to 6 partitions using df.repartition(6) - got 8 partitions instead", "sort_order": 49}, "content": "Use both `repartition` and `coalesce`, like so:\n\n```python\ndf = df.repartition(6)\n\ndf = df.coalesce(6)\n\ndf.write.parquet('fhv/2019/10', mode='overwrite')\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-5/050_05dc43cda8_jupyter-notebook-or-sparkui-not-loading-properly-a.md", "metadata": {"id": "05dc43cda8", "question": "Jupyter Notebook or SparkUI not loading properly at localhost after port forwarding from VS code?", "sort_order": 50}, "content": "**Possible Solution:** Try to forward the port using SSH CLI instead of VS Code. \n\nRun the following command:\n\n```bash\nssh -L <local port>:<VM host/ip>:<VM port> <ssh hostname>\n```\n\n- `ssh hostname` is the name you specified in the `~/.ssh/config` file.\n\nFor example, in case of Jupyter Notebook, run:\n\n```bash\nssh -L 8888:localhost:8888 gcp-vm\n```\n\nfrom your local machine’s CLI.\n\n**Note:** If you logout from the session, the connection would break. While creating the Spark session, check the block's log because sometimes it fails to run at 4040 and switches to 4041.\n\nIf you are having trouble accessing localhost ports from a GCP VM, consider adding the forwarding instructions to the `.ssh/config` file as follows:\n\n```bash\nHost <hostname>\n  Hostname <external-gcp-ip>\n  User xxxx\n  IdentityFile yyyy\n  LocalForward 8888 localhost:8888\n  LocalForward 8080 localhost:8080\n  LocalForward 5432 localhost:5432\n  LocalForward 4040 localhost:4040\n```\n\nThis should automatically forward all ports and will enable accessing localhost ports."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-5/051_72d6369d8a_installing-java-11-on-codespaces.md", "metadata": {"id": "72d6369d8a", "question": "Installing Java 11 on codespaces", "sort_order": 51}, "content": "1. Use the command below to check for available Java SDK versions:\n   \n   ```bash\n   sdk list java\n   ```\n\n2. Install the desired version, for example:\n   \n   ```bash\n   sdk install java 11.0.22-amzn\n   ```\n\n3. If prompted, press 'Y' to change the default Java version.\n\n4. Verify the installation by checking the Java version:\n   \n   ```bash\n   java -version\n   ```\n   \n5. If the version does not work correctly, set the default version with:\n   \n   ```bash\n   sdk default java 11.0.22-amzn\n   ```\n   \n   Adjust this command to match the version you have installed."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-5/052_0c9fc7e7d3_error-insufficient-ssd_total_gb-quota-requested-50.md", "metadata": {"id": "0c9fc7e7d3", "images": [{"description": "image #1", "id": "image_1", "path": "images/data-engineering-zoomcamp/image_6ab99490.png"}], "question": "Error: Insufficient 'SSD_TOTAL_GB' quota. Requested 500.0, available 470.0.", "sort_order": 52}, "content": "Sometimes while creating a dataproc cluster on GCP, the following error is encountered.\n\n<{IMAGE:image_1}>\n\nSolutions:\n\n1. As mentioned [here](https://stackoverflow.com/a/59038704/22748533), sometimes there might not be enough resources in the given region to allocate the request. Usually, resources get freed up in a bit, and one can create a cluster.\n\n2. Changing the type of boot-disk from PD-Balanced to PD-Standard in Terraform helped solve the problem."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-5/053_1da0437718_homework-how-to-convert-the-time-difference-of-two.md", "metadata": {"id": "1da0437718", "question": "Homework: how to convert the time difference of two timestamps to hours", "sort_order": 53}, "content": "Pyspark converts the difference of two `TimestampType` values to Python's native `datetime.timedelta` object. The `timedelta` object stores the duration in terms of days, seconds, and microseconds. Each of these units must be manually converted into hours to express the total duration between the two timestamps using only hours.\n\nAnother method to achieve this is using the `datediff` SQL function. It requires the following parameters:\n\n- **Upper Date**: The closer date, e.g., `dropoff_datetime`.\n- **Lower Date**: The farther date, e.g., `pickup_datetime`.\n\nThe result is returned in days, so you can multiply the result by 24 to get the duration in hours."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-5/054_1913393ea4_picklingerror-could-not-serialize-object-indexerro.md", "metadata": {"id": "1913393ea4", "question": "PicklingError: Could not serialize object: IndexError: tuple index out of range", "sort_order": 54}, "content": "This version combination worked for resolving the issue:\n\n- **PySpark**: 3.3.2\n- **Pandas**: 1.5.3\n\nIf you continue to encounter the error:\n\n```\nPy4JJavaError: An error occurred while calling o180.showString. : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 6.0 failed 1 times, most recent failure: Lost task 0.0 in stage 6.0 (TID 6) (host.docker.internal executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\n```\n\nTry running the following before initializing `SparkSession`:\n\n```python\nimport os\nimport sys\n\nos.environ['PYSPARK_PYTHON'] = sys.executable\nos.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-5/055_cb1c023dc2_runtimeerror-python-in-worker-has-different-versio.md", "metadata": {"id": "cb1c023dc2", "question": "RuntimeError: Python in worker has different version 3.11 than that in driver 3.10, PySpark cannot run with different minor versions. Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.", "sort_order": 55}, "content": "To resolve the version mismatch error between the worker and driver Python versions in PySpark, set the environment variables `PYSPARK_PYTHON` and `PYSPARK_DRIVER_PYTHON` to the same executable.\n\n```python\nimport os\nimport sys\n\nos.environ['PYSPARK_PYTHON'] = sys.executable\nos.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n```\n\nFor further information on Dataproc Pricing, visit: [Dataproc Pricing](https://cloud.google.com/dataproc/pricing#on_gke_pricing)"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-5/056_d707493842_dataproc-qn-is-it-essential-to-have-a-vm-on-gcp-fo.md", "metadata": {"id": "d707493842", "question": "Dataproc Qn: Is it essential to have a VM on GCP for running Dataproc and submitting jobs?", "sort_order": 56}, "content": "No, you can submit a job to Dataproc from your local computer by installing and configuring `gsutil`. For installation instructions, visit [https://cloud.google.com/storage/docs/gsutil_install](https://cloud.google.com/storage/docs/gsutil_install).\n\nYou can execute the following command from your local computer:\n\n```bash\ngcloud dataproc jobs submit pyspark \\\n  --cluster=de-zoomcamp-cluster \\\n  --region=europe-west6 \\\n  gs://dtc_data_lake_de-zoomcamp-nytaxi/code/06_spark_sql.py \\\n  -- \\\n  --input_green=gs://dtc_data_lake_de-zoomcamp-nytaxi/pq/green/2020/*/ \\\n  --input_yellow=gs://dtc_data_lake_de-zoomcamp-nytaxi/pq/yellow/2020/*/ \\\n  --output=gs://dtc_data_lake_de-zoomcamp-nytaxi/report-2020\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-5/057_0c116f6001_in-module-531-trying-to-run-sparkcreatedataframedf.md", "metadata": {"id": "0c116f6001", "images": [{"description": "image #1", "id": "image_1", "path": "images/data-engineering-zoomcamp/image_91f633ae.png"}], "question": "In module 5.3.1, trying to run `spark.createDataFrame(df_pandas).show()` returns error", "sort_order": 57}, "content": "```\nAttributeError: 'DataFrame' object has no attribute 'iteritems'\n```\n\nThis error occurs because a method inside PySpark refers to a package that has been deprecated ([Stack Overflow](https://stackoverflow.com/questions/76404811/attributeerror-dataframe-object-has-no-attribute-iteritems)).\n\n### Solutions\n\n- Refer to the code mentioned in the Stack Overflow link.\n\n  <{IMAGE:image_1}>\n\n- Another workaround is to create a conda environment to downgrade Python's version to 3.8 and Pandas to 1.5.3:\n\n  ```bash\n  conda create -n pyspark_env python=3.8 pandas=1.5.3 jupyter\n  \n  conda activate pyspark_env\n  ```"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-5/058_e058b2432a_cannot-create-a-cluster-insufficient-ssd_total_gb.md", "metadata": {"id": "e058b2432a", "question": "Cannot create a cluster: Insufficient 'SSD_TOTAL_GB' quota. Requested 500.0, available 250.0.", "sort_order": 58}, "content": "The master and worker nodes are allocated a maximum of 250 GB of memory combined. In the configuration section, adhere to the following specifications:\n\n- **Master Node:**\n  - Machine type: `n2-standard-2`\n  - Primary disk size: 85 GB\n\n- **Worker Node:**\n  - Number of worker nodes: 2\n  - Machine type: `n2-standard-2`\n  - Primary disk size: 80 GB\n\nYou can allocate up to 82.5 GB memory for worker nodes, keeping in mind that the total memory allocated across all nodes cannot exceed 250 GB."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-5/059_a5a8a6ac39_setting-java_home-with-homebrew-on-apple-silicon.md", "metadata": {"id": "a5a8a6ac39", "question": "Setting JAVA_HOME with Homebrew on Apple Silicon", "sort_order": 59}, "content": "The MacOS setup instruction ([here](https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/05-batch/setup/macos.md#installing-java)) for setting the `JAVA_HOME` environment variable is for Intel-based Macs, which have a default install location at `/usr/local/`. If you have an Apple Silicon Mac, you need to set `JAVA_HOME` to `/opt/homebrew/`. Update your `.bashrc` or `.zshrc` file with the following:\n\n```bash\nexport JAVA_HOME=\"/opt/homebrew/opt/openjdk/bin\"\nexport PATH=\"$JAVA_HOME:$PATH\"\n```\n\nConfirm that your path was correctly set by running the command:\n\n```bash\nwhich java\n```\n\nYou should expect to see the output:\n\n```\n/opt/homebrew/opt/openjdk/bin/java\n```\n\nCheck the Java version with the following command:\n\n```bash\njava -version\n```\n\nReference: [https://docs.brew.sh/Installation](https://docs.brew.sh/Installation)"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-5/060_a25406395b_subnetwork-default-does-not-support-private-google.md", "metadata": {"id": "a25406395b", "question": "Subnetwork 'default' does not support Private Google Access which is required for Dataproc clusters when 'internal_ip_only' is set to 'true'. Enable Private Google Access on subnetwork 'default' or set 'internal_ip_only' to 'false'.", "sort_order": 60}, "content": "To resolve this issue, follow these steps:\n\n1. Search for VPC in Google Cloud Console.\n2. Navigate to the \"SUBNETS IN CURRENT PROJECT\" tab.\n3. Locate the region/location where your Dataproc will be located and click on it.\n4. Click the edit button and toggle on \"Private Google Access.\"\n5. Save changes."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-6/001_5b1d465332_spark-is-working-however-nothing-appears-in-the-sp.md", "metadata": {"id": "5b1d465332", "question": "Spark: Is working, however, nothing appears in the Spark UI (e.g., .show())?", "sort_order": 1}, "content": "Since we used multiple notebooks during the course, it's possible that there are more than one Spark session active. It’s highly likely that you are observing the incorrect one. Follow these steps to troubleshoot:\n\n- Spark uses port 4040 by default, but if more than one session is active, it will try to use the next available port (e.g., 4041).\n\n- Ensure you're viewing the correct Spark Web UI for the application where your jobs are running.\n\n- Verify the current application session address:\n  \n  ```python\n  # Using the following command in your session\n  spark.sparkContext.uiWebUrl\n  ```\n  \n  Expected output might look like:\n  \n  ```\n  http://your.application.session.address.internal:4041\n  ```\n  \n  Indicating port 4041.\n\n- If using a VM, make sure you forward the identified port to access the web UI on `localhost:<port>`."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-6/002_94ccf8c158_docker-could-not-start-docker-image-control-center.md", "metadata": {"id": "94ccf8c158", "question": "Docker: Could not start docker image \"control-center\" from the docker-compose.yaml file.", "sort_order": 2}, "content": "Check Docker Compose File:\n\nEnsure that your `docker-compose.yaml` file is correctly configured with the necessary details for the \"control-center\" service. Check the service name, image name, ports, volumes, environment variables, and any other configurations required for the container to start.\n\nOn Mac OSX 12.2.1 (Monterey), if you encounter this issue, try the following:\n\n- Open Docker Desktop and check for any images still running from previous sessions.\n- If there are images still running and they do not appear with the `docker ps` command, they may need to be deleted directly from Docker Desktop.\n- Once those images are removed, try starting the Kafka environment again.\n\nThis approach resolved the issue on Mac OSX 12.2.1 for a similar setup."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-6/003_a1ea959c7e_module-kafka-not-found-when-trying-to-run-producer.md", "metadata": {"id": "a1ea959c7e", "question": "Module “kafka” not found when trying to run producer.py", "sort_order": 3}, "content": "To resolve the \"Module 'kafka' not found\" error when running `producer.py`, you can create a virtual environment and install the required packages. Follow these steps:\n\n1. **Create a Virtual Environment**\n   \n   Run the following command to create a virtual environment:\n   \n   ```bash\n   python -m venv env\n   ```\n\n2. **Activate the Virtual Environment**\n\n   - On macOS and Linux:\n     \n     ```bash\n     source env/bin/activate\n     ```\n   \n   - On Windows:\n     \n     ```bash\n     env\\Scripts\\activate\n     ```\n\n3. **Install Required Packages**\n   \n   Install the packages listed in `requirements.txt`:\n   \n   ```bash\n   pip install -r ../requirements.txt\n   ```\n\n4. **Deactivate the Virtual Environment**\n   \n   When you're done, deactivate the virtual environment:\n   \n   ```bash\n   deactivate\n   ```\n\n**Note:** Ensure that Docker images are running before executing the Python file. The virtual environment is meant for running the Python files locally."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-6/004_1cc0ab1fea_error-importing-cimpl-dll-when-running-avro-exampl.md", "metadata": {"id": "1cc0ab1fea", "question": "Error: Importing cimpl DLL when running Avro examples", "sort_order": 4}, "content": "```\nImportError: DLL load failed while importing cimpl: The specified module could not be found\n```\n\n### Steps to Resolve:\n\n1. **Verify Python Version:**\n   \n   Ensure you are using a compatible version of Python with the Avro library. Check the Python version and compatibility requirements specified by the Avro library documentation.\n\n2. **Load Required DLL:**\n   \n   You may need to load `librdkafka-5d2e2910.dll` in your code before importing Avro. Add the following:\n   \n   ```python\n   from ctypes import CDLL\n\n   CDLL(\"C:\\\\Users\\\\YOUR_USER_NAME\\\\anaconda3\\\\envs\\\\dtcde\\\\Lib\\\\site-packages\\\\confluent_kafka.libs\\\\librdkafka-5d2e2910.dll\")\n   ```\n   \n   Note that this error may occur depending on the OS and Python version installed.\n\n3. **Alternative Solution:**\n\n   If you encounter `ImportError: DLL load failed while importing cimpl`, you can try the following solution in PowerShell:\n\n   ```bash\n   $env:CONDA_DLL_SEARCH_MODIFICATION_ENABLE=1\n   ```\n\n   This sets the DLL manually in the Conda environment.\n\n### Source:\n\n[Confluent-Kafka Python Issue #1186](https://githubhot.com/repo/confluentinc/confluent-kafka-python/issues/1186?page=2)"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-6/005_0a441d9976_modulenotfounderror-no-module-named-avro.md", "metadata": {"id": "0a441d9976", "question": "ModuleNotFoundError: No module named 'avro'", "sort_order": 5}, "content": "### Solution\n\nTo resolve the error, install the Avro module using the following command:\n\n```bash\npip install confluent-kafka[avro]\n```\n\nNote: This issue may occur because Conda does not include the Avro module when installing `confluent-kafka` via pip.\n\n### Additional Resources\n\nFor more information on Anaconda and `confluent-kafka` issues, visit the following links:\n\n- [GitHub Issue 590](https://github.com/confluentinc/confluent-kafka-python/issues/590)\n- [GitHub Issue 1221](https://github.com/confluentinc/confluent-kafka-python/issues/1221)\n- [StackOverflow Discussion](https://stackoverflow.com/questions/69085157/cannot-import-producer-from-confluent-kafka)"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-6/006_44cc6eaa10_error-while-running-python3-streampy-worker.md", "metadata": {"id": "44cc6eaa10", "question": "Error while running python3 stream.py worker", "sort_order": 6}, "content": "If you get an error while running the command `python3 stream.py worker`, follow these steps:\n\n1. Uninstall the current `kafka-python` package:\n   \n   ```bash\n   pip uninstall kafka-python\n   ```\n\n2. Install the specific version of `kafka-python`:\n\n   ```bash\n   pip install kafka-python==1.4.6\n   ```"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-6/007_f2a0d0c3f0_what-is-the-use-of-redpanda.md", "metadata": {"id": "f2a0d0c3f0", "question": "What is the use of Redpanda?", "sort_order": 7}, "content": "Redpanda is built on top of the Raft consensus algorithm and is designed as a high-performance, low-latency alternative to Kafka. It uses a log-centric architecture similar to Kafka but with different underlying principles."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-6/008_b943770904_negsignalsigkill-while-converting-data-files-to-pa.md", "metadata": {"id": "b943770904", "question": "Negsignal:SIGKILL while converting data files to parquet format", "sort_order": 8}, "content": "Got this error because the Docker container memory was exhausted. The data file was up to 800MB but my Docker container does not have enough memory to handle that.\n\n**Solution:**\n\n- Load the file in chunks with Pandas.\n- Create multiple parquet files for each data file being processed.\n\nThis approach worked smoothly and resolved the issue."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-6/009_dcdd7eda4d_resourcesridescsv-is-missing.md", "metadata": {"id": "dcdd7eda4d", "question": "resources/rides.csv is missing", "sort_order": 9}, "content": "Copy the file found in the Java example: `data-engineering-zoomcamp/week_6_stream_processing/java/kafka_examples/src/main/resources/rides.csv`"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-6/010_348f696873_kafka-python-videos-have-low-audio-and-are-hard-to.md", "metadata": {"id": "348f696873", "question": "Kafka: Python videos have low audio and are hard to follow up", "sort_order": 10}, "content": "To improve the audio quality:\n\n- **Download the videos** and use VLC media player. You can set the audio to 200% of the original volume for better sound quality.\n- Alternatively, **use auto-generated captions** directly on YouTube for better clarity."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-6/011_759b5d80ec_kafka-python-videos-ridescsv.md", "metadata": {"id": "759b5d80ec", "question": "Kafka Python Videos: Rides.csv", "sort_order": 11}, "content": "There is no clear explanation of the `rides.csv` data that the `producer.py` Python programs use. You can find it here: [Rides CSV File](https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/2bd33e89906181e424f7b12a299b70b19b7cfcd5/week_6_stream_processing/python/resources/rides.csv)."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-6/012_ea9c96ab72_kafkaerrorsnobrokersavailable-nobrokersavailable.md", "metadata": {"id": "ea9c96ab72", "question": "kafka.errors.NoBrokersAvailable: NoBrokersAvailable", "sort_order": 12}, "content": "If you encounter this error, it is likely that your Kafka broker Docker container is not running.\n\n- Use the following command to check the running containers:\n\n  ```bash\n  docker ps\n  ```\n\n- Navigate to the folder containing your Docker Compose YAML file and execute the following command to start all instances:\n\n  ```bash\n  docker-compose up -d\n  ```"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-6/013_443b9afd04_kafka-homework-q3-there-are-options-that-support-t.md", "metadata": {"id": "443b9afd04", "question": "Kafka homework Q3: There are options that support the scaling concept more than the others.", "sort_order": 13}, "content": "Focus on the horizontal scaling option.\n\nThink of scaling in terms of scaling from the consumer end, or consuming messages via horizontal scaling."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-6/014_664833c280_docker-how-to-fix-docker-compose-error-error-respo.md", "metadata": {"id": "664833c280", "question": "Docker: How to fix docker compose error: Error response from daemon: pull access denied for spark-3.3.1, repository does not exist or may require 'docker login': denied: requested access to the resource is denied", "sort_order": 14}, "content": "If you get this error, it means you have not built your Spark and Jupyter images. These images aren’t readily available on DockerHub.\n\nTo resolve this:\n\n- In the Spark folder, run the following command from a bash CLI to build all images before running docker compose:\n  \n  ```bash\n  ./build.sh\n  ```"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-6/015_e821d8fe04_python-kafka-buildsh-permission-denied-error.md", "metadata": {"id": "e821d8fe04", "question": "Python Kafka: ./build.sh: Permission denied Error", "sort_order": 15}, "content": "Run this command in the terminal in the same directory (`/docker/spark`):\n\n```bash\nchmod +x build.sh\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-6/016_b2c1c1d0ec_python-kafka-kafkatimeouterror-failed-to-update-me.md", "metadata": {"id": "b2c1c1d0ec", "question": "Python Kafka: ‘KafkaTimeoutError: Failed to update metadata after 60.0 secs.’ when running stream-example/producer.py", "sort_order": 16}, "content": "Restarting all services worked for me:\n\n```bash\ndocker-compose down\n\ndocker-compose up\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-6/017_477104b863_python-kafka-spark-submitsh-streamingpy-error-stan.md", "metadata": {"id": "477104b863", "question": "Python Kafka: ./spark-submit.sh streaming.py - ERROR StandaloneSchedulerBackend: Application has been killed. Reason: All masters are unresponsive! Giving up.", "sort_order": 17}, "content": "While following [tutorial 13.2](https://www.youtube.com/watch?v=5hRJ8-6Fpyk&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=79), when running `./spark-submit.sh streaming.py`, encountered the following error:\n\n```\n24/03/11 09:48:36 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://localhost:7077...\n\n24/03/11 09:48:36 INFO TransportClientFactory: Successfully created connection to localhost/127.0.0.1:7077 after 10 ms (0 ms spent in bootstraps)\n\n24/03/11 09:48:54 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n\n24/03/11 09:48:56 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://localhost:7077…\n\n24/03/11 09:49:16 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://localhost:7077...\n\n24/03/11 09:49:36 WARN StandaloneSchedulerBackend: Application ID is not initialized yet.\n\n24/03/11 09:49:36 ERROR StandaloneSchedulerBackend: Application has been killed. Reason: All masters are unresponsive! Giving up.\n\npy4j.protocol.Py4JJavaError: An error occurred while calling None.org.apache.spark.sql.SparkSession.\n\n: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\n```\n\n**Solution:**\n\n1. **Downgrade PySpark:**\n   - Downgrade your local PySpark to 3.3.1 (ensure it matches the version used in Dockerfile).\n   - The mismatch of PySpark versions can be a cause of the failed connection.\n   - Check the logs of `spark-master` in the Docker container for confirmation.\n\n2. **Check Spark Version:**\n   - Run `pyspark --version` and `spark-submit --version` to check your local Spark version.\n   - Adjust the `SPARK_VERSION` variable in `build.sh` to match your current Spark version."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-6/018_e7015b4263_python-kafka-spark-submitsh-streamingpy-how-to-che.md", "metadata": {"id": "e7015b4263", "question": "Python Kafka: ./spark-submit.sh streaming.py - How to check why Spark master connection fails", "sort_order": 18}, "content": "- Start a new terminal.\n\n- Run the following command to list running containers:\n\n  ```bash\n  docker ps\n  ```\n\n- Copy the `CONTAINER ID` of the `spark-master` container.\n\n- Execute the following command to access the container's shell:\n\n  ```bash\n  docker exec -it <spark_master_container_id> bash\n  ```\n\n- Run this command to view the Spark master logs:\n\n  ```bash\n  cat logs/spark-master.out\n  ```\n\n- Check the log for the timestamp when the error occurred.\n\n- Search the error message online for further troubleshooting."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-6/019_0e8bce921a_python-kafka-spark-submitsh-streamingpy-error-py4j.md", "metadata": {"id": "0e8bce921a", "question": "Python Kafka: ./spark-submit.sh streaming.py Error: py4j.protocol.Py4JJavaError: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.", "sort_order": 19}, "content": "Make sure your Java version is 11 or 8.\n\n- Check your version by:\n\n  ```bash\n  java --version\n  ```\n\n- Check all your installed Java versions by:\n\n  ```bash\n  /usr/libexec/java_home -V\n  ```\n\n- If you already have Java 11 but it's not set as the default, select the specific version by:\n\n  ```bash\n  export JAVA_HOME=$(/usr/libexec/java_home -v 11.0.22)\n  ```\n\n  (or another version of 11)"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-6/020_d01cbfa9cb_java-kafka-project_name-10-snapshotjar-errors-pack.md", "metadata": {"id": "d01cbfa9cb", "question": "Java Kafka: <project_name>-1.0-SNAPSHOT.jar errors: package xxx does not exist even after gradle build", "sort_order": 20}, "content": "In my setup, all of the dependencies listed in `build.gradle` were not installed in `<project_name>-1.0-SNAPSHOT.jar`.\n\nSolution:\n\n1. In the `build.gradle` file, add the following at the end:\n   \n   ```groovy\n   shadowJar {\n       archiveBaseName = \"java-kafka-rides\"\n       archiveClassifier = ''\n   }\n   ```\n\n2. In the command line, run:\n   \n   ```bash\n   gradle shadowjar\n   ```\n\n3. Execute the script from `java-kafka-rides-1.0-SNAPSHOT.jar` created by the shadowjar."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-6/021_2763850d3e_python-kafka-installing-dependencies-for-python3-0.md", "metadata": {"id": "2763850d3e", "question": "Python Kafka: Installing dependencies for python3 06-streaming/python/avro_example/producer.py", "sort_order": 21}, "content": "To install the necessary dependencies for running `producer.py` in the `avro_example` directory, use the following commands:\n\n- Install `confluent-kafka`:\n  - Using pip:\n  ```bash\n  pip install confluent-kafka\n  ```\n  - Using conda:\n  ```bash\n  conda install conda-forge::python-confluent-kafka\n  ```\n\n- Install `fastavro`:\n  ```bash\n  pip install fastavro\n  ```"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-6/022_2d274b7acd_can-i-install-the-faust-library-for-module-6-pytho.md", "metadata": {"id": "2d274b7acd", "question": "Can I install the Faust Library for Module 6 Python Version due to dependency conflicts?", "sort_order": 22}, "content": "The Faust repository and library is no longer maintained - [GitHub Repository](https://github.com/robinhood/faust)\n\nIf you do not know Java, you now have the option to follow the Python Videos 6.13 & 6.14 here: [YouTube Playlist](https://www.youtube.com/watch?v=BgAlVknDFlQ&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=80). Follow the RedPanda Python version here: [RedPanda Example](https://github.com/DataTalksClub/data-engineering-zoomcamp/tree/main/06-streaming/python/redpanda_example).\n\n**Note:** I highly recommend watching the Java videos to understand the concept of streaming, but you can skip the coding parts. All will become clear when you get to the Python videos and RedPanda files."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-6/023_5ca6890c1a_java-kafka-how-to-run-producerconsumerkstreamsetc.md", "metadata": {"id": "5ca6890c1a", "question": "Java Kafka: How to run producer/consumer/kstreams/etc in terminal", "sort_order": 23}, "content": "In the project directory, run:\n\n```bash\njava -cp build/libs/<jar_name>-1.0-SNAPSHOT.jar:out src/main/java/org/example/JsonProducer.java\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-6/024_cd8a62fc55_java-kafka-when-running-the-producerconsumeretc-ja.md", "metadata": {"id": "cd8a62fc55", "question": "Java Kafka: When running the producer/consumer/etc java scripts, no results retrieved or no message sent", "sort_order": 24}, "content": "For example, when running `JsonConsumer.java`, you might see:\n\n```\nConsuming form kafka started\n\nRESULTS:::0\n\nRESULTS:::0\n\nRESULTS:::0\n```\n\nOr when running `JsonProducer.java`, you might encounter:\n\n```\nException in thread \"main\" java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.SaslAuthenticationException: Authentication failed\n```\n\n**Solution:**\n\n1. Ensure the `StreamsConfig.BOOTSTRAP_SERVERS_CONFIG` in the scripts located at `src/main/java/org/example/` (e.g., `JsonConsumer.java`, `JsonProducer.java`) is pointing to the correct server URL (e.g., `europe-west3` vs `europe-west2`).\n\n2. Verify that the cluster key and secrets are updated in `src/main/java/org/example/Secrets.java` (`KAFKA_CLUSTER_KEY` and `KAFKA_CLUSTER_SECRET`)."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-6/025_d039adfb76_java-kafka-tests-are-not-picked-up-in-vscode.md", "metadata": {"id": "d039adfb76", "images": [{"description": "image #1", "id": "image_1", "path": "images/data-engineering-zoomcamp/image_07f863ec.png"}, {"description": "image #2", "id": "image_2", "path": "images/data-engineering-zoomcamp/image_df52218e.png"}, {"description": "image #3", "id": "image_3", "path": "images/data-engineering-zoomcamp/image_deaf91b1.png"}, {"description": "image #4", "id": "image_4", "path": "images/data-engineering-zoomcamp/image_8584c5ce.png"}], "question": "Java Kafka: Tests are not picked up in VSCode", "sort_order": 25}, "content": "In VS Code, you might expect to see a triangle icon next to each test method in your Java files. If you don't see it, here are the steps to fix the issue:\n\n1. Open **Explorer** (first icon on the left navigation bar).\n2. Navigate to **JAVA PROJECTS** (bottom collapsable section).\n3. Click the icon in the rightmost position next to **JAVA PROJECTS** to open the options.\n   \n   <{IMAGE:image_1}>\n\n   <{IMAGE:image_2}>\n   \n   <{IMAGE:image_3}>\n\n4. Select **Clean Workspace**.\n5. Confirm by clicking **Reload and Delete**.\n\nFollowing these steps should restore the triangle icons you expect to see next to each test, similar to those visible in Python tests.\n\nExample:\n\n<{IMAGE:image_4}>\n\nAdditionally, you can add classes and packages in this window instead of creating files directly in the project directory."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-6/026_30fbb4f5b8_confluent-kafka-where-can-i-find-schema-registry-u.md", "metadata": {"id": "30fbb4f5b8", "question": "Confluent Kafka: Where can I find schema registry URL?", "sort_order": 26}, "content": "In [Confluent Cloud](https://confluent.cloud/):\n\n- Navigate to your Environment (e.g., default or a custom name).\n- Use the right navigation bar to find \"Stream Governance API.\"\n- The URL can be found under \"Endpoint.\"\n- Create credentials from the Credentials section below it."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-6/027_663c1e915e_how-do-i-check-compatibility-of-local-and-containe.md", "metadata": {"id": "663c1e915e", "question": "How do I check compatibility of local and container Spark versions?", "sort_order": 27}, "content": "You can check the version of your local Spark using:\n\n```bash\nspark-submit --version\n```\n\nIn the `build.sh` file of the Python folder, ensure that `SPARK_VERSION` matches your local version. Similarly, ensure the PySpark you installed via pip also matches this version."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-6/028_7c3cf3b8fc_how-to-fix-the-error-modulenotfounderror-no-module.md", "metadata": {"id": "7c3cf3b8fc", "question": "How to fix the error \"ModuleNotFoundError: No module named 'kafka.vendor.six.moves'?\"", "sort_order": 28}, "content": "According to [GitHub](https://github.com/dpkp/kafka-python/):\n\n\"DUE TO ISSUES WITH RELEASES, IT IS SUGGESTED TO USE [GitHub](https://github.com/wbarnha/kafka-python-ng) FOR THE TIME BEING.\"\n\nUse the following command to install:\n\n```bash\npip install kafka-python-ng\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/module-6/029_8fe89183d7_how-to-fix-connection-failed-connection-to-server.md", "metadata": {"id": "8fe89183d7", "question": "How to fix “connection failed: connection to server at \"127.0.0.1\", port 5432 failed” error when setting up Postgres connection in pgAdmin?", "sort_order": 29}, "content": "Instead of using “localhost” as the host name/address, try “postgres”, or “host.docker.internal” instead.\n\nAlternative Solution:\n\n- If you have installed Postgres locally and disabled persist data on the Postgres container in Docker (i.e., volume: removed), use a Postgres port other than 5432, such as 5433.\n- For the pgAdmin host name/address, if 'localhost', 'postgres', or 'host.docker.internal' are not working, you can use your own IPv4 Address.\n\n  To find your IPv4 Address on Windows OS:\n  \n  1. Open Command Prompt.\n  2. Run the command:\n     \n     ```bash\n     ipconfig\n     ```\n  \n  3. Look under Wireless LAN adapter WiFi 2 for the IPv4 Address. For example:\n\n     ```text\n     IPv4 Address. . . . . . . . . . . : 192.168.0.148\n     ```"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/project/001_56d8f7ae9a_why-is-my-table-not-being-created-in-postgresql-wh.md", "metadata": {"id": "56d8f7ae9a", "question": "Why is my table not being created in PostgreSQL when I submit a job?", "sort_order": 1}, "content": "There could be a few reasons for this issue:\n\n- **Race Conditions**: If you're running multiple processes in parallel.\n\n- **Database Connection Issues**: The job might not be connecting to the correct PostgreSQL database, or there could be authentication or permission issues preventing table creation.\n\n- **Missing Table Creation Logic**: The code responsible for creating the table might not be properly included or executed in the job submission process.\n\nAs a best practice, it's generally recommended to pre-create tables in PostgreSQL to avoid runtime errors. This ensures the database schema is properly set up before any jobs are executed.\n\nExtra: Use `CREATE TABLE IF NOT EXISTS` in your code. This will prevent errors if the table already exists and ensure smooth job execution."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/project/002_ab1b812541_how-is-my-capstone-project-going-to-be-evaluated.md", "metadata": {"id": "ab1b812541", "question": "How is my capstone project going to be evaluated?", "sort_order": 2}, "content": "Each submitted project will be evaluated by three randomly assigned students who have also submitted the project.\n\nYou will also be responsible for grading projects from three fellow students yourself. Please note that not complying with this rule will result in failing to achieve the Certificate at the end of the course.\n\nThe final grade you receive will be the median score of the grades from peer reviewers.\n\nThe peer review criteria for evaluating or being evaluated must follow the guidelines defined [here](https://github.com/DataTalksClub/data-engineering-zoomcamp/tree/main/week_7_project#peer-review-criteria)."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/project/003_5a491cd847_can-i-collaborate-with-others-on-the-capstone-proj.md", "metadata": {"id": "5a491cd847", "question": "Can I collaborate with others on the capstone project?", "sort_order": 3}, "content": "Collaboration is not allowed for the capstone submission. However, you can discuss ideas and get feedback from peers in the forums or Slack channels."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/project/004_5c5321c295_project-1-project-2.md", "metadata": {"id": "5c5321c295", "question": "Project 1 & Project 2", "sort_order": 4}, "content": "There is only ONE project for this Zoomcamp. You do not need to submit or create two projects.\n\nThere are simply TWO chances to pass the course. You can use the Second Attempt if you:\n\n- Fail the first attempt\n- Do not have the time due to other engagements such as holidays or sickness to enter your project into the first attempt.\n\n**Project Evaluation - Reproducibility**\n\nEven with thorough documentation, ensuring that a peer reviewer can follow your steps can be challenging. Here’s how this criterion will be evaluated:\n\n> \"Ideally yes, you should try to re-run everything. But I understand that not everyone has time to do it, so if you check the code by looking at it and try to spot errors, places with missing instructions, and so on - then it's already great.\"\n\n**Certificates: How do I get it?**\n\nSee the `certificate.mdx` file."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/project/005_2220285e58_does-anyone-know-nice-and-relatively-large-dataset.md", "metadata": {"id": "2220285e58", "question": "Does anyone know nice and relatively large datasets?", "sort_order": 5}, "content": "See a list of datasets here: [GitHub Datasets](https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/projects/datasets.md)"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/project/006_a3776dc060_how-to-run-python-as-a-startup-script.md", "metadata": {"id": "a3776dc060", "question": "How to run Python as a startup script?", "sort_order": 6}, "content": "You need to redefine the Python environment variable to that of your user account."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/project/007_ce77e05d24_spark-streaming-how-do-i-read-from-multiple-topics.md", "metadata": {"id": "ce77e05d24", "question": "Spark Streaming: How do I read from multiple topics in the same Spark Session", "sort_order": 7}, "content": "To read from multiple topics in the same Spark session, follow these steps:\n\n1. **Initiate a Spark Session:**\n   \n   ```python\n   spark = (SparkSession\n       .builder\n       .appName(app_name)\n       .master(master=master)\n       .getOrCreate())\n   \n   spark.streams.resetTerminated()\n   ```\n\n2. **Read Streams from Multiple Topics:**\n   \n   ```python\n   query1 = spark\n       .readStream\n       ...\n       ...\n       .load()\n   \n   query2 = spark\n       .readStream\n       ...\n       ...\n       .load()\n   \n   query3 = spark\n       .readStream\n       ...\n       ...\n       .load()\n   ```\n\n3. **Start the Queries:**\n   \n   ```python\n   query1.start()\n   query2.start()\n   query3.start()\n   ```\n\n4. **Await Termination:**\n   \n   ```python\n   spark.streams.awaitAnyTermination()  # Waits for any one of the queries to receive a kill signal or error failure. This is asynchronous.\n   ```\n\n   Note: `query3.start().awaitTermination()` is a blocking call. It works well when we are reading only from one topic."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/project/008_1c7bfa9357_data-transformation-from-databricks-to-azure-sql-d.md", "metadata": {"id": "1c7bfa9357", "question": "Data Transformation from Databricks to Azure SQL DB", "sort_order": 8}, "content": "Transformed data can be moved into Azure Blob Storage and then it can be moved into Azure SQL DB, instead of moving directly from Databricks to Azure SQL DB."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/project/009_c81e613d5a_orchestrating-dbt-with-airflow.md", "metadata": {"id": "c81e613d5a", "question": "Orchestrating dbt with Airflow", "sort_order": 9}, "content": "The trial dbt account provides access to the dbt API. A job will still need to be added manually. Airflow can run the job using a Python operator that calls the API. You will need to provide an API key, job ID, etc., and be careful not to commit this information to GitHub.\n\n- Detailed explanation: [dbt and Airflow Spiritual Alignment](https://docs.getdbt.com/blog/dbt-airflow-spiritual-alignment)\n- Source code example: [GitHub dbt Cloud Example](https://github.com/sungchun12/airflow-toolkit/blob/95d40ac76122de337e1b1cdc8eed35ba1c3051ed/dags/examples/dbt_cloud_example.py)"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/project/010_ec1001476f_orchestrating-dataproc-with-airflow.md", "metadata": {"id": "ec1001476f", "question": "Orchestrating DataProc with Airflow", "sort_order": 10}, "content": "For orchestrating DataProc with Airflow, you can refer to the following documentation:\n\n- [Airflow DataProc Operators - API Docs](https://airflow.apache.org/docs/apache-airflow-providers-google/stable/_api/airflow/providers/google/cloud/operators/dataproc/index.html)\n- [Airflow DataProc Operators - Module Docs](https://airflow.apache.org/docs/apache-airflow-providers-google/stable/_modules/airflow/providers/google/cloud/operators/dataproc.html)\n\n### Roles for Service Account\n\nEnsure that you assign the following roles to your service account:\n\n- **DataProc Administrator**\n- **Service Account User**\n  \n  For more details, see the explanation on [Stack Overflow](https://stackoverflow.com/questions/63941429/user-not-authorized-to-act-as-service-account-when-using-workload-identity).\n\n### Operators to Use\n\n- `DataprocSubmitPySparkJobOperator`\n- `DataprocDeleteClusterOperator`\n- `DataprocCreateClusterOperator`\n\n### Important Note\n\nWhen using `DataprocSubmitPySparkJobOperator`, make sure to add the BigQuery Connector, as DataProc does not include it by default:\n\n```python\n  dataproc_jars = [\"gs://spark-lib/bigquery/spark-bigquery-with-dependencies_2.12-0.24.0.jar\"]\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/project/011_6c9785b291_orchestrating-dbt-cloud-with-mage.md", "metadata": {"id": "6c9785b291", "images": [{"description": "image #1", "id": "image_1", "path": "images/data-engineering-zoomcamp/image_981381c8.png"}, {"description": "image #2", "id": "image_2", "path": "images/data-engineering-zoomcamp/image_ebc771b3.png"}], "question": "Orchestrating dbt cloud with Mage", "sort_order": 11}, "content": "You can trigger your dbt job in a Mage pipeline. Follow these steps:\n\n1. Retrieve your dbt cloud API key from **Settings > API Tokens > Personal Tokens**. Add this key safely to your `.env` file. For example:\n\n   ```bash\n   dbt_api_trigger=dbt_**\n   ```\n\n2. Navigate to the job page in dbt cloud and find the API trigger link.\n\n3. Create a custom Mage Python block with a simple HTTP request, as shown in [this example](https://github.com/Nogromi/ukraine-vaccinations/blob/master/2_mage/vaccination/custom/trigger_dbt_cloud.py).\n\n   <{IMAGE:image_1}>\n\n   <{IMAGE:image_2}>\n\n4. Use the following script to trigger the dbt job:\n\n   ```python\n   from dotenv import load_dotenv\n   from pathlib import Path\n\n   dotenv_path = Path('/home/src/.env')\n   load_dotenv(dotenv_path=dotenv_path)\n\n   dbt_api_trigger = os.getenv('dbt_api_trigger')\n   \n   url = f\"https://cloud.getdbt.com/api/v2/accounts/{dbt_account_id}/jobs/<job_id>/run/\"\n\n   headers = {\n       \"Authorization\": f\"Token {dbt_api_trigger}\",\n       \"Content-Type\": \"application/json\"\n   }\n\n   body = {\n       \"cause\": \"Triggered via API\"\n   }\n\n   response = requests.post(url, headers=headers, json=body)\n   ```\n\n   Voilà! You've triggered a dbt job from your Mage pipeline."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/project/012_ec5e405fa6_key-vault-in-azure-cloud-stack.md", "metadata": {"id": "ec5e405fa6", "question": "Key Vault in Azure cloud stack", "sort_order": 12}, "content": "The Key Vault in Azure Cloud is used to store credentials, passwords, or secrets for different technologies used within Azure. For example, if you do not want to expose the password of an SQL database, you can save the password under a given name and use it in other Azure services."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/project/013_b3bb998ae2_how-to-connect-pyspark-with-bigquery.md", "metadata": {"id": "b3bb998ae2", "question": "How to connect Pyspark with BigQuery?", "sort_order": 13}, "content": "To connect Pyspark with BigQuery, include the following line in the Pyspark configuration:\n\n```python\n# Example initialization of SparkSession variable\n\nspark = (SparkSession.builder\n\n    .master(\"...\")\n    .appName(\"...\")\n    \n    # Add the following configuration\n    .config(\"spark.jars.packages\", \"com.google.cloud.spark:spark-3.5-bigquery:0.37.0\")\n)\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/project/014_e839b64165_how-to-run-a-dbt-core-project-as-an-airflow-task-g.md", "metadata": {"id": "e839b64165", "images": [{"description": "image #1", "id": "image_1", "path": "images/data-engineering-zoomcamp/image_bd4861e1.png"}], "question": "How to run a dbt-core project as an Airflow Task Group on Google Cloud Composer using a service account JSON key", "sort_order": 14}, "content": "1. Install the [astronomer-cosmos](https://github.com/astronomer/astronomer-cosmos) package as a dependency. Refer to the installation guide [here](https://cloud.google.com/composer/docs/composer-2/install-python-dependencies#install_custom_packages_in_a_environment) and see a Terraform [example](https://github.com/wndrlxx/ca-trademarks-data-pipeline/blob/4e6a0e757495a99e01ff6c8b981a23d6dc421046/terraform/main.tf#L100).\n\n2. Create a new folder, `dbt/`, inside the `dags/` folder of your Composer GCP bucket and copy your dbt-core project there. See the [example](https://github.com/wndrlxx/ca-trademarks-data-pipeline/tree/4e6a0e757495a99e01ff6c8b981a23d6dc421046/dags/dbt/ca_trademarks_dp).\n\n3. Ensure your `profiles.yml` is configured to authenticate with a service account key. Refer to the BigQuery [example](https://docs.getdbt.com/docs/core/connect-data-platform/bigquery-setup#service-account-file).\n\n4. Create a new DAG using the `DbtTaskGroup` class. Use a `ProfileConfig` specifying a `profiles_yml_filepath` that points to the location of your JSON key file. See this [example](https://github.com/wndrlxx/ca-trademarks-data-pipeline/blob/4e6a0e757495a99e01ff6c8b981a23d6dc421046/dags/6_dbt_cosmos_task_group.py#L47).\n\nYour dbt lineage graph should now appear as tasks inside a task group like this:\n\n<{IMAGE:image_1}>"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/project/015_13c4d27a91_how-can-i-run-uv-in-kestra-without-installing-it-o.md", "metadata": {"id": "13c4d27a91", "question": "How can I run UV in Kestra without installing it on every flow execution?", "sort_order": 15}, "content": "To avoid reinstalling `uv` on each flow run, you can create a custom Docker image based on the official Kestra image with `uv` pre-installed. Here's how:\n\n1. **Create a Dockerfile (e.g., Dockerfile) with the following content:**\n\n   ```dockerfile\n   # Use the official Kestra image as a base\n   FROM kestra/kestra\n   \n   # Install uv\n   RUN pip install uv\n   ```\n\n2. **Update your `docker-compose.yml` to build this custom image instead of pulling the default one:**\n\n   ```yaml\n   services:\n     kestra:\n       build:\n         context: .\n         dockerfile: Dockerfile\n       image: custom-kestra\n   ```\n\nThis approach ensures that `uv` is available in the container at runtime without requiring installation during each flow execution."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/project/016_7ece5b3182_is-it-possible-to-create-external-tables-in-bigque.md", "metadata": {"id": "7ece5b3182", "question": "Is it possible to create external tables in BigQuery using URLs, such as those from the NY Taxi data website?", "sort_order": 16}, "content": "Not really, only Bigtable, Cloud Storage, and Google Drive are supported data stores."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/project/017_c0c68e1ee8_is-it-ok-to-use-ny_taxi-data-for-the-project.md", "metadata": {"id": "c0c68e1ee8", "question": "Is it ok to use NY_Taxi data for the project?", "sort_order": 17}, "content": "No."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/project/018_df38a5d809_how-to-use-dbt-core-with-athena.md", "metadata": {"id": "df38a5d809", "question": "How to use dbt-core with Athena?", "sort_order": 18}, "content": "If you don’t have access to dbt Cloud, which is natively supported by AWS, you can use the community-built [dbt-Athena Adapter](https://dbt-athena.github.io/) for dbt-Core. Here are some references:\n\n- [AWS Blog](https://aws.amazon.com/blogs/big-data/from-data-lakes-to-insights-dbt-adapter-for-amazon-athena-now-supported-in-dbt-cloud/)\n- [YouTube Tutorial](https://youtu.be/JEizJAaaBkg?si=niTYdWoeiyC_w3h7)\n- [dbt Guides: Athena](https://docs.getdbt.com/guides/athena?step=1)\n- [dbt Athena Setup Documentation](https://docs.getdbt.com/docs/core/connect-data-platform/athena-setup)\n\n### Key Features:\n\n- Enables dbt to work with AWS Athena using dbt Core\n- Allows data transformation using `CREATE TABLE AS` or `CREATE VIEW` SQL queries\n\n### Not Yet Supported Features:\n\n- Python models\n- Persisting documentation for views\n\nThis adapter can be a valuable resource for those who need to work with Athena using dbt Core."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/workshop-1-dlthub/001_70072fcf7a_solving-dbt-athena-library-conflicts.md", "metadata": {"id": "70072fcf7a", "question": "Solving dbt-Athena library conflicts", "sort_order": 1}, "content": "When working on a dbt-Athena project, do not install `dbt-athena-adapter`. Instead, always use the `dbt-athena-community` package, ensuring it matches the version of `dbt-core` to avoid compatibility conflicts.\n\n### Best Practice\n\n- **Always pin the versions of `dbt-core` and `dbt-athena-community` to the same version.**\n  \n  ```\n  Example: dbt-core~=1.9.3 dbt-athena-community~=1.9.3\n  ```\n\n### Why?\n\n- `dbt-athena-adapter` is outdated and no longer maintained.\n- `dbt-athena-community` is the actively maintained package and is compatible with the latest versions of `dbt-core`.\n\n### Steps to Avoid Conflicts\n\n1. **Check the compatibility matrix** in the [dbt-athena-community](https://github.com/dbt-labs/dbt-adapters/tree/main/dbt-athena-community) GitHub repository.\n2. **Update `requirements.txt`** to use the latest compatible versions of `dbt-core` and `dbt-athena-community`.\n3. **Avoid mixing** `dbt-athena-adapter` with `dbt-athena-community` in the same environment.\n\nBy following this practice, you can avoid the conflicts we faced previously and ensure a smooth development experience."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/workshop-1-dlthub/002_d7abe28e77_which-set-up-should-i-use-for-my-dlt-homework.md", "metadata": {"id": "d7abe28e77", "question": "Which set-up should I use for my dlt homework?", "sort_order": 2}, "content": "Technically you can use any code editor or Jupyter Notebook, as long as you can run dbt and answer the homework questions. A lot of code is provided by the instructor on the homework page to give you a head start in the right direction: [dlt Homework Instructions](https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/cohorts/2025/workshops/dlt/dlt_homework.md).\n\nThe most practical way is to use the provided Colabs Jupyter notebook called ‘dlt - Homework.ipynb’ which you can find here: [Colab Notebook](https://colab.research.google.com/drive/1plqdl33K_HkVx0E0nGJrrkEUssStQsW7#scrollTo=BtsSxtFfXQs3) since all of the provided code is applicable in the Colabs set-up."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/workshop-1-dlthub/003_122d2b0aed_how-do-i-install-the-necessary-dependencies-to-run.md", "metadata": {"id": "122d2b0aed", "question": "How do I install the necessary dependencies to run the code?", "sort_order": 3}, "content": "To run the provided code, ensure that the `dlt[duckdb]` package is installed. You can do this by executing the following installation command in a Jupyter notebook:\n\n```bash\n!pip install dlt[duckdb]\n```\n\nIf you’re installing it locally, make sure to also have `duckdb` installed before the `duckdb` package is loaded:\n\n```zsh\npip install \"dlt[duckdb]\"\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/workshop-1-dlthub/004_ee564fdf82_other-packages-needed-but-not-listed.md", "metadata": {"id": "ee564fdf82", "question": "Other packages needed but not listed", "sort_order": 4}, "content": "If you are running Jupyter Notebook on a fresh new Codespace or in a local machine with a new virtual environment, you will need these packages to run the starter Jupyter Notebook offered by the teacher. Execute this command to install all the necessary dependencies:\n\n```bash\npip install duckdb pandas numpy pyarrow\n```\n\nOr save it into a `requirements.txt` file:\n\n```\ndlt[duckdb]\nduckdb\npandas\nnumpy\npyarrow  # Optional, needed for Parquet support\n```\n\nThen run:\n\n```bash\npip install -r requirements.txt\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/workshop-1-dlthub/005_60b8576833_how-can-i-use-duckdb-in-memory-database-with-dlt.md", "metadata": {"id": "60b8576833", "question": "How can I use DuckDB In-Memory database with dlt?", "sort_order": 5}, "content": "Alternatively, you can switch to in-file storage with:"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/workshop-1-dlthub/006_5b5d82a924_homework-dlt-exercise-3-merge-a-generator-concerns.md", "metadata": {"id": "5b5d82a924", "images": [{"description": "image #1", "id": "image_1", "path": "images/data-engineering-zoomcamp/image_4c7b9e6b.png"}, {"description": "image #2", "id": "image_2", "path": "images/data-engineering-zoomcamp/image_b4d5ed3c.png"}, {"description": "image #3", "id": "image_3", "path": "images/data-engineering-zoomcamp/image_2ba9606d.png"}], "question": "Homework: dlt Exercise 3 - Merge a generator concerns", "sort_order": 6}, "content": "After loading, you should have a total of 8 records, and ID 3 should have age 33.\n\n**Question:** Calculate the sum of ages of all the people loaded as described above.\n\n- The sum of all eight records' respective ages is too big to be in the choices.\n- You need to first filter out the people whose occupation is equal to `None` in order to get an answer that is close to or present in the given choices. \n\n---\n\n### Issue:\n\nI'm having an issue with the DLT workshop notebook, specifically in the 'Load to Parquet file' section. No matter what I change the file path to, it's still saving the DLT files directly to my C drive.\n\n### Solution:\n\nUse a raw string and keep the `file:///` at the start of your file path.\n\n```python\n# Set the bucket_url. We can also use a local folder\nos.environ['DESTINATION__FILESYSTEM__BUCKET_URL'] = r'file:///content/.dlt/my_folder'\n\nurl = \"https://storage.googleapis.com/dtc_zoomcamp_api/yellow_tripdata_2009-06.jsonl\"\n\n# Define your pipeline\npipeline = dlt.pipeline(\n    pipeline_name='my_pipeline',\n    destination='filesystem',\n    dataset_name='mydata'\n)\n\n# Run the pipeline with the generator we created earlier.\nload_info = pipeline.run(stream_download_jsonl(url), table_name=\"users\", loader_file_format=\"parquet\")\n\nprint(load_info)\n\n# Get a list of all Parquet files in the specified folder\nparquet_files = glob.glob('/content/.dlt/my_folder/mydata/users/*.parquet')\n\n# Show Parquet files\nfor file in parquet_files:\n    print(file)\n```\n\n<{IMAGE:image_1}>\n\n<{IMAGE:image_2}>\n\n<{IMAGE:image_3}>"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/workshop-1-dlthub/007_27fa940238_problem-with-importing-the-dlt-or-dltsources-modul.md", "metadata": {"id": "27fa940238", "question": "Problem with importing the dlt or dlt.sources module", "sort_order": 7}, "content": "Make sure you don’t have a `dlt.py` file saved in the same directory as your working file."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/workshop-1-dlthub/008_4ecc97beb5_how-to-set-credentials-in-google-colab-notebook-to.md", "metadata": {"id": "4ecc97beb5", "question": "How to set credentials in Google Colab notebook to connect to BigQuery", "sort_order": 8}, "content": "In the secrets sidebar, create a secret `BIGQUERY_CREDENTIALS` with the value being your Google Cloud service account key. Then load it with:\n\n```python\nimport os\nfrom google.colab import userdata\n\nos.environ[\"DESTINATION__BIGQUERY__CREDENTIALS\"] = userdata.get('BIGQUERY_CREDENTIALS')\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/workshop-1-dlthub/009_d2add610e3_how-do-i-set-up-credentials-to-run-dlt-in-my-envir.md", "metadata": {"id": "d2add610e3", "question": "How do I set up credentials to run dlt in my environment (not Google Colab)?", "sort_order": 9}, "content": "You can set up credentials for `dlt` in several ways. Here are the two most common methods:\n\n### Environment Variables (Easiest)\n\nSet credentials via environment variables. For example, to configure Google Cloud credentials. This method avoids hardcoding secrets in your code and works seamlessly with most environments.\n\n### Configuration Files (Recommended for Local Use)\n\n- Use `.dlt/secrets.toml` for sensitive credentials and `.dlt/config.toml` for non-sensitive configurations.\n- Example for Google Cloud in `secrets.toml`:\n\n```toml\n[google_cloud]\nservice_account_key = \"YOUR_SERVICE_ACCOUNT_KEY\"\n```\n\n- Place these files in the `.dlt` folder of your project.\n\n### Additional Notes:\n\n- Never commit `secrets.toml` to version control (add it to `.gitignore`).\n- Credentials can also be loaded via vaults, AWS Parameter Store, or custom setups.\n\nFor additional methods and detailed information, refer to the [official dlt documentation](https://dlthub.com/docs/general-usage/credentials/)"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/workshop-1-dlthub/010_1075e9e2b6_make-dlt-comply-with-the-xdg-base-dir-specificatio.md", "metadata": {"id": "1075e9e2b6", "question": "Make DLT comply with the XDG Base Dir Specification", "sort_order": 10}, "content": "You can set the environment variable in your shell init script:\n\nFor Bash or ZSH:\n\n```bash\nexport DLT_DATA_DIR=$XDG_DATA_HOME/dlt\n```\n\nFor Fish (in `config.fish`):\n\n```bash\nset -x DLT_DATA_DIR \"$XDG_DATA_HOME/dlt\"\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/workshop-1-dlthub/011_f48b484cd4_embedding-dlt-into-apache-airflow.md", "metadata": {"id": "f48b484cd4", "question": "Embedding dlt into Apache Airflow", "sort_order": 11}, "content": "To integrate a `dlt` pipeline into Apache Airflow, follow this example:\n\n```python\nfrom airflow import DAG\nfrom airflow.operators.python import PythonOperator\nfrom datetime import datetime, timedelta\nimport dlt\nfrom my_dlt_pipeline import load_data  # Import your dlt pipeline function\n\ndefault_args = {\n    \"owner\": \"airflow\",\n    \"depends_on_past\": False,\n    \"start_date\": datetime(2024, 2, 16),\n    \"retries\": 1,\n    \"retry_delay\": timedelta(minutes=5),\n}\n\ndef run_dlt_pipeline():\n    pipeline = dlt.pipeline(\n        pipeline_name=\"my_pipeline\",\n        destination=\"duckdb\",  # Change this based on your database\n        dataset_name=\"my_dataset\"\n    )\n    info = pipeline.run(load_data())\n    print(info)  # Logs for debugging\n\nwith DAG(\n    \"dlt_airflow_pipeline\",\n    default_args=default_args,\n    schedule_interval=\"@daily\",\n    catchup=False,\n) as dag:\n    run_dlt_task = PythonOperator(\n        task_id=\"run_dlt_pipeline\",\n        python_callable=run_dlt_pipeline,\n    )\n    run_dlt_task\n```\n\nEnsure to replace `\"duckdb\"` with your actual database name and adjust the `load_data` function according to your specific `dlt` pipeline."}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/workshop-1-dlthub/012_eaae17dfea_embedding-dlt-into-kestra.md", "metadata": {"id": "eaae17dfea", "question": "Embedding dlt into Kestra", "sort_order": 12}, "content": "```yaml\nid: dlt_ingestion\n\nnamespace: my.dlt\n\ndescription: \"Run dlt pipeline with Kestra\"\n\ntasks:\n\n- id: run_dlt\n\n  type: io.kestra.plugin.scripts.python.Commands\n\n  commands:\n\n  - |\n\n    import dlt\n\n    from my_dlt_pipeline import load_data  # Import your dlt function\n\n    pipeline = dlt.pipeline(\n\n      pipeline_name=\"kestra_pipeline\",\n\n      destination=\"duckdb\",\n\n      dataset_name=\"kestra_dataset\"\n\n    )\n\n    info = pipeline.run(load_data())\n\n    print(info)\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/data-engineering-zoomcamp/workshop-1-dlthub/013_7be69bdcb4_loading-dlt-exports-from-gcs-filesystems.md", "metadata": {"id": "7be69bdcb4", "question": "Loading Dlt Exports from GCS Filesystems", "sort_order": 13}, "content": "When using the filesystem destination, you may have issues reading the files exported because DLT will by default compress the files. If you are using `loader_file_format=\"parquet\"` then BigQuery should cope with this compression OK. If you want to use JSONL or CSV format, however, you may need to disable file compression to avoid issues with reading the files directly in BigQuery. To do this, set the following config:\n\n```bash\n[normalize.data_writer]\ndisable_compression = true\n```\n\nThere is further information at [DLTHub Docs](https://dlthub.com/docs/dlt-ecosystem/destinations/filesystem#file-compression).\n\n**Warning:**\n```yaml\nTest 'test.taxi_rides_ny.relationships_stg_yellow_tripdata_dropoff_locationid__locationid__ref_taxi_zone_lookup_csv_.085c4830e7' (models/staging/schema.yml) depends on a node named 'taxi_zone_lookup.csv' in package '' which was not found\n```\n\n**Solution:** This warning indicates that dbt is trying to reference a model or source named `taxi_zone_lookup.csv`, but it cannot find it. We might have a typo in our `ref()` function.\n\n**Tests:**\n\n- **Name:** relationships_stg_yellow_tripdata_dropoff_locationid\n  \n  **Description:** Ensure `dropoff_location_id` exists in `taxi_zone_lookup.csv`\n\n  **Relationships:**\n  - **To:** `ref('taxi_zone_lookup.csv')`  # ❌ Wrong reference\n\n  - **Field:** `locationid`\n\n  - **To:** `ref('taxi_zone_lookup')`  # ✅ Correct reference\n\n**Pandas and Spark Version Mismatch:**\n\nWhen running `df_spark = spark.createDataFrame(df_pandas)`, an error indicating a version mismatch between Pandas and Spark was encountered. To resolve this, either:\n\n1. Downgrade Pandas to a version below 2.\n2. Upgrade Spark to version 3.5.5.\n\nI chose to upgrade Spark to 3.5.5, and it worked.\n\n**Avoiding Backpressure in Flink:**\n\n**What’s Backpressure?**\n\n- It occurs when Flink processes data slower than Kafka produces it.\n- This leads to increased memory usage and can slow down or crash the job.\n\n**How to Fix It?**\n\n- Adjust Kafka’s consumer parallelism to match the producer rate.\n- Increase partitions in Kafka to allow more parallel processing.\n- Monitor Flink metrics to detect backpressure.\n\n```python\nenv.set_parallelism(4)  # Adjust parallelism to avoid bottlenecks\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/llm-zoomcamp/general/001_74eb249bbf_i-just-discovered-the-course-can-i-still-join.md", "metadata": {"id": "74eb249bbf", "question": "I just discovered the course. Can I still join?", "sort_order": 1}, "content": "Yes, but if you want to receive a certificate, you need to submit your project while we’re still accepting submissions."}
{"source": "DataTalksClub/faq", "filename": "_questions/llm-zoomcamp/general/002_977bf7786c_course-i-have-registered-for-the-llm-zoomcamp-when.md", "metadata": {"id": "977bf7786c", "question": "Course: I have registered for the LLM Zoomcamp. When can I expect to receive the confirmation email?", "sort_order": 2}, "content": "You don't need it. You're accepted. You can also just start learning and submitting homework (while the form is open) without registering. It is not checked against any registered list. Registration is just to gauge interest before the start date."}
{"source": "DataTalksClub/faq", "filename": "_questions/llm-zoomcamp/general/003_489dd1c9d9_what-is-the-videozoom-link-to-the-stream-for-the-o.md", "metadata": {"id": "489dd1c9d9", "question": "What is the video/zoom link to the stream for the “Office Hours” or live/workshop sessions?", "sort_order": 3}, "content": "The zoom link is only published to instructors/presenters/TAs.\n\nStudents participate via YouTube Live and submit questions to Slido (link is pinned in the chat when live). The video URL should be posted in the [announcements channel on Telegram & Slack](https://t.me/dezoomcamp) before it begins. You can also watch live on the DataTalksClub [YouTube Channel](https://www.youtube.com/c/DataTalksClub).\n\nDon’t post questions in chat as they may be missed if the room is very active."}
{"source": "DataTalksClub/faq", "filename": "_questions/llm-zoomcamp/general/004_c6c2888275_cloud-alternatives-with-gpu.md", "metadata": {"id": "c6c2888275", "question": "Cloud alternatives with GPU", "sort_order": 4}, "content": "Check the quota and reset cycle carefully. Is the free hours limit per month or per week? Usually, if you change the configuration, the free hours quota might also be adjusted, or it might be billed separately.\n\nPotential options include:\n\n- Google Colab\n- Kaggle\n- Databricks (possibly)\n\nConsider using GPTs to discover more options. Be aware that some platforms might have restrictions on what you can and cannot install, so ensure to read what is included in the free vs paid tier."}
{"source": "DataTalksClub/faq", "filename": "_questions/llm-zoomcamp/general/005_c2903069a0_leaderboard-i-am-not-on-the-leaderboard-how-do-i-k.md", "metadata": {"id": "c2903069a0", "images": [{"description": "image #1", "id": "image_1", "path": "images/llm-zoomcamp/image_7ba0bc8e.png"}], "question": "Leaderboard: I am not on the leaderboard / how do I know which one I am on the leaderboard?", "sort_order": 5}, "content": "When you set up your account, you are automatically assigned a random name, such as “Lucid Elbakyan.” Click on the \"Jump to your record on the leaderboard\" link to find your entry.\n\nIf you want to see what your Display name is, click on the \"Edit Course Profile\" button.\n\n<{IMAGE:image_1}>\n\n- **First field:** This is your nickname/displayed name. You can change it if you want to be known by your Slack username, GitHub username, or any other nickname of your choice. This is useful if you want to remain anonymous.\n- **Second field:** Change this to your official name as in your identification documents—passport, national ID card, driver's license, etc. This is mandatory if you do not want \"Lucid Elbakyan\" on your certificate. This name will appear on your Certificate!"}
{"source": "DataTalksClub/faq", "filename": "_questions/llm-zoomcamp/general/006_69d122f12e_certificate-can-i-follow-the-course-in-a-self-pace.md", "metadata": {"id": "69d122f12e", "question": "Certificate: Can I follow the course in a self-paced mode and get a certificate?", "sort_order": 6}, "content": "No, you can only get a certificate if you finish the course with a \"live\" cohort.\n\nWe don't award certificates for the self-paced mode. The reason is you need to peer-review 3 capstone(s) after submitting your project.\n\nYou can only peer-review projects at the time the course is running; after the form is closed and the peer-review list is compiled."}
{"source": "DataTalksClub/faq", "filename": "_questions/llm-zoomcamp/general/007_9f689c185f_i-missed-the-first-homework-can-i-still-get-a-cert.md", "metadata": {"id": "9f689c185f", "question": "I missed the first homework - can I still get a certificate?", "sort_order": 7}, "content": "Yes, you need to pass the Capstone project to get the certificate. Homework is not mandatory, though it is recommended for reinforcing concepts, and the points awarded count towards your rank on the leaderboard."}
{"source": "DataTalksClub/faq", "filename": "_questions/llm-zoomcamp/general/008_96286b4be4_homework-why-does-the-content-keep-changing.md", "metadata": {"id": "96286b4be4", "question": "Homework: Why does the content keep changing?", "sort_order": 8}, "content": "This course is being offered for the first time, and things will keep changing until a given module is ready, at which point it shall be announced. Working on the material or homework in advance will be at your own risk, as the final version could be different."}
{"source": "DataTalksClub/faq", "filename": "_questions/llm-zoomcamp/general/009_bd31146b0e_when-will-the-course-be-offered-next.md", "metadata": {"id": "bd31146b0e", "question": "When will the course be offered next?", "sort_order": 9}, "content": "Summer 2025."}
{"source": "DataTalksClub/faq", "filename": "_questions/llm-zoomcamp/general/010_31456f4b5f_are-there-any-lecturesvideos-where-are-they.md", "metadata": {"id": "31456f4b5f", "question": "Are there any lectures/videos? Where are they?", "sort_order": 10}, "content": "Please check the bookmarks and pinned links, especially DataTalks.Club’s YouTube account."}
{"source": "DataTalksClub/faq", "filename": "_questions/llm-zoomcamp/general/011_f3dd94f323_wsl2-responseerror-model-requires-more-system-memo.md", "metadata": {"id": "f3dd94f323", "question": "WSL2: ResponseError: model requires more system memory (X.X GiB) than is available (Y.Y GiB). My system has more than X.X GiB.", "sort_order": 11}, "content": "Your WSL2 is set to use Y.Y GiB, not all your computer memory. To allocate more RAM, follow these steps:\n\n1. Create a `.wslconfig` file under your Windows user profile directory: `C:\\Users\\YourUsername\\.wslconfig`.\n\n2. Include the desired RAM allocation in the file:\n\n   ```ini\n   [wsl2]\n   memory=8GB\n   ```\n\n3. Restart WSL using the command:\n\n   ```bash\n   wsl --shutdown\n   ```\n\n4. Run the `free` command in WSL to verify the changes.\n\nFor more details, read [this article](https://www.aleksandrhovhannisyan.com/blog/limiting-memory-usage-in-wsl-2/)."}
{"source": "DataTalksClub/faq", "filename": "_questions/llm-zoomcamp/general/012_c774542f32_server-error-500-when-logging-in-to-course-homewor.md", "metadata": {"id": "c774542f32", "question": "Server Error (500) When logging in to course homework using GitHub", "sort_order": 12}, "content": "Additional error text seen:\n\n```\nThird-Party Login Failure\n\nAn error occurred while attempting to login via your third-party account.\n```\n\nThe current solution is to use Google or Slack to log in and submit homework answers, as the root cause analysis for the GitHub issue is sporadic and doesn’t impact all users."}
{"source": "DataTalksClub/faq", "filename": "_questions/llm-zoomcamp/general/013_de32098d49_what-other-alternatives-to-elasticsearch-are-there.md", "metadata": {"id": "de32098d49", "question": "What other alternatives to ElasticSearch are there?", "sort_order": 13}, "content": "You could use some of these free alternatives to Elasticsearch:\n\n- [Milvus](https://milvus.io/): An open-source library with similar functionalities to Elasticsearch.\n\n- [OpenSearch](https://opensearch.org/): Another free open-source library that provides the same functionalities as Elasticsearch.\n\n### Additional Considerations:\n\nIf you start using `multi-qa-distilbert-cos-v1` from [huggingface.co](https://huggingface.co/sentence-transformers/multi-qa-distilbert-cos-v1) and create embeddings to index them, consider the following:\n\n- If the model is updated by the author, the indexed embeddings may become incompatible, requiring re-indexing.\n\n- To prevent this, save the model locally. This ensures that your code continues to work even if the cloud model changes."}
{"source": "DataTalksClub/faq", "filename": "_questions/llm-zoomcamp/module-1/001_b7cdde6b25_why-are-we-not-using-langchain-in-the-course.md", "metadata": {"id": "b7cdde6b25", "question": "Why are we not using Langchain in the course?", "sort_order": 1}, "content": "Langchain is a framework for building LLM-powered apps. We're not using it to learn the basics; think of it like learning HTML, CSS, and JavaScript before learning React or Angular."}
{"source": "DataTalksClub/faq", "filename": "_questions/llm-zoomcamp/module-1/002_f5df151c59_openai-error-when-running-openai-chatcompletionscr.md", "metadata": {"id": "f5df151c59", "question": "OpenAI: Error when running OpenAI chat.completions.create command", "sort_order": 2}, "content": "You may receive the following error when running the OpenAI `chat.completions.create` command due to insufficient credits in your OpenAI account:\n\n```\nOpenAI API Error: Insufficient credits\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/llm-zoomcamp/module-1/003_152af39a53_openai-error-ratelimiterror-error-code-429.md", "metadata": {"id": "152af39a53", "images": [{"description": "image #1", "id": "image_1", "path": "images/llm-zoomcamp/image_d8b5644f.png"}], "question": "OpenAI: Error: RateLimitError: Error code: 429 -", "sort_order": 3}, "content": "```json\nRateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: [https://platform.openai.com/docs/guides/error-codes/api-errors.](https://platform.openai.com/docs/guides/error-codes/api-errors.)', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}\n```\n\nThe above errors are related to your OpenAI API account’s quota. There is no free usage of OpenAI’s API, so you will need to add funds using a credit card (see pay-as-you-go in the OpenAI settings at [platform.openai.com](http://platform.openai.com)). Once added, re-run your Python command and you should receive a successful return code.\n\nSteps to resolve:\n\n1. Add credits to your account [here](https://platform.openai.com/settings/organization/billing/overview) (min $5).\n2. In `chat.completions.create(model='gpt-4o', …)` specify one of the models available to you:\n\n   <{IMAGE:image_1}>\n\n3. You might need to recreate an API key after adding credits to your account and update it locally."}
{"source": "DataTalksClub/faq", "filename": "_questions/llm-zoomcamp/module-1/004_35e1ef85a1_openai-error-cannot-import-name-openai-from-openai.md", "metadata": {"id": "35e1ef85a1", "question": "OpenAI: Error: 'Cannot import name OpenAI from openai'; How to fix?", "sort_order": 4}, "content": "Update openai version from 0.27.0 to any 1.x version."}
{"source": "DataTalksClub/faq", "filename": "_questions/llm-zoomcamp/module-1/005_554d0eb78b_openai-how-much-will-i-have-to-spend-to-use-the-op.md", "metadata": {"id": "554d0eb78b", "question": "OpenAI: How much will I have to spend to use the Open AI API?", "sort_order": 5}, "content": "Using the OpenAI API does not cost much. You can recharge starting from $5. For initial usage, it might cost as little as 5 cents."}
{"source": "DataTalksClub/faq", "filename": "_questions/llm-zoomcamp/module-1/006_85384a18e5_openai-do-i-have-to-subscribe-and-pay-for-open-ai.md", "metadata": {"id": "85384a18e5", "question": "OpenAI: Do I have to subscribe and pay for Open AI API for this course?", "sort_order": 6}, "content": "No, you don't have to pay for this service in order to complete the course homeworks. You could use some of the free alternatives listed in the course GitHub.\n\n[llm-zoomcamp/01-intro/open-ai-alternatives.md at main · DataTalksClub/llm-zoomcamp (github.com)](https://github.com/DataTalksClub/llm-zoomcamp/blob/main/01-intro/open-ai-alternatives.md)"}
{"source": "DataTalksClub/faq", "filename": "_questions/llm-zoomcamp/module-1/007_66ae44829d_elasticsearch-error-badrequesterror-badrequesterro.md", "metadata": {"id": "66ae44829d", "question": "ElasticSearch: ERROR: BadRequestError: BadRequestError(400, 'media_type_header_exception', 'Invalid media-type value on headers [Content-Type, Accept]', Accept version must be either version 8 or 7, but found 9.", "sort_order": 7}, "content": "**Reason:** ElasticSearch client and server are on different versions.\n\n**Solution:**\n\n- Upgrade ElasticSearch on Docker to version 9:\n\n  ```bash\n  docker run -it \\\n  --rm \\\n  --name elasticsearch \\\n  -p 9200:9200 \\\n  -p 9300:9300 \\\n  -e \"discovery.type=single-node\" \\\n  -e \"xpack.security.enabled=false\" \\\n  elasticsearch:9.0.1\n  ```\n\n- If upgrading to version 9 doesn’t work, check the client version (Python module) using:\n  \n  ```bash\n  pip show elasticsearch\n  ```\n  \n- Then install that specific version of ElasticSearch on Docker. Check if it worked using:\n  \n  ```bash\n  curl http://localhost:9200/\n  ```\n  \n**Example output of `pip show elasticsearch`:**\n\n```\nName: elasticsearch\nVersion: 9.0.2\nSummary: Python client for Elasticsearch\nHome-page: [GitHub](https://github.com/elastic/elasticsearch-py)\nAuthor:\nAuthor-email: Elastic Client Library Maintainers <client-libs@elastic.co>\nLicense-Expression: Apache-2.0\nLocation: /home/codespace/.python/current/lib/python3.12/site-packages\nRequires: elastic-transport, python-dateutil, typing-extensions\nRequired-by:\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/llm-zoomcamp/module-1/008_a5a76b7c76_fix-badrequesterror-badrequesterror400-media_type_.md", "metadata": {"id": "a5a76b7c76", "question": "Fix BadRequestError: BadRequestError(400, 'media_type_header_exception', 'Invalid media-type value on headers [Content-Type, Accept]', Accept version must be either version 8 or 7, but found 9. Accept=application/vnd.elasticsearch+json; compatible-with=9)", "sort_order": 8}, "content": "When trying to connect to the Elasticsearch server/node version 8.17.6 running within a Docker container with the Python client Elasticsearch version 9.x or more, you may encounter the following `BadRequestError`:\n\n```plaintext\nBadRequestError(400, 'media_type_header_exception', 'Invalid media-type value on headers [Content-Type, Accept]', Accept version must be either version 8 or 7, but found 9. Accept=application/vnd.elasticsearch+json; compatible-with=9)\n```\n\nThis issue arises because `pip install elasticsearch` installs Elasticsearch 9.x Python client, which is incompatible with Elasticsearch 8.17.6. To resolve this issue, use the following command to install a compatible Elasticsearch client version:\n\n```bash\npip install \"elasticsearch>=8,<9\"\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/llm-zoomcamp/module-1/009_2db8d0cd4d_elasticsearch-error-elasticsearch-exited-unexpecte.md", "metadata": {"id": "2db8d0cd4d", "question": "ElasticSearch: ERROR: Elasticsearch exited unexpectedly", "sort_order": 9}, "content": "If you encounter the error \"Elasticsearch exited unexpectedly,\" it's likely due to insufficient RAM allocated to Elasticsearch.\n\n### Solution 1: Specify RAM Size\n\nSpecify the RAM size in the configuration:\n\n```bash\ndocker run -it \\\n  --rm \\\n  --name elasticsearch \\\n  -m 4GB \\\n  -p 9200:9200 \\\n  -p 9300:9300 \\\n  -e \"discovery.type=single-node\" \\\n  -e \"xpack.security.enabled=false\" \\\n  docker.elastic.co/elasticsearch/elasticsearch:8.4.3\n```\n\nYou can also try using `-m 2GB`.\n\n### Solution 2: Set Memory Lock to False\n\nAnother possible solution is to set the `memory_lock` to false:\n\n```bash\ndocker run -it \\\n  --rm \\\n  --name elasticsearch \\\n  -p 9200:9200 \\\n  -p 9300:9300 \\\n  -e \"discovery.type=single-node\" \\\n  -e \"xpack.security.enabled=false\" \\\n  -e \"ES_JAVA_OPTS=-Xms512m -Xmx512m\" \\\n  -e \"bootstrap.memory_lock=false\" \\\n  docker.elastic.co/elasticsearch/elasticsearch:8.4.3\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/llm-zoomcamp/module-1/010_eb4378abaa_elasticsearch-error-elasticsearchindex-got-an-unex.md", "metadata": {"id": "eb4378abaa", "question": "ElasticSearch: ERROR: Elasticsearch.index() got an unexpected keyword argument 'document'", "sort_order": 10}, "content": "Instead of `document` as used in the course video, use `doc`."}
{"source": "DataTalksClub/faq", "filename": "_questions/llm-zoomcamp/module-1/011_02df9d0302_docker-how-do-i-store-data-persistently-in-elastic.md", "metadata": {"id": "02df9d0302", "question": "Docker: How do I store data persistently in Elasticsearch?", "sort_order": 11}, "content": "When you stop the container, the data you previously added to Elasticsearch will be gone. To avoid this, add volume mapping:\n\n1. Create a Docker volume:\n\n   ```bash\n   docker volume create elasticsearch_data\n   ```\n\n2. Run the Elasticsearch container with volume mapping:\n\n   ```bash\n   docker run -it \\\n   --rm \\\n   --name elasticsearch \\\n   -p 9200:9200 \\\n   -p 9300:9300 \\\n   -v elasticsearch_data:/usr/share/elasticsearch/data \\\n   -e \"discovery.type=single-node\" \\\n   -e \"xpack.security.enabled=false\" \\\n   docker.elastic.co/elasticsearch/elasticsearch:8.4.3\n   ```"}
{"source": "DataTalksClub/faq", "filename": "_questions/llm-zoomcamp/module-1/012_8d23fe792f_authentication-safe-and-easy-way-to-store-and-load.md", "metadata": {"id": "8d23fe792f", "question": "Authentication: Safe and easy way to store and load API keys", "sort_order": 12}, "content": "You can store your different API keys in a YAML file that you will add to your `.gitignore` file. Be careful to never push or share this file.\n\nFor example, create a new file named `api_keys.yml` in your repository.\n\nThen, add it to your `.gitignore` file:\n\n```\n#api_keys\n\napi_keys.yml\n```\n\nFill your `api_keys.yml` file:\n\n```\nOPENAI_API_KEY: \"sk[...]\"\nGROQ_API_KEY: \"gqk_[...]\"\n```\n\nSave your file.\n\nYou will need the pyyaml library to load your YAML file, so run this command in your terminal:\n\n```bash\npip install pyyaml\n```\n\nNow, open your Jupyter notebook.\n\nYou can load your YAML file and the associated keys with this code:\n\n```python\nimport yaml\n\n# Open the file\nwith open('api_keys.yml', 'r') as file:\n    # Load the data from the file\n    data = yaml.safe_load(file)\n\n# Get the API key (Groq example here)\ngroq_api_key = data['GROQ_API_KEY']\n```\n\nNow, you can easily replace the `api_key` value directly with the loaded values without loading your environment variables."}
{"source": "DataTalksClub/faq", "filename": "_questions/llm-zoomcamp/module-1/013_233dabe430_how-to-store-and-load-api-keys-using-env-file.md", "metadata": {"id": "233dabe430", "question": "How to store and load API keys using .env file", "sort_order": 13}, "content": "Store the API key in a `.env` file, then use the following steps to load it:\n\n1. Import the necessary modules:\n   \n   ```python\n   import os\n   from dotenv import load_dotenv\n   ```\n\n2. Load the `.env` file:\n   \n   ```python\n   load_dotenv(os.path.abspath(\"<path-to-.env>\"))\n   ```\n\n3. Retrieve the API key:\n   \n   ```python\n   os.getenv(\"API_KEY_abc\")\n   ```\n\n- Ensure to add the `.env` file to your `.gitignore` to prevent it from being checked into version control."}
{"source": "DataTalksClub/faq", "filename": "_questions/llm-zoomcamp/module-1/014_86d99bbf21_authentication-why-is-my-openai_api_key-not-found.md", "metadata": {"id": "86d99bbf21", "question": "Authentication: Why is my OPENAI_API_KEY not found in the Jupyter notebook?", "sort_order": 14}, "content": "There are two options to resolve this issue:\n\n**Option 1: Using direnv**\n\n1. Create a `.envrc` file and add your API key.\n2. Run `direnv allow` in the terminal.\n\nIf you encounter the error:\n\n```python\nOpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n```\n\n- Install `dotenv` by running:\n\n    ```bash\n    pip install python-dotenv\n    ```\n\n- Add the following to a cell in the notebook:\n\n    ```python\n    from dotenv import load_dotenv\n    \n    load_dotenv('.envrc')\n    ```\n\n**Option 2: Using Codespaces Secrets**\n\n1. Log in to your GitHub account and navigate to **Settings > Codespaces**.\n2. In the Secrets section, create a secret like `OPENAI_API_KEY` and select the repositories for which the secret should be available.\n3. Once set up, the key will be available in your Codespaces session."}
{"source": "DataTalksClub/faq", "filename": "_questions/llm-zoomcamp/module-1/015_fe8fed31e6_opensource-i-am-using-groq-and-it-doesnt-provide-a.md", "metadata": {"id": "fe8fed31e6", "question": "OpenSource: I am using Groq, and it doesn't provide a tokenizer library based on my research. How can we estimate the number of OpenAI tokens asked in homework question 6?", "sort_order": 15}, "content": "The question asks for the number of tokens in the GPT-4o model. `tiktoken` is a Python library that can be used to get the number of tokens. You don't need an OpenAI API key to get the number of tokens. You can use the code provided in the question to get the number of tokens."}
{"source": "DataTalksClub/faq", "filename": "_questions/llm-zoomcamp/module-1/016_ee43413718_opensource-can-i-use-groq-instead-of-openai.md", "metadata": {"id": "ee43413718", "question": "OpenSource: Can I use Groq instead of OpenAI?", "sort_order": 16}, "content": "You can use any LLM platform for your experiments and your project. The homework is designed so that you don’t need access to any paid services and can complete it locally. However, you will need to adjust the code for that platform. Refer to their documentation pages."}
{"source": "DataTalksClub/faq", "filename": "_questions/llm-zoomcamp/module-1/017_3fd5c51182_opensource-can-i-use-open-source-alternatives-to-o.md", "metadata": {"id": "3fd5c51182", "question": "OpenSource: Can I use open-source alternatives to OpenAI API?", "sort_order": 17}, "content": "Yes. See module 2 and the [open-ai-alternatives.md](https://github.com/DataTalksClub/llm-zoomcamp/blob/main/01-intro/open-ai-alternatives.md) in the module 1 folder."}
{"source": "DataTalksClub/faq", "filename": "_questions/llm-zoomcamp/module-1/018_83da208a64_homework-returning-empty-list-after-filtering-my-q.md", "metadata": {"id": "83da208a64", "question": "Homework: Returning Empty list after filtering my query (HW Q3)", "sort_order": 18}, "content": "This is likely to be an error when indexing the data. First, you need to add the index settings before adding the data to the indices, then you will be good to go applying your filters and query."}
{"source": "DataTalksClub/faq", "filename": "_questions/llm-zoomcamp/module-1/019_cf93377279_modulenotfounderror-on-import-docx-in-parse-faqipy.md", "metadata": {"id": "cf93377279", "question": "ModuleNotFoundError on import docx in parse-faq.ipynb", "sort_order": 19}, "content": "The correct package name for `docx` is `python-docx`, not `docx`. Make sure to install the package using:\n\n```bash\npip install python-docx\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/llm-zoomcamp/module-1/020_bf0403d21b_openai-why-does-my-token-count-differ-from-what-op.md", "metadata": {"id": "bf0403d21b", "question": "OpenAI: Why does my token count differ from what OpenAI reports?", "sort_order": 20}, "content": "When using `tiktoken.encode()` to count tokens in your prompt, you might see a difference compared to OpenAI’s API response. For instance, you might get 320 tokens, while OpenAI reports 327. This is due to internal tokens added by OpenAI’s chat formatting.\n\nHere’s what happens:\n\n- Each message in a `chat.completions.create()` call (e.g., `{role: \"user\", content: \"...\"}`) adds 4 structural tokens (role, content, separators).\n- The API adds 2 tokens globally to mark the start of assistant response generation.\n- Extra newlines, whitespace, or uncommon Unicode characters in your content may slightly increase the token count.\n\nThus, even if your visible text is 320 tokens, OpenAI may count 327 due to these internal additions."}
{"source": "DataTalksClub/faq", "filename": "_questions/llm-zoomcamp/module-1/021_1d0b969028_ollama-how-to-install-ollama.md", "metadata": {"id": "1d0b969028", "question": "Ollama: How to install Ollama?", "sort_order": 21}, "content": "First, install Ollama by visiting [https://ollama.com/download](https://ollama.com/download) and choosing your operating system:\n\n- **macOS**: Download the `.pkg` and install it.\n- **Windows**: Download the `.msi` and install it.\n- **Linux**: Run the following command in the terminal:\n\n  ```bash\n  curl -fsSL https://ollama.com/install.sh | sh\n  ```\n\nOnce installed, open a terminal and type:\n\n```bash\nollama run llama3\n```\n\nThis command will:\n\n- Download the LLaMA 3 model (~4GB).\n- Start the model locally.\n- Open a chat-like interface where you can type questions.\n\nTo test the Ollama local server, run the following command:\n\n```bash\ncurl http://localhost:11434\n```\n\nYou should receive a response similar to:\n\n```json\n{\"models\": [...]}  \n```\n\nThen, install the Python client with:\n\n```bash\npip install ollama\n```\n\nHere is a minimal Python example:\n\n```python\nimport ollama\n\nresponse = ollama.chat(\n    model='llama3',\n    messages=[{\"role\": \"user\", \"content\": your_prompt}]\n)\n\nprint(response['message']['content'])\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/llm-zoomcamp/module-1/022_50507316aa_homework-when-i-re-run-the-code-in-jupyter-noteboo.md", "metadata": {"id": "50507316aa", "question": "Homework: When I re-run the code in Jupyter notebook multiple times for homework#1, the index building code snippet fails.", "sort_order": 22}, "content": "To resolve this issue, you can delete any existing index with the same name before creating a new one.\n\n```python\n# Check if the index exists and delete it if it does\nif es_client.indices.exists(index=index_name):\n    print(f\"Deleting existing index: {index_name}\")\n    es_client.indices.delete(index=index_name)\n    print(f\"Index {index_name} deleted.\")\n```\n\nIf you encounter issues with the index getting messed up and seeing different score outputs, follow these steps:\n\n1. Go to Docker Desktop and stop the Elasticsearch container.\n2. Delete the container image and re-initiate the Elasticsearch container by following the instructions in '1.6 Searching with ElasticSearch'.\n3. Change the name of the index in your code to something other than `index_name = \"course-questions\"`."}
{"source": "DataTalksClub/faq", "filename": "_questions/llm-zoomcamp/module-1/023_c256749315_question.md", "metadata": {"id": "c256749315", "question": "Question", "sort_order": 23}, "content": "Answer"}
{"source": "DataTalksClub/faq", "filename": "_questions/llm-zoomcamp/module-2/001_ssl-error-when-connecting-to-l.md", "metadata": {"id": "6494916130", "question": "SSL Error when connecting to locally running ElasticSearch instance via SDK:", "sort_order": 1}, "content": "The issue is likely that you’re trying to use HTTPS instead of HTTP when you call local.\n\nTo remove ES authentication constraints, set `xpack.security.enabled=false` in the ES docker settings."}
{"source": "DataTalksClub/faq", "filename": "_questions/llm-zoomcamp/module-2/002_69430a79a8_what-are-embeddings.md", "metadata": {"id": "69430a79a8", "question": "What are embeddings?", "sort_order": 2}, "content": "Embeddings refer to the process of converting non-numerical data into numerical data while preserving meaning and context. When similar non-numerical data is input into an embedding algorithm, it should yield similar numerical data. The proximity of these numerical values allows for the use of mathematical semantic similarity algorithms. Related concepts include the \"vector space model\" and \"dimensionality reduction.\""}
{"source": "DataTalksClub/faq", "filename": "_questions/llm-zoomcamp/module-2/003_cdd2e59551_find-maximum-of-a-numpy-array-of-any-dimension.md", "metadata": {"id": "cdd2e59551", "question": "Find maximum of a numpy array (of any dimension):", "sort_order": 3}, "content": "```python\nmax_value = numpy_array.max()\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/llm-zoomcamp/module-2/004_db78580409_what-is-the-cosine-similarity.md", "metadata": {"id": "db78580409", "question": "What is the cosine similarity?", "sort_order": 4}, "content": "Cosine similarity is a measure used to calculate the similarity between two non-zero vectors, often used in text analysis to determine how similar two documents are based on their content. This metric computes the cosine of the angle between two vectors, which are typically word counts or TF-IDF values of the documents. The cosine similarity value ranges from -1 to 1, where 1 indicates that the vectors are identical, 0 indicates that the vectors are orthogonal (no similarity), and -1 represents completely opposite vectors."}
{"source": "DataTalksClub/faq", "filename": "_questions/llm-zoomcamp/module-2/005_can-i-use-another-vector-db-fo.md", "metadata": {"id": "3699337420", "question": "Can I use another vector db for running RAGs vector search?", "sort_order": 5}, "content": "Yes, there are other vector databases. For example, Milvus, which is open sourced. You can see the documentation here: [Milvus Documentation](https://milvus.io/docs/overview.md)"}
{"source": "DataTalksClub/faq", "filename": "_questions/llm-zoomcamp/module-2/006_95feb4e75b_why-does-cosine-similarity-reduce-to-a-matrix-mult.md", "metadata": {"id": "95feb4e75b", "question": "Why does cosine similarity reduce to a matrix multiplication between the embeddings and the query vector?", "sort_order": 6}, "content": "Cosine similarity measures how aligned two vectors are, regardless of their magnitude. When all vectors (including the query) are normalized to unit length, their magnitudes no longer matter. In this case, cosine similarity is equivalent to simply taking the dot product between the query and each document embedding. This allows us to compute similarities efficiently using matrix multiplication."}
{"source": "DataTalksClub/faq", "filename": "_questions/llm-zoomcamp/module-2/007_e087742204_why-am-i-getting-docker-invalid-reference-format-w.md", "metadata": {"id": "e087742204", "question": "Why am I getting `docker: invalid reference format` when trying to run Qdrant with a volume in Windows?", "sort_order": 7}, "content": "If you're running the `docker run` command on **Windows (especially Command Prompt or PowerShell)** and you use `$(pwd)` to mount a volume, you'll likely get the following error:\n\n```\ndocker: invalid reference format.\n```\n\nThe expression `$(pwd)` is a Unix-style command used to get the current working directory. It **won’t work in Windows**, which causes Docker to misinterpret the image name or the `-v` argument, hence the “invalid reference format” error.\n\n**Solution:**\n\n1. Use the full absolute path instead of `$(pwd)`, for example:\n\n   ```bash\n   docker run -p 6333:6333 -p 6334:6334 \\\n   -v C:/Users/youruser/path/to/qdrant_storage:/qdrant/storage:z \\\n   qdrant/qdrant\n   ```\n\n2. Alternatively, use a named volume:\n\n   ```bash\n   docker volume create qdrant_storage\n   \n   docker run -p 6333:6333 -p 6334:6334 \\\n   -v qdrant_storage:/qdrant/storage \\\n   qdrant/qdrant\n   ```"}
{"source": "DataTalksClub/faq", "filename": "_questions/llm-zoomcamp/module-2/008_a047fed655_importerror-dll-load-failed-while-importing-onnxru.md", "metadata": {"id": "a047fed655", "question": "ImportError: DLL load failed while importing onnxruntime_pybind11_state: A dynamic link library (DLL) initialization routine failed.", "sort_order": 8}, "content": "If you encounter this error while using Anaconda or Miniconda, try re-installing `onnxruntime` with the following command:\n\n```bash\nconda install -c conda-forge onnxruntime\n```\n\nTo create an environment for using `qdrant-client[fastembed]>=1.14.2`, which may trigger this error, follow these steps:\n\n1. **Create a Conda Environment**\n   \n   ```bash\n   conda create --name llm-zoomcamp-env python=3.10\n   ```\n\n2. **Activate the Environment**\n   \n   ```bash\n   conda activate llm-zoomcamp-env\n   ```\n\n3. **Install the Dependency**\n   \n   ```bash\n   pip install \"qdrant-client[fastembed]>=1.14.2\"\n   ```\n\n4. **Use the Environment**\n   \n   - **Jupyter Notebook**: Activate the environment before launching Jupyter.\n   - **VSCode/Cursor**: Use `Ctrl+Shift+P` (or `Cmd+Shift+P` on Mac), then select \"Python Interpreter\" and choose \"llm-zoomcamp-env\"."}
{"source": "DataTalksClub/faq", "filename": "_questions/llm-zoomcamp/module-3/001_d811edcea2_to-set-up-a-qdrant-client-when-to-use-client-qdran.md", "metadata": {"id": "d811edcea2", "question": "To set up a Qdrant client, when to use `client = QdrantClient(\"http://localhost:6333\")` vs `client = QdrantClient(\":memory:\")`?", "sort_order": 1}, "content": "Use the former if you are running Qdrant in Docker locally and need to connect your notebook to the Qdrant server running in Docker.\n\nThe latter option creates an in-memory Qdrant instance that runs inside your Python process. This means:\n\n- It's only for testing or prototyping.\n- It is not connected to your Docker-based Qdrant.\n- It is wiped clean when the notebook or script stops."}
{"source": "DataTalksClub/faq", "filename": "_questions/llm-zoomcamp/module-3/002_38fe6ace53_im-getting-the-error-cannot-import-name-vectorsear.md", "metadata": {"id": "38fe6ace53", "question": "I'm getting the error “cannot import name 'VectorSearch' from 'minsearch'” even though I installed the latest version of minsearch. How can I fix it?", "sort_order": 2}, "content": "If you're working with Jupyter notebooks, make sure the kernel you're using has the correct version of `minsearch`. You can check the version in your kernel with:\n\n```python\nminsearch.__version__\n```\n\nYou can also try installing the latest version directly from a notebook cell using:\n\n```python\n%pip install -U minsearch\n```\n\n`%pip` is a Jupyter magic command that ensures the package gets installed in the same environment your notebook kernel is using. This can prevent issues that arise with `!pip`, which might install it in a different environment."}
{"source": "DataTalksClub/faq", "filename": "_questions/llm-zoomcamp/module-3/003_92cd22cada_why-was-dot-used-directly-to-compute-cosine-simila.md", "metadata": {"id": "92cd22cada", "question": "Why was .dot(...) used directly to compute cosine similarity in the lesson, but normalization is emphasized in the homework?", "sort_order": 3}, "content": "In the lesson, `.dot(...)` was used under the assumption that the embeddings returned by the model (e.g., `model.encode(...)` from OpenAI) are already normalized to have unit length. In that case, the dot product is mathematically equivalent to cosine similarity.\n\nIn the homework, however, we use classic embeddings like TF-IDF + SVD, which are not normalized by default. This means that the dot product does not represent cosine similarity unless we manually normalize the vectors."}
{"source": "DataTalksClub/faq", "filename": "_questions/llm-zoomcamp/module-4/001_da7192f754_question.md", "metadata": {"id": "da7192f754", "question": "Question", "sort_order": 1}, "content": "Answer"}
{"source": "DataTalksClub/faq", "filename": "_questions/llm-zoomcamp/module-4/002_aa3f597424_warning-model-multi-qa-mpnet-base-dot-v1-was-made.md", "metadata": {"id": "aa3f597424", "question": "Warning: 'model \"multi-qa-mpnet-base-dot-v1\" was made on sentence transformers v3.0.0 bet' how to suppress?", "sort_order": 2}, "content": "To suppress the warning, upgrade `sentence-transformers` to version 3.0.0 or higher. You can do this by running the following command:\n\n```bash\npip install sentence-transformers>=3.0.0\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/llm-zoomcamp/module-4/003_15e931476c_in-windows-os-oserror-winerror-126-the-specified-m.md", "metadata": {"id": "15e931476c", "images": [{"description": "image #1", "id": "image_1", "path": "images/llm-zoomcamp/image_c9b96ca3.png"}], "question": "In Windows OS: OSError: [WinError 126] The specified module could not be found. Error loading \"C:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\lib\\fbgemm.dll\" or one of its dependencies.", "sort_order": 3}, "content": "- **Solution 1**: Install Visual C++ Redistributable.\n\n- **Solution 2**: Install Visual Studio, not Visual Studio Code. \n\n  <{IMAGE:image_1}>\n\n  For more details, please follow this link: [discuss.pytorch.org](https://discuss.pytorch.org/t/failed-to-import-pytorch-fbgemm-dll-or-one-of-its-dependencies-is-missing/201969)"}
{"source": "DataTalksClub/faq", "filename": "_questions/llm-zoomcamp/module-4/004_4b65d5542d_operationalerror-when-running-python-preppy-psycop.md", "metadata": {"id": "4b65d5542d", "question": "OperationalError when running python prep.py: psycopg2. OperationalError: could not translate host name \"postgres\" to address: No such host is known. How do I fix this issue?", "sort_order": 4}, "content": "To resolve this error, update the `.env` file:\n\n- Change the `POSTGRES_HOST` variable to `localhost`.\n\n```ini\nPOSTGRES_HOST=localhost\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/llm-zoomcamp/module-4/005_641aedbbf5_how-set-pandas-to-show-entire-text-content-in-a-co.md", "metadata": {"id": "641aedbbf5", "images": [{"description": "image #1", "id": "image_1", "path": "images/llm-zoomcamp/image_603bbc41.png"}], "question": "How set Pandas to show entire text content in a column. Useful to view the entire Explanation column content in the LLM-as-judge section of the offline-rag-evaluation notebook", "sort_order": 5}, "content": "By default, Pandas truncates text content in a column to 50 characters. To view the entire explanation provided by the judge LLM for a non-relevant answer, use the following instruction:\n\n```python\npd.set_option('display.max_colwidth', None)\n```\n\n### Explanation:\n\n- **Option:** `display.max_colwidth`\n- **Type:** `int` or `None`\n- **Description:** Sets the maximum width in characters of a column in the representation of a pandas data structure. When a column overflows, a \"...\" placeholder is used in the output. Setting it to 'None' allows unlimited width.\n- **Default:** 50\n\nRefer to the [official documentation](https://pandas.pydata.org/docs/user_guide/options.html) for more details.\n\n<{IMAGE:image_1}>"}
{"source": "DataTalksClub/faq", "filename": "_questions/llm-zoomcamp/module-4/006_5039707c1a_how-to-normalize-vectors-in-a-pandas-dataframe-col.md", "metadata": {"id": "5039707c1a", "question": "How to normalize vectors in a Pandas DataFrame column (or Pandas Series)?", "sort_order": 6}, "content": "To normalize vectors in a Pandas DataFrame column, you can use the following approach:\n\n```python\nimport numpy as np\n\nnormalize_vec = lambda v: v / np.linalg.norm(v)\n\ndf[\"new_col\"] = df[\"org_col\"].apply(normalize_vec)\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/llm-zoomcamp/module-4/007_b4c8ac5a0c_how-to-compute-the-quantile-or-percentile-of-panda.md", "metadata": {"id": "b4c8ac5a0c", "question": "How to compute the quantile or percentile of Pandas DataFrame column (or Pandas Series)?", "sort_order": 7}, "content": "To compute the 75% percentile or 0.75 quantile:\n\n```python\nquantile = df[\"col\"].quantile(q=0.75)\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/llm-zoomcamp/module-4/008_66ccbb7da0_how-can-i-remove-all-docker-containers-images-and.md", "metadata": {"id": "66ccbb7da0", "question": "How can I remove all Docker containers, images, and volumes, and builds from the terminal?", "sort_order": 8}, "content": "1. Delete all containers (including running ones):\n\n```bash\ndocker rm -f $(docker ps -aq)\n```\n\n2. Remove all images:\n\n```bash\ndocker rmi -f $(docker images -q)\n```\n\n3. Delete all volumes:\n\n```bash\ndocker volume rm $(docker volume ls -q)\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/llm-zoomcamp/module-4/009_2e729f1271_session-state-i-want-the-user-to-only-be-able-to-g.md", "metadata": {"id": "2e729f1271", "question": "Session State: I want the user to only be able to give feedback once per submission (+1 or -1). When I submit text using the ask button, the buttons should be disabled if `st.session.submitted` is False. The issue is mainly with `st.session.submitted`, which gets reassigned to True again despite one feedback button being pressed.", "sort_order": 9}, "content": "Solved:\n\n[Refer to the solution on Streamlit Discuss](https://discuss.streamlit.io/t/streamlit-session-attributes-reassigned-somewhere/76059/2?u=mohammed2)"}
{"source": "DataTalksClub/faq", "filename": "_questions/llm-zoomcamp/module-6/001_e8df9f0d12_docker-when-trying-to-run-a-streamlit-app-using-do.md", "metadata": {"id": "e8df9f0d12", "question": "Docker: When trying to run a streamlit app using docker-compose, I get: Error response from daemon: failed to create task for container: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: exec: \"streamlit\": executable file not found in $PATH: unknown. The app runs fine outside of docker-compose", "sort_order": 1}, "content": "To resolve this issue:\n\n1. Ensure you have created a `Dockerfile`.\n2. Add `streamlit` to the `docker-compose` configuration.\n3. Run the following command to rebuild and start the application:\n\n   ```bash\n   docker-compose up --build\n   ```"}
{"source": "DataTalksClub/faq", "filename": "_questions/llm-zoomcamp/module-6/002_b9993f038e_question.md", "metadata": {"id": "b9993f038e", "question": "Question", "sort_order": 2}, "content": "Answer"}
{"source": "DataTalksClub/faq", "filename": "_questions/llm-zoomcamp/project/001_ec3aa9c08c_question.md", "metadata": {"id": "ec3aa9c08c", "question": "Question", "sort_order": 1}, "content": "Answer"}
{"source": "DataTalksClub/faq", "filename": "_questions/llm-zoomcamp/project/002_0fab61eca2_is-it-a-group-project.md", "metadata": {"id": "0fab61eca2", "question": "Is it a group project?", "sort_order": 2}, "content": "No, the capstone is a solo project."}
{"source": "DataTalksClub/faq", "filename": "_questions/llm-zoomcamp/project/003_d5fc98925d_do-we-submit-2-projects-what-does-attempt-1-and-2.md", "metadata": {"id": "d5fc98925d", "question": "Do we submit 2 projects, what does attempt 1 and 2 mean?", "sort_order": 3}, "content": "You only need to submit one project. If the submission at the first attempt fails, you can improve it and re-submit during the attempt#2 submission window.\n\n- If you want to submit two projects for the experience and exposure, you must use different datasets and problem statements.\n- If you can’t make it to the attempt#1 submission window, you still have time to catch up to meet the attempt#2 submission window.\n\nRemember that the submission does not count towards the certification if you do not participate in the peer-review of three peers in your cohort."}
{"source": "DataTalksClub/faq", "filename": "_questions/llm-zoomcamp/project/004_e892d44e42_does-the-competition-count-as-the-capstone.md", "metadata": {"id": "e892d44e42", "question": "Does the competition count as the capstone?", "sort_order": 4}, "content": "No, it does not. You can participate in the [math-kaggle-llm-competition](https://datatalks-club.slack.com/archives/C0791HB4A58) as a group if you want to form teams; but the capstone is an individual attempt."}
{"source": "DataTalksClub/faq", "filename": "_questions/llm-zoomcamp/project/005_9a2e2d2008_how-is-my-capstone-project-going-to-be-evaluated.md", "metadata": {"id": "9a2e2d2008", "question": "How is my capstone project going to be evaluated?", "sort_order": 5}, "content": "Each submitted project will be evaluated by three randomly assigned students who have also submitted the project.\n\nYou will also be responsible for grading the projects from three fellow students yourself. Please be aware that not complying with this rule implies you may fail to achieve the Certificate at the end of the course.\n\nThe final grade you receive will be the median score of the grades from the peer reviewers. The peer review criteria for evaluation must follow the guidelines defined here (TBA for link)."}
{"source": "DataTalksClub/faq", "filename": "_questions/llm-zoomcamp/project/006_eae0fb50aa_when-and-how-will-we-be-assigned-projects-for-revi.md", "metadata": {"id": "eae0fb50aa", "question": "When and how will we be assigned projects for review/grading?", "sort_order": 6}, "content": "After the submission deadline has passed, an Excel sheet will be shared with 3 projects being assigned to each participant."}
{"source": "DataTalksClub/faq", "filename": "_questions/llm-zoomcamp/project/007_a8a7fef016_ive-already-submitted-my-project-why-cant-i-review.md", "metadata": {"id": "a8a7fef016", "question": "I’ve already submitted my project. Why can’t I review any projects?", "sort_order": 7}, "content": "Once the project submission deadline has passed, projects will be assigned to you for evaluation. You can't choose which projects to evaluate, and you can’t review before the list has been released."}
{"source": "DataTalksClub/faq", "filename": "_questions/llm-zoomcamp/project/008_e76a70cde3_how-can-i-find-some-good-ideas-or-datasets-for-the.md", "metadata": {"id": "e76a70cde3", "question": "How can I find some good ideas or datasets for the project?", "sort_order": 8}, "content": "Please check [this GitHub page](https://github.com/DataTalksClub/llm-zoomcamp/blob/main/project.md) for several ideas and datasets that could be used for the project, along with tips and guidelines."}
{"source": "DataTalksClub/faq", "filename": "_questions/llm-zoomcamp/project/009_b3fd85b2bd_do-i-have-to-use-elasticsearch-or-x-library.md", "metadata": {"id": "b3fd85b2bd", "question": "Do I have to use ElasticSearch or X library?", "sort_order": 9}, "content": "No, you don’t have to use ElasticSearch. You can use any library you want. Just make sure it is documented so your peer-reviewers can reproduce your project."}
{"source": "DataTalksClub/faq", "filename": "_questions/llm-zoomcamp/workshops-agents/001_cefd5ecc70_what-is-the-approximate-cost-of-running-the-worksh.md", "metadata": {"id": "cefd5ecc70", "question": "What is the approximate cost of running the workshop notebook (DLT + Cognee)?", "sort_order": 1}, "content": "The total cost is approximately $0.09 USD, based on pricing as of July 7, 2025.\n\nThis estimate includes all API calls to OpenAI for generating embeddings and relationship extraction, as well as local operations for loading data into Qdrant and Kuzu."}
{"source": "DataTalksClub/faq", "filename": "_questions/llm-zoomcamp/workshops-agents/002_c6fc2d4d11_connection-refused-error-on-prompting-the-ollam-ra.md", "metadata": {"id": "c6fc2d4d11", "question": "Connection refused error on prompting the ollam RAG?", "sort_order": 2}, "content": "If you encounter this error while doing the homework, you can resolve it by restarting the Ollama server using the following command:\n\n```bash\n!nohup ollama serve > nohup.out 2>&1 &\n```\n\nMake sure to rerun the cell containing `ollama serve` if you stop and restart the notebook cell."}
{"source": "DataTalksClub/faq", "filename": "_questions/llm-zoomcamp/workshops-agents/003_13505f864c_error-connecting-to-elasticsearch-at-elasticsearch.md", "metadata": {"id": "13505f864c", "question": "Error: Connecting to Elasticsearch at [elasticsearch:9200](http://elasticsearch:9200)", "sort_order": 3}, "content": "Try removing the driver bridge from the configuration."}
{"source": "DataTalksClub/faq", "filename": "_questions/llm-zoomcamp/workshops-agents/004_5307cb0fa9_evaluation-multiple-retrieval-approaches-are-evalu.md", "metadata": {"id": "5307cb0fa9", "question": "Evaluation: Multiple retrieval approaches are evaluated, and the best one is used (2 points). I am trying to evaluate a project. The person used only minsearch for evaluation but applied boosting and posted the boosting parameter. Do they get one mark?", "sort_order": 4}, "content": "The evaluation criteria state that to receive 2 points, multiple RAG approaches must be evaluated, and the best one must be used. Since the individual is using only minsearch for evaluation, despite applying boosting, this does not qualify as evaluating multiple RAG approaches.\n\nTherefore, they would receive only 1 point for utilizing a single RAG approach (minsearch) in their evaluation, even though they incorporated a boosting parameter. Boosting itself does not constitute a separate approach; it is simply an enhancement applied to the single method being used."}
{"source": "DataTalksClub/faq", "filename": "_questions/llm-zoomcamp/workshops-agents/005_fe87f03913_elasticsearch-version-error.md", "metadata": {"id": "fe87f03913", "question": "Elasticsearch version error", "sort_order": 5}, "content": "### Elasticsearch version error\n\n**Error:**\n```plaintext\nelasticsearch.BadRequestError: BadRequestError(400, 'media_type_header_exception', 'Invalid media-type value on headers [Content-Type, Accept]', Accept version must be either version 8 or 7, but found 9. Accept=application/vnd.elasticsearch+json; compatible-with=9)\n```\n\n**Fix:**\n1. Uninstall the current Elasticsearch package:\n   ```bash\n   pip uninstall elasticsearch\n   ```\n2. Install the correct version (8.10.0) of the Elasticsearch package:\n   ```bash\n   pip install elasticsearch==8.10.0\n   ```"}
{"source": "DataTalksClub/faq", "filename": "_questions/llm-zoomcamp/workshops-agents/006_094dc060a4_appendableindex-error-in-minsearch.md", "metadata": {"id": "094dc060a4", "question": "AppendableIndex error in minsearch", "sort_order": 6}, "content": "### Error\n\n```\nImportError: cannot import name 'AppendableIndex' from 'minsearch'\n```\n\n### Fix\n\n1. Run the following command to upgrade:\n   \n   ```bash\n   pip install --upgrade minsearch\n   ```\n\n2. Ensure you are using minsearch version 0.0.4.\n\n3. Restart the Jupyter kernel after the upgrade."}
{"source": "DataTalksClub/faq", "filename": "_questions/llm-zoomcamp/workshops-agents/007_84acbe420a_appendableindex-error-in-minsearch-not-resolved-by.md", "metadata": {"id": "84acbe420a", "question": "AppendableIndex error in minsearch (not resolved by upgrading minsearch)", "sort_order": 7}, "content": "**Error:**\n\n```python\nImportError: cannot import name 'AppendableIndex' from 'minsearch'\n```\n\n**Fix:**\n\n```python\nfrom minsearch.append import AppendableIndex\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/llm-zoomcamp/workshops-agents/008_0470c11c69_appendableindex-error-in-minsearch-not-resolved-by.md", "metadata": {"id": "0470c11c69", "question": "AppendableIndex error in minsearch (not resolved by upgrading minsearch or importing from minsearch.append)", "sort_order": 8}, "content": "Error:\n\n```\nImportError: cannot import name 'AppendableIndex' from 'minsearch'\n```\n\nFix:\n\n- Rename the previously downloaded `minsearch.py` file to avoid conflicts.\n- Reinstall `minsearch` using pip so the import works correctly."}
{"source": "DataTalksClub/faq", "filename": "_questions/llm-zoomcamp/workshops-agents/009_0d74a3616f_any-free-models-with-tool-use-support.md", "metadata": {"id": "0d74a3616f", "question": "Any free models with tool use support?", "sort_order": 9}, "content": "Several Groq models offer tool use, such as Deepseek R1 or Llama 4, all of which can be used for free for development.\n\nFor more details, please visit: [Groq Tool Use Documentation](https://console.groq.com/docs/tool-use)"}
{"source": "DataTalksClub/faq", "filename": "_questions/llm-zoomcamp/workshops-agents/010_5beafdd0c8_i-passed-a-float-to-my-tool-but-got-a-validation-e.md", "metadata": {"id": "5beafdd0c8", "question": "I passed a float to my tool, but got a validation error saying it expected a number. Isn’t float a number?", "sort_order": 10}, "content": "Yes — in Python, `float` is a numeric type. But when working with FastMCP, tool inputs are validated against JSON Schema, which uses the term \"number\" to represent any numeric value (integers or floats).\n\nThe important thing is not the type you use in Python, but whether the JSON you send matches the tool's declared input schema.\n\nExample:\n\n```json\n\"inputSchema\": {\n  \"type\": \"object\",\n  \"properties\": {\n    \"temp\": {\n      \"type\": \"number\"\n    }\n  },\n  \"required\": [\"temp\"]\n}\n```\n\nMake sure the values in \"arguments\" match the types declared in the tool’s schema — not Python types, but JSON types (string, number, boolean, etc.)."}
{"source": "DataTalksClub/faq", "filename": "_questions/llm-zoomcamp/workshops-agents/011_d09e8d4843_install-mcp-inspector.md", "metadata": {"id": "d09e8d4843", "question": "Install MCP Inspector", "sort_order": 11}, "content": "1. Ensure Node.js is installed.\n\n2. To install the MCP Inspector, run the following command in your terminal:\n\n   ```bash\n   npm i @modelcontextprotocol/inspector\n   ```"}
{"source": "DataTalksClub/faq", "filename": "_questions/llm-zoomcamp/workshops-agents/012_42c7af7ed5_run-mcp-inspector.md", "metadata": {"id": "42c7af7ed5", "question": "Run MCP Inspector", "sort_order": 12}, "content": "To run the MCP Inspector, execute the following command in the terminal:\n\n```bash\nnpx @modelcontextprotocol/inspector\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/llm-zoomcamp/workshops-agents/013_9c2fd5451c_inspect-mcp-server.md", "metadata": {"id": "9c2fd5451c", "images": [{"description": "image #1", "id": "image_1", "path": "images/llm-zoomcamp/image_3f1169b4.png"}, {"description": "image #2", "id": "image_2", "path": "images/llm-zoomcamp/image_2736f221.png"}], "question": "Inspect MCP Server", "sort_order": 13}, "content": "- Connect to the MCP Server\n\n  <{IMAGE:image_1}>\n\n- The inspector can list tools, templates, resources, and prompts from the MCP Server\n\n  <{IMAGE:image_2}>\n\nReference:\n\n[https://medium.com/@anil.goyal0057/how-to-test-your-mcp-server-using-mcp-inspector-c873c417eec1](https://medium.com/@anil.goyal0057/how-to-test-your-mcp-server-using-mcp-inspector-c873c417eec1)"}
{"source": "DataTalksClub/faq", "filename": "_questions/llm-zoomcamp/workshops-agents/014_f96fde1726_how-to-solve-runtimeerror-already-running-asyncio.md", "metadata": {"id": "f96fde1726", "question": "How to Solve \"RuntimeError: Already running asyncio in this thread\"", "sort_order": 14}, "content": "Jupyter notebooks already run an event loop in the main thread to handle asynchronous code. For this reason, when you try to call `asyncio.run()` inside a cell, you get the following error:\n\n```plaintext\nRuntimeError: asyncio.run() cannot be called from a running event loop\n```\n\nInstead of using `asyncio.run()`, simply use `await` directly in the notebook cell.\n\n**Incorrect:**\n\n```python\nimport asyncio\n\nasync def main():\n    async with Client(weather_server.mcp) as mcp_client:\n        # your code here\n\n# This will cause the RuntimeError\nresult = asyncio.run(main())\n```\n\n**Correct:**\n\n```python\nasync def main():\n    async with Client(weather_server.mcp) as mcp_client:\n        # your code here\n\n# Use await directly\nresult = await main()\n```\n\nJupyter notebooks automatically create an asyncio event loop when they start. Since `asyncio.run()` attempts to create a new event loop, it conflicts with the existing loop. By using `await` directly, you leverage the already running event loop."}
{"source": "DataTalksClub/faq", "filename": "_questions/llm-zoomcamp/workshops-agents/015_b9c7c6bb34_i-am-using-azure-openai-and-i-am-still-getting-an.md", "metadata": {"id": "b9c7c6bb34", "question": "I am using Azure OpenAI and I am still getting an error of Error code: 400 - {'error': {'message': \"Missing required parameter: 'tools[0].function'.\", 'type': 'invalid_request_error', 'param': 'tools[0].function', 'code': 'missing_required_parameter'}}?", "sort_order": 15}, "content": "Modify the `get_weather_tool` JSON to be the following:\n\n```json\nget_weather_tool = {\n    \"type\": \"function\",\n    \"function\": {\n        \"name\": \"get_weather\",\n        \"description\": \"Get the current weather for a specific city or generate fake data\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"city\": {\n                    \"type\": \"string\",\n                    \"description\": \"The name of the city to get the weather for.\"\n                }\n            },\n            \"required\": [\"city\"],\n            \"additionalProperties\": false\n        }\n    }\n}\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/llm-zoomcamp/workshops-dlthub/001_70e2c19e28_question.md", "metadata": {"id": "70e2c19e28", "question": "Question", "sort_order": 1}, "content": "Answer"}
{"source": "DataTalksClub/faq", "filename": "_questions/llm-zoomcamp/workshops-dlthub/002_d850a0e7ea_can-i-use-the-workshop-materials-for-my-own-projec.md", "metadata": {"id": "d850a0e7ea", "question": "Can I use the workshop materials for my own projects or share them with others?", "sort_order": 2}, "content": "Since dlt is open-source, you can use the content of this workshop for a capstone project. As the main goal of dlt is to load and store data easily, you can even use it for other Zoomcamps (like the MLOps Zoomcamp project). Feel free to ask questions or use it directly in your projects."}
{"source": "DataTalksClub/faq", "filename": "_questions/llm-zoomcamp/workshops-dlthub/003_6bceeebf04_how-to-set-up-a-new-dlt-project-when-loading-from.md", "metadata": {"id": "6bceeebf04", "question": "How to set up a new dlt project when loading from cloud?", "sort_order": 3}, "content": "Start with the following command on the command line:\n\n```bash\n dlt init filesystem duckdb\n```\n\nMore directions can be found at [dlthub.com](https://dlthub.com/docs/tutorial/filesystem)"}
{"source": "DataTalksClub/faq", "filename": "_questions/llm-zoomcamp/workshops-dlthub/004_there-is-an-error-when-opening-the-table-using-dbt.md", "metadata": {"id": "0407213397", "question": "There is an error when opening the table using `dbtable = db.open_table(\"notion_pages___homework\")`: `FileNotFoundError: Table notion_pages___homework does not exist. Please first call db.create_table(notion_pages___homework, data)`", "sort_order": 4}, "content": "The error indicates that you have not changed all instances of \"employee_handbook\" to \"homework\" in your pipeline settings."}
{"source": "DataTalksClub/faq", "filename": "_questions/llm-zoomcamp/workshops-dlthub/005_e95bfafc0e_there-is-an-error-when-running-main-filenotfounder.md", "metadata": {"id": "e95bfafc0e", "question": "There is an error when running main(): FileNotFoundError: Table notion_pages___homework does not exist. Please first call db.create_table(notion_pages___homework, data)", "sort_order": 5}, "content": "Make sure you open the correct table in line 3:\n\n```python\ndbtable = db.open_table(\"notion_pages___homework\")\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/llm-zoomcamp/workshops-dlthub/006_e394e6f738_how-do-i-know-which-tables-are-in-the-db.md", "metadata": {"id": "e394e6f738", "question": "How do I know which tables are in the db?", "sort_order": 6}, "content": "You can use the `db.table_names()` method to list all the tables in the database."}
{"source": "DataTalksClub/faq", "filename": "_questions/llm-zoomcamp/workshops-dlthub/007_cd19f122f3_does-dlt-have-connectors-to-clickhouse-or-starrock.md", "metadata": {"id": "cd19f122f3", "question": "Does DLT have connectors to ClickHouse or StarRocks?", "sort_order": 7}, "content": "Currently, DLT does not have connectors for ClickHouse or StarRocks but is open to contributions from the community to add these connectors."}
{"source": "DataTalksClub/faq", "filename": "_questions/llm-zoomcamp/workshops-dlthub/008_0dfba07ace_notebook-does-not-have-secret-access-or-401-client.md", "metadata": {"id": "0dfba07ace", "question": "Notebook does not have secret access or 401 Client Error: Unauthorized for url: [api.notion.com](https://api.notion.com/v1/search)", "sort_order": 8}, "content": "If you encounter this error, it typically indicates an authorization issue with the Notion API. Here’s how you can resolve it:\n\n1. **Check API Key**: Ensure that you are using the correct API key with appropriate permissions.\n2. **Verify API Endpoint**: Confirm that you are hitting the correct Notion API endpoint.\n3. **Token Expiry**: Check if your token has expired and regenerate it if necessary.\n4. **Configurations**: Double-check all access configurations in your application.\n\nIf the error persists, review the API documentation and make sure all necessary authentication steps are correctly implemented."}
{"source": "DataTalksClub/faq", "filename": "_questions/llm-zoomcamp/workshops-dlthub/009_4b30b918bc_error-how-to-fix-requests-library-only-installs-v2.md", "metadata": {"id": "4b30b918bc", "images": [{"description": "image #1", "id": "image_1", "path": "images/llm-zoomcamp/image_9e3aa5c7.png"}], "question": "Error: How to fix requests library only installs v2.28 instead of v2.32 required for lancedb?", "sort_order": 9}, "content": "If you encounter a 401 Client Error, it may indicate the need to grant access to the key or that the key is incorrect.\n\nTo install the correct version directly from the source, use the following command:\n\n```bash\npip install \"requests @ https://github.com/psf/requests/archive/refs/tags/v2.32.3.zip\"\n```\n\n<{IMAGE:image_1}>"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/general/001_0e38656cfb_how-do-i-submit-homework.md", "metadata": {"id": "0e38656cfb", "question": "How do I submit homework?", "sort_order": 1}, "content": "- Do the tasks locally\n- Publish your code (e.g., in your own GitHub repo)\n- Submit your answers via the homework form and include the URL to your code\n- You will see the answers only after the deadline\n- Homeworks are in the cohorts folder, e.g. for 2025 it's [`cohorts/2025`](https://github.com/DataTalksClub/machine-learning-zoomcamp/tree/master/cohorts/2025)\n- The forms for submitting the homework are in the [course management platform](https://courses.datatalks.club/)"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/general/001_226a4baf2f_whats-new-2025-edition.md", "metadata": {"id": "226a4baf2f", "question": "What’s new in the 2025 edition?", "sort_order": 1}, "content": "- Deployment module updated to **FastAPI** (replacing Flask) and new tools.\n- Neural networks taught with **PyTorch** (theory videos in Keras are kept; an additional PyTorch implementation video is provided).\n- Deep learning deployment uses **ONNX Runtime** on AWS Lambda (replacing TensorFlow Lite)."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/general/001_4d5aa45b03_are-jupyter-notebooks-used.md", "metadata": {"id": "4d5aa45b03", "question": "Are Jupyter Notebooks used?", "sort_order": 1}, "content": "Yes. You’ll work extensively with notebooks alongside standard Python files and CLI tools."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/general/001_c7e2b1ef88_no-prior-ml-experience.md", "metadata": {"id": "c7e2b1ef88", "question": "Do I need prior machine learning experience?", "sort_order": 1}, "content": "No. The course starts from first principles. We do an introduction to ML, a gentle math refresher (only what you need), and the basics of linear algebra taught via code. You can begin with zero ML background and ramp up through hands-on exercises."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/general/001_f9c9e0b9eb_updates-fastapi-pytorch-repo.md", "metadata": {"id": "f9c9e0b9eb", "question": "Will the repo already have all FastAPI/uv/PyTorch updates?", "sort_order": 1}, "content": "The repository does not lock in every update at once. Some updates for FastAPI/uv and PyTorch are released as the cohort progresses. For theory, older videos remain; for implementation, updated workshops and videos are linked (for example, the FastAPI+uv workshop and the PyTorch add-on). To find the latest materials, check the module pages for updated workshop links. If a critical update is pushed, instructors will annotate or replace the relevant material accordingly."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/general/002_8fd8cd336d_how-do-i-sign-up.md", "metadata": {"id": "8fd8cd336d", "question": "How do I sign up?", "sort_order": 2}, "content": "In the course [GitHub repository](http://mlzoomcamp.com), there’s a link to sign up. Here it is: [airtable.com](https://airtable.com/shryxwLd0COOEaqXo)"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/general/002_9155c70324_when-is-the-next-cohort.md", "metadata": {"id": "9155c70324", "question": "When is the next cohort?", "sort_order": 2}, "content": "The course is available in a self-paced mode, allowing you to go through the materials anytime. If you prefer to join a cohort with other students, live cohorts start every September.\n\nZoomcamps are scheduled throughout the year. For more information, refer to [A Guide to Free Online Courses at DataTalks.Club](https://datatalks.club/blog/guide-to-free-online-courses-at-datatalks-club.html).\n\nCourse videos are pre-recorded and available to watch immediately. We also occasionally host office hours for live Q&A, which are recorded and made available later. You can find these recordings and pre-recorded course videos on the Course Channel’s Bookmarks or [DTC’s YouTube channel](https://www.youtube.com/@DataTalksClub/playlists)."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/general/002_9cf1c56e5b_should-i-star-the-github-repo.md", "metadata": {"id": "9cf1c56e5b", "question": "Should I star the GitHub repo?", "sort_order": 2}, "content": "Yes, please star the repository: https://github.com/DataTalksClub/machine-learning-zoomcamp/. Starring helps it trend so others can discover the course, and the instructor explicitly asks attendees to star it to boost visibility."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/general/002_e0a95572a6_join-slack-invite-not-arrived.md", "metadata": {"id": "e0a95572a6", "question": "How do I join Slack if the invite email didn’t arrive?", "sort_order": 2}, "content": "Go to DataTalks.Club, request a Slack invite, or use the manual request form (processed daily). After joining, browse channels and join **#course-ml-zoomcamp**."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/general/003_1f17886bde_how-to-get-help-if-stuck.md", "metadata": {"id": "1f17886bde", "question": "How do I get help if I’m stuck?", "sort_order": 3}, "content": "- [Slack](https://datatalks.club/slack.html): paste your code and errors, and use threads to keep discussions organized.\n- this FAQ and [past office hours](https://www.youtube.com/@DataTalksClub/search?query=ml%20zoomcamp%20office%20hours): check these resources for guidance before asking.\n- AI assistants like ChatGPT: use them for quick guidance or brainstorming.\n- Peers and instructors: they may chime in when available."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/general/003_2b5ff70c77_no-enrollment-before-submit.md", "metadata": {"id": "2b5ff70c77", "question": "Do I need to enroll in the course before submitting homework?", "sort_order": 3}, "content": "No enrollment is required to submit homework. Just log into the homework form when it opens. The Airtable registration you may see is only for announcements; actual submissions are made on the course platform forms and via your GitHub as specified in the homework guidelines."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/general/003_34c01677b8_what-if-i-miss-a-session.md", "metadata": {"id": "34c01677b8", "question": "What if I miss a session?", "sort_order": 3}, "content": "Everything is recorded, so you won’t miss office hours or any content. You can ask your questions in advance, and they will be covered during the live stream. Additionally, you can ask questions in Slack."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/general/003_ef6fba8f4d_prerequisites-terminal-docker.md", "metadata": {"id": "ef6fba8f4d", "question": "What other prerequisites should I have?", "sort_order": 3}, "content": "- Be ready to use the terminal. You’ll use Git, the command line, Docker, and basic OS tasks. These are demonstrated in the course, but you should be comfortable running commands and reading errors.\n- If you’re new to any of these tools, consider a quick hands-on practice session before the course starts.\n- You don’t need to master everything before day one, but a basic comfort level with the command line will help you keep up as you learn."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/general/004_0d854c74af_donations-sponsorships.md", "metadata": {"id": "0d854c74af", "question": "How do donations/sponsorships work?", "sort_order": 4}, "content": "- The course is free.\n- Sponsors may be added.\n- Voluntary donations help too. You can donate here: https://github.com/sponsors/alexeygrigorev.\n- If you have a company training budget, you can request an invoice to support the course."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/general/004_79e181d005_how-much-theory-will-you-cover.md", "metadata": {"id": "79e181d005", "question": "How much theory will you cover?", "sort_order": 4}, "content": "The bare minimum. The focus is more on practice, and we'll cover the theory only on the intuitive level.\n\nFor example, we won't derive the gradient update rule for logistic regression (there are other great courses for that), but we'll cover how to use logistic regression and make sense of the results."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/general/005_30bf904a5b_do-i-need-to-know-python.md", "metadata": {"id": "30bf904a5b", "question": "Do I need to know Python already?", "sort_order": 5}, "content": "Not strictly required, but you must be comfortable programming. If you know another major language (Java, JavaScript, Rust, etc.), you’ll pick up the small subset of Python we use. Expect a lot of coding and 'code-along' sessions."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/general/005_37813677f1_i-dont-know-math-can-i-take-the-course.md", "metadata": {"id": "37813677f1", "question": "I don't know math. Can I take the course?", "sort_order": 5}, "content": "Math is not strictly a prerequisite to start learning machine learning (ML), but having a strong foundation in certain mathematical concepts can significantly improve your understanding and ability to work with ML models.\n\nYes! We'll cover some linear algebra in the course, but in general, there will be very few formulas, mostly code.\n\nHere are some interesting videos covering linear algebra that you can already watch:\n\n- [ML Zoomcamp 1.8 - Linear Algebra Refresher](https://www.youtube.com/watch?v=zZyKUeOR4Gg&list=PL3MmuxUbc_hIhxl5Ji8t4O6lPAOpHaCLR&index=8&ab_channel=DataTalksClub%E2%AC%9B) from Alexey Grigorev\n- The excellent playlist from 3Blue1Brown: [Vectors | Chapter 1, Essence of linear algebra](https://www.youtube.com/watch?v=fNk_zzaMoSs&list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab&ab_channel=3Blue1Brown)\n\nNever hesitate to ask the community for help if you have any questions."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/general/005_90323db1ac_None.md", "metadata": {"id": "90323db1ac", "question": "What’s the recent pass/completion rate?", "sort_order": 5}, "content": "In the most recent cohort's dashboard snapshot, roughly 85% of project attempts passed."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/general/006_2453bf4760_spark-bigdata-tooling.md", "metadata": {"id": "2453bf4760", "question": "Will we cover Spark or big-data tooling?", "sort_order": 6}, "content": "No Spark in this course. The focus is on core ML engineering and deployment patterns. We do cover deployment tooling relevant to ML (e.g., Kubernetes, TensorFlow Serving, KServe) as part of the deployment modules; see Modules 5–11 for details."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/general/006_d57d1f7b18_i-filled-the-form-but-havent-received-a-confirmati.md", "metadata": {"id": "d57d1f7b18", "question": "I filled the form, but haven't received a confirmation email. Is it normal?", "sort_order": 6}, "content": "Normally, you'd receive the email shortly after you signed up.\n\n- Check your promotions tab in Gmail as well as spam, as the email might have been filtered there.\n- If you unsubscribed from our newsletter, you won't receive course-related updates.\n\nDon't worry, it’s not a problem. To make sure you don’t miss anything, join the [#course-ml-zoomcamp channel in Slack](https://app.slack.com/client/T01ATQK62F8/C0288NJ5XSA) and [our telegram channel with announcements](https://t.me/mlzoomcamp). This is sufficient to follow the course."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/general/007_742d3dbb55_how-long-is-the-course.md", "metadata": {"id": "742d3dbb55", "question": "How long is the course?", "sort_order": 7}, "content": "Approximately 4 months, but it may take longer if you want to engage in extra activities such as an additional project or writing an article."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/general/008_36c30c8e2f_how-much-time-do-i-need-for-this-course.md", "metadata": {"id": "36c30c8e2f", "question": "How much time do I need for this course?", "sort_order": 8}, "content": "Around ~10 hours per week.\n\nYou can see how much time people spend on the 2024 edition [here](https://courses.datatalks.club/ml-zoomcamp-2024/dashboard)."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/general/009_9b3aa73ec4_will-i-get-a-certificate.md", "metadata": {"id": "9b3aa73ec4", "question": "Will I get a certificate?", "sort_order": 9}, "content": "Yes, if you finish at least 2 out of 3 projects and review 3 peers’ projects by the deadline, you will get a certificate. This is what it looks like: [this](https://certificate.datatalks.club/mlzoomcamp/2021/35fc7e051003fddcc6909a8ee96703bd9c31b454.pdf)."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/general/009_ef9594914c_how-to-get-answers-common-questions.md", "metadata": {"id": "ef9594914c", "question": "How can I get answers to common questions?", "sort_order": 9}, "content": "Start with the FAQ document (general + per-module Q&A). There’s also a Zoomcamp Q&A bot in Slack—use it thoughtfully; often the FAQ or recent messages already contain your answer."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/general/010_62f5aa4d83_data-engineers-to-dsml.md", "metadata": {"id": "62f5aa4d83", "question": "Can this program help data engineers move into DS/ML engineering?", "sort_order": 10}, "content": "Yes. The program’s project-first flow, deployment modules (FastAPI/Lambda/Kubernetes), and evaluation practices map well to ML engineer roles. Practically, you’ll gain experience with end-to-end ML workflows—from data engineering pipelines and model deployment to evaluation and monitoring—using tools commonly used in DS/ML roles."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/general/010_f22955c57d_will-i-get-a-certificate-if-i-missed-the-midterm-p.md", "metadata": {"id": "f22955c57d", "question": "Will I get a certificate if I missed the midterm project?", "sort_order": 10}, "content": "Yes, it's possible. See the previous answer."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/general/011_352573dfb0_how-much-python-should-i-know.md", "metadata": {"id": "352573dfb0", "question": "How much Python should I know?", "sort_order": 11}, "content": "Check [this article](https://mlbookcamp.com/article/python). If you know everything in this article, you know enough. If not, read the article and consider joining the course [Introduction to Python – Machine Learning Bootcamp](https://mlbookcamp.com/article/python).\n\nYou can also follow the free English course \"Learn Python Basics for Data Analysis\" on the OpenClassrooms e-learning platform: [Learn Python Basics for Data Analysis - OpenClassrooms](https://openclassrooms.com/fr/courses/2304731-learn-python-basics-for-data-analysis).\n\nIt's important to know some basics such as:\n\n- How to run a Jupyter notebook\n- How to import libraries (and understand what libraries are)\n- How to declare a variable (and understand what variables are)\n- Some important operations regarding data analysis"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/general/012_edf4b7b73e_do-i-need-any-special-hardware.md", "metadata": {"id": "edf4b7b73e", "question": "Do I need any special hardware?", "sort_order": 12}, "content": "For the Machine Learning part, all you need is a working laptop with an internet connection. The Deep Learning part is more resource intensive, but for that you can use a cloud. We use SaturnCloud, but it can be anything else."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/general/013_a11770e750_im-new-to-slack-and-cant-find-the-course-channel-w.md", "metadata": {"id": "a11770e750", "question": "I’m new to Slack and can’t find the course channel. Where is it?", "sort_order": 13}, "content": "Here’s how you join in Slack: [https://slack.com/help/articles/205239967-Join-a-channel](https://slack.com/help/articles/205239967-Join-a-channel)\n\n- Click “All channels” at the top of your left sidebar. If you don't see this option, click “More” to find it.\n- Browse the list of public channels in your workspace, or use the search bar to search by channel name or description.\n- Select a channel from the list to view it.\n- Click Join Channel.\n\nDo we need to provide the GitHub link to only our code corresponding to the homework questions?\n\nYes. You are required to provide the URL to your repo in order to receive a grade."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/general/014_41aabbd7c5_the-course-has-already-started-can-i-still-join-it.md", "metadata": {"id": "41aabbd7c5", "question": "The course has already started. Can I still join it?", "sort_order": 14}, "content": "Yes, you can. Even though you missed the start date, you can register for the course. You won’t be able to submit some of the homeworks, but you can still take part in the course.\n\nIn order to get a certificate, you need to submit 2 out of 3 course projects and review 3 peers by the deadline. It means that if you join the course at the end of November and manage to work on two projects, you will still be eligible for a certificate."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/general/014_c02a8e0913_how-announcements-work.md", "metadata": {"id": "c02a8e0913", "question": "How do announcements work?", "sort_order": 14}, "content": "Announcements are posted in the Telegram channel and mirrored into Slack. It’s best to join Telegram for clean, broadcast-only updates; otherwise you’ll still see mirrored posts in Slack."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/general/015_1a4a7bd286_homeworks-optional-certification.md", "metadata": {"id": "1a4a7bd286", "question": "Are homeworks required to get the certificate?", "sort_order": 15}, "content": "Homeworks are optional for certification, but strongly recommended to check understanding. Certification is based on projects, not homework scores."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/general/015_3ff40b74f9_can-i-submit-the-homework-after-the-due-date.md", "metadata": {"id": "3ff40b74f9", "question": "Can I submit the homework after the due date?", "sort_order": 15}, "content": "No, it’s not possible. The form is closed after the due date. But don’t worry, homework is not mandatory for finishing the course."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/general/016_679b977c92_where-materials.md", "metadata": {"id": "679b977c92", "question": "Where can I find all course materials and how can I keep track of lessons and modules?", "sort_order": 16}, "content": "All course materials and links live in the main ML Zoomcamp GitHub repository:\n- Repository: https://github.com/DataTalksClub/machine-learning-zoomcamp/\n- Modules: folders for each unit, with a README/markdown per unit, plus videos and notes.\n- Cohort-specific items (homework and dates) are under cohorts/2025.\n- YouTube playlist: https://www.youtube.com/playlist?list=PL3MmuxUbc_hIhxl5Ji8t4O6lPAOpHaCLR contains all lessons.\n- Year-specific playlists (2021–2025) contain cohort-specific streams and extras.\n- Quick access tips: browse the repo, check the modules folders, and look in cohorts/2025 for cohort-specific items. The syllabus is also available in the course repo mlzoomcamp.com/#syllabus."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/general/017_e053b881de_what-are-the-deadlines-in-this-course.md", "metadata": {"id": "e053b881de", "question": "What are the deadlines in this course?", "sort_order": 17}, "content": "For the 2024 cohort, you can find the deadlines [here](https://courses.datatalks.club/ml-zoomcamp-2024/) (it’s taken from [the 2024 cohort page](https://github.com/DataTalksClub/machine-learning-zoomcamp/tree/master/cohorts/2024)) or in [Google Calendar](https://calendar.google.com/calendar/?cid=cGtjZ2tkbGc1OG9yb2lxa2Vwc2g4YXMzMmNAZ3JvdXAuY2FsZW5kYXIuZ29vZ2xlLmNvbQ)."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/general/019_c0f1901d39_submitting-learning-in-public-links.md", "metadata": {"id": "c0f1901d39", "question": "Submitting learning in public links", "sort_order": 19}, "content": "When you post about what you learned from the course on your social media pages, use the tag `#mlzoomcamp`. When you submit your homework, there’s a section in the form for putting the links there. Separate multiple links by any whitespace character (linebreak, space, tab, etc).\n\n- For posting the learning in public links, you get extra scores.\n- The number of scores is limited to 7 points: if you put more than 7 links in your homework form, you’ll get only 7 points.\n- The same content can be posted to 7 different social sites and still earn you 7 points if you add 7 URLs per week.\n\nFor midterms/capstones, the awarded points are doubled as the duration is longer. So for projects, the points are capped at 14 for 14 URLs."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/general/020_c090262408_can-i-share-my-answers-of-the-homework-with-the-co.md", "metadata": {"id": "c090262408", "question": "Can I share my answers of the Homework with the community to compare before I submit them?", "sort_order": 20}, "content": "We kindly ask you not to share your answers."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/general/100_7c7ec892e4_certificate-timeline.md", "metadata": {"id": "7c7ec892e4", "question": "Can I finish early and get the certificate in under four months?", "sort_order": 100}, "content": "No. Project timelines structure the cohort. The earliest certificates typically land around **January** (after Capstone 1/2 windows and reviews)."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/general/101_9d57199de9_skip-topics-optional-content.md", "metadata": {"id": "9d57199de9", "question": "Can I skip topics I already know?", "sort_order": 101}, "content": "Yes. All lesson content is optional; only the projects are mandatory. Move ahead at your own pace (you don’t need to wait for a “module start” date)."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/general/102_89fda350ee_live-or-recorded-lessons.md", "metadata": {"id": "89fda350ee", "question": "Are lessons live or recorded?", "sort_order": 102}, "content": "Core lessons are pre-recorded and already available. Occasional live streams (like the launch/Q&A) happen, but there are no weekly office hours at this time. For help, please use the recorded office hours from prior years and check the FAQ for related guidance."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/general/103_bc2e78caf3_slack-house-rules.md", "metadata": {"id": "bc2e78caf3", "question": "What are the Slack “house rules”?", "sort_order": 103}, "content": "Slack house rules:\n- Ask course questions in `#course-ml-zoomcamp` (not `#general`).\n- Use threads to reply.\n- Paste text/code instead of screenshots or phone photos.\n- Don’t tag instructors; many peers can help and instructors see messages anyway.\n- Keep the channel tidy and on-topic."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/general/104_3b78e09b80_uv-anaconda-env-tools.md", "metadata": {"id": "3b78e09b80", "question": "Which tools do we use for environments: uv or Anaconda?", "sort_order": 104}, "content": "- Install Python easily with Anaconda (especially on Windows).\n- Use uv for virtual environments and package installs (recommended over conda for this course).\n- The uv + FastAPI workshop is included in Module 5."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/general/105_0953e15dc6_cloud-provider-used-aws.md", "metadata": {"id": "0953e15dc6", "question": "Which cloud provider is used?", "sort_order": 105}, "content": "Examples use AWS (you should have or create an AWS account). Concepts transfer to other clouds (GCP/Azure) with minor adjustments."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/general/106_9198d40c17_system-design-included.md", "metadata": {"id": "9198d40c17", "question": "Is system design included?", "sort_order": 106}, "content": "There is no dedicated system-design module. The course emphasizes end-to-end ML projects and deployments (web services, Lambda, Kubernetes). You will learn by building and deploying models rather than focusing on standalone system-design concepts."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/general/107_9ffc454a24_leaderboard-advantage.md", "metadata": {"id": "9ffc454a24", "question": "Is there any advantage to high homework/leaderboard scores?", "sort_order": 107}, "content": "Indirectly. The leaderboard highlights active learners (correct answers + learning-in-public links). That visibility helps with networking and recruiter attention, even though the certificate itself is pass/fail."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/general/108_a84ccd240c_demonstrating-business-impact.md", "metadata": {"id": "a84ccd240c", "question": "How can I show business impact from ML projects? Does the course teach this?", "sort_order": 108}, "content": "The course focuses on building and deploying real ML services, which gives you practical, portfolio-worthy projects. Business impact depends on the domain context and is best learned on the job. By the end, you’ll have a visible, high-quality portfolio you can discuss with employers. For each project, consider the problem, approach, measurable outcomes, and a clear narrative to make the impact easy to communicate in interviews."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/general/109_a6f7295637_strategy-pivot-ml-ai.md", "metadata": {"id": "a6f7295637", "question": "What strategy do you recommend for an unemployed SWE pivoting to ML/AI soon?", "sort_order": 109}, "content": "- Ship projects and learn in public daily (posts, blogs, code).\n- Publish everything on GitHub, write about your approach/results, engage on Slack/LinkedIn, and build a network.\n- Use the course projects and optional activities (Kaggle, articles) as portfolio centerpieces."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/general/111_1e54b4b3ba_why-learn-traditional-ml.md", "metadata": {"id": "1e54b4b3ba", "question": "Why learn traditional ML if LLMs can do it for me?", "sort_order": 111}, "content": "Assistants are great accelerators, but you still need **conceptual understanding** to debug, adapt, and own your systems when AI tools make mistakes or hit limits.\n\nThe course teaches you to build, reason about, and deploy ML systems you control."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/misc/001_6aa17f1eab_why-do-i-need-to-provide-a-trainpy-file-when-i-alr.md", "metadata": {"id": "6aa17f1eab", "question": "Why do I need to provide a train.py file when I already have the notebook.ipynb file?", "sort_order": 1}, "content": "The `train.py` file will be used by your peers to review your midterm project. It is for them to cross-check that your training process works on someone else’s system. It should also be included in the environment in conda or with pipenv."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/misc/002_17eeab7562_loading-the-image-with-pillow-library-and-converti.md", "metadata": {"id": "17eeab7562", "question": "Loading the Image with PILLOW library and converting to numpy array", "sort_order": 2}, "content": "To load an image using the PILLOW library and convert it to a NumPy array, you can follow these steps:\n\n1. **Install the Pillow library:**\n   \n   ```bash\n   pip install pillow\n   ```\n\n2. **Use the following code to load an image and convert it:**\n\n   ```python\n   from PIL import Image\n   from numpy import asarray\n\n   # Open the image file\n   img = Image.open('aeroplane.png')\n\n   # Convert the image to a NumPy array\n   numdata = asarray(img)\n   ```"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/misc/003_b6d8547280_is-a-trainpy-file-necessary-when-you-have-a-traini.md", "metadata": {"id": "b6d8547280", "question": "Is a train.py file necessary when you have a train.ipynb file in your midterm project directory?", "sort_order": 3}, "content": "train.py has to be a Python file. This is because running a Python script for training a model is much simpler than running a notebook, and that's how training jobs usually look like in real life."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/misc/004_79d7bebfbf_is-there-a-way-to-serve-up-a-form-for-users-to-ent.md", "metadata": {"id": "79d7bebfbf", "question": "Is there a way to serve up a form for users to enter data for the model to crunch on?", "sort_order": 4}, "content": "Yes, you can create a mobile app or interface that manages these forms and validations. However, it is important to also perform validations on the backend.\n\nYou can also check Streamlit: [https://github.com/DataTalksClub/project-of-the-week/blob/main/2022-08-14-frontend.md](https://github.com/DataTalksClub/project-of-the-week/blob/main/2022-08-14-frontend.md)"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/misc/005_3e3a6ddca7_errno-12-cannot-allocate-memory-in-aws-elastic-con.md", "metadata": {"id": "3e3a6ddca7", "question": "[Errno 12] Cannot allocate memory in AWS Elastic Container Service", "sort_order": 5}, "content": "In the Elastic Container Service task log, error \"[Errno 12] Cannot allocate memory\" showed up.\n\nTo resolve this issue, increase the RAM and CPU in your task definition."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/misc/006_3e621509d2_pickle-error-cant-get-attribute-xxx-on-module-__ma.md", "metadata": {"id": "3e621509d2", "question": "Pickle error: can’t get attribute XXX on module __main__", "sort_order": 6}, "content": "When running a Docker container with Waitress serving the `app.py` for making predictions, you may encounter a pickle error: \"can't get attribute `<name_of_class>` on module __main__\".\n\nThis error does not occur when Flask is used directly, i.e., not through Waitress.\n\n**Cause**\n\nThe issue arises because the model uses a custom column transformer class. When the model was saved, it was saved from the `__main__` module (e.g., via `python train.py`). Pickle references the class in the global namespace (top-level code): `__main__.<custom_class>`.\n\nWhen using Waitress, it loads the `predict_app` module, and this calls `pickle.load`, which tries to find `__main__.<custom_class>`, but it does not exist in that namespace.\n\n**Solution**\n\n1. Move the custom class into a separate module.\n2. Import this module in both the script that saves the model (e.g., `train.py`) and the script that loads the model (e.g., `predict.py`).\n\n**Note:** If Flask is used (without Waitress) in `predict.py`, and `predict.py` defines the class, executing `python predict.py` will work because the class is in the same namespace as when the model was saved (`__main__`).\n\nFor more information, check out the [detailed explanation on Stack Overflow](https://stackoverflow.com/questions/27732354/unable-to-load-files-using-pickle-and-multiple-modules)."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/misc/007_091ce4f5ff_how-to-handle-outliers-in-a-dataset.md", "metadata": {"id": "091ce4f5ff", "question": "How to handle outliers in a dataset?", "sort_order": 7}, "content": "There are different techniques, but the most commonly used are the following:\n\n- **Dataset Transformation**: Apply transformations such as log transformation to normalize data.\n- **Clipping High Values**: Limit the range of data by capping extremes.\n- **Dropping Observations**: Remove the outlier observations from the dataset."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/misc/008_128c829dbd_reproducibility-do-we-have-to-run-everything.md", "metadata": {"id": "128c829dbd", "question": "Reproducibility: Do we have to run everything?", "sort_order": 8}, "content": "You are encouraged to run them if you can, as this provides another opportunity to learn from others. Not everyone will be able to run all the files, particularly the neural networks.\n\nAlternatively, ensure everything you need to reproduce is there: the dataset, the instructions, and check for any obvious errors."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/misc/009_3f5dbbd3b5_model-too-big.md", "metadata": {"id": "3f5dbbd3b5", "question": "Model too big", "sort_order": 9}, "content": "If your model is too big for GitHub, one option is to compress the model using `joblib`. For example:\n\n```python\njoblib.dump(model, model_filename, compress=('zlib', 6))\n```\n\nThis will use zlib to compress the model. Note that this process may take a few moments as the model is being compressed."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/misc/010_5a7cdb33f2_permissions-to-push-docker-to-google-container-reg.md", "metadata": {"id": "5a7cdb33f2", "question": "Permissions to push docker to Google Container Registry", "sort_order": 10}, "content": "When you try to push the Docker image to Google Container Registry and receive the message:\n\n```\nunauthorized: You don't have the needed permissions to perform this operation, and you may have invalid credentials.\n```\n\nFollow these steps:\n\n1. Install the Google Cloud SDK from [https://cloud.google.com/sdk/docs/install](https://cloud.google.com/sdk/docs/install).\n2. Run the following command in your console:\n\n   ```bash\n   gcloud auth configure-docker\n   ```"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/misc/011_424d68d110_error-when-running-imagedatageneratorflow_from_dat.md", "metadata": {"id": "424d68d110", "question": "Error when running ImageDataGenerator.flow_from_dataframe", "sort_order": 11}, "content": "Error: ImageDataGenerator name 'scipy' is not defined.\n\nTo resolve this issue:\n\n1. Ensure that `scipy` is installed in your environment:\n   \n   ```bash\n   pip install scipy\n   ```\n\n2. Restart the Jupyter kernel and try running the code again."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/misc/012_9db0f62601_error-unidentifiedimageerror-cannot-identify-image.md", "metadata": {"id": "9db0f62601", "question": "Error: UnidentifiedImageError: cannot identify image file", "sort_order": 12}, "content": "In deploying the model, I encountered an error while testing my model locally on a test-image data.\n\nThe initial command was:\n\n```python\nurl = '[GitHub](https://github.com/bhasarma/kitchenware-classification-project/blob/main/test-image.jpg')\n\nX = preprocessor.from_url(url)\n```\n\nThe error received:\n\n```python\nUnidentifiedImageError: cannot identify image file <_io.BytesIO object at 0x7f797010a590>\n```\n\n**Solution:**\n\n- Add `?raw=true` after `.jpg` in the URL. For example:\n\n  ```python\n  url = '[GitHub](https://github.com/bhasarma/kitchenware-classification-project/blob/main/test-image.jpg?raw=true)'\n  ```"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/misc/013_0f19932601_pipenvexceptionsresolutionfailure-warning-your-dep.md", "metadata": {"id": "0f19932601", "question": "[pipenv.exceptions.ResolutionFailure]: Warning: Your dependencies could not be resolved. You likely have a mismatch in your sub-dependencies", "sort_order": 13}, "content": "**Problem:** If you run `pipenv install` and get this message, it may indicate a mismatch in your sub-dependencies.\n\n**Solution:**\n\n1. You may need to manually update `Pipfile` and `Pipfile.lock`.\n2. Run the following command to resolve the dependency issues:\n\n   ```bash\n   pipenv lock\n   ```"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/misc/014_25ff7dd14c_error-decoding-json-response-expecting-value-line.md", "metadata": {"id": "25ff7dd14c", "question": "Error decoding JSON response: Expecting value: line 1 column 1 (char 0)", "sort_order": 14}, "content": "This problem occurs when contacting the server to send your predict-test data in the correct shape. The issue is that the input format to the model wasn't in the right shape.\n\nThe server receives data in JSON format (dict), which is not suitable for the model. You should convert it to a format like numpy arrays."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/misc/015_e855e1b1b8_free-cloud-alternatives.md", "metadata": {"id": "e855e1b1b8", "question": "Free cloud alternatives", "sort_order": 15}, "content": "I tried deploying my Docker image on Render, but it won't work. I get `SIGTERM` every time.\n\nI think 0.5GB RAM is not enough. Is there any other free alternative available?\n\nThere are several free alternatives:\n\n- **AWS (Amazon)** and **GCP (Google)**: Both offer free micro instances for an extended period along with additional free resources.\n- **Saturn Cloud**: Provides free GPU instances. Recent promotional information for Saturn Cloud is available for ML Zoomcamp participants:\n  \n  \"You can sign up here: [https://bit.ly/saturn-mlzoomcamp](https://bit.ly/saturn-mlzoomcamp)\n\n  When you sign up, write in the chat box that you're an ML Zoomcamp student and you should get extra GPU hours (around 150).\""}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/misc/016_7efbf0d444_chart-for-classes-and-predictions.md", "metadata": {"id": "7efbf0d444", "question": "Chart for classes and predictions", "sort_order": 16}, "content": "How to visualize the predictions per classes after training a neural net:\n\n```python\nclasses, predictions = zip(*dict(zip(classes, predictions)).items())\n\nplt.figure(figsize=(12, 3))\n\nplt.bar(classes, predictions)\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/misc/017_d2cdec8f35_convert-dictionary-values-to-dataframe-table.md", "metadata": {"id": "d2cdec8f35", "question": "Convert dictionary values to Dataframe table", "sort_order": 17}, "content": "You can convert the prediction output values to a DataFrame using the following code:\n\n```python\nimport pandas as pd\n\ndf = pd.DataFrame.from_dict(your_dict, orient='index', columns=['Prediction'])\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/misc/018_0156cf3b62_kitchenware-classification-competition-dataset-gen.md", "metadata": {"id": "0156cf3b62", "question": "Kitchenware Classification Competition Dataset Generator", "sort_order": 18}, "content": "The image dataset for the competition was in a different layout from what we used in the dino vs dragon lesson. Since that’s what was covered, some folks were more comfortable with that setup, so a script was written to generate it for them.\n\nIt can be found here: [kitchenware-dataset-generator | Kaggle](https://www.kaggle.com/code/clamytoe/kitchenware-dataset-generator)"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/misc/019_e6d04a94b1_cuda-toolkit-and-cudnn-install-for-tensorflow.md", "metadata": {"id": "e6d04a94b1", "question": "CUDA toolkit and cuDNN Install for Tensorflow", "sort_order": 19}, "content": "Install Nvidia drivers: [https://www.nvidia.com/download/index.aspx](https://www.nvidia.com/download/index.aspx).\n\nWindows:\n\n- Install Anaconda prompt: [https://www.anaconda.com/](https://www.anaconda.com/)\n- Two options:\n  1. Install package `tensorflow-gpu` in Anaconda.\n  2. Install Tensorflow [with pip](https://www.tensorflow.org/install/pip#windows-native)\n\nWSL/Linux:\n\n- WSL: Use the Windows Nvidia drivers, do not modify them.\n- Two options:\n  1. Install the Tensorflow [with pip](https://www.tensorflow.org/install/pip#linux_1)\n     - Make sure to follow step 4 to install CUDA by environment.\n     - Also run:\n\n     ```bash\n     echo 'export XLA_FLAGS=--xla_gpu_cuda_data_dir=$CONDA_PREFIX/lib/' > $CONDA_PREFIX/etc/conda/activate.d/env_vars.sh\n     ```\n  2. Install CUDA toolkit 11.x.x: [link](https://developer.nvidia.com/cuda-toolkit-archive)\n  \n- Install cuDNN: [link](https://developer.nvidia.com/rdp/cudnn-download)\n\nNow you should be able to perform training/inference with GPU in Tensorflow."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/misc/020_1a78bec215_i-may-end-up-submitting-the-assignment-late-would.md", "metadata": {"id": "1a78bec215", "question": "I may end up submitting the assignment late. Would it be evaluated?", "sort_order": 20}, "content": "Depends on whether the form will still be open. If it's open, you can submit your homework and it will be evaluated. If closed, it's too late."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/misc/021_5851324f04_does-the-github-repository-need-to-be-public.md", "metadata": {"id": "5851324f04", "question": "Does the GitHub repository need to be public?", "sort_order": 21}, "content": "Yes. Whoever corrects the homework will only be able to access the link if the repository is public."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/misc/022_b389358dc7_how-to-install-conda-environment-in-my-local-machi.md", "metadata": {"id": "b389358dc7", "question": "How to install Conda environment in my local machine?", "sort_order": 22}, "content": "You don’t install a conda environment. First, you create it, then you activate it.\n\n**Step 1: How to create a conda environment?**\n\nIn a terminal, write the command (ml-zoomcamp is the name of the environment):\n\n```bash\nconda create -n ml-zoomcamp\n```\n\n**Step 2: How to activate a conda environment?**\n\n```bash\nconda activate ml-zoomcamp\n```\n\nYou can verify that it worked if you see `(ml-zoomcamp)` prepended to your command prompt.\n\n**Note:**\n\nThe answer above assumes Anaconda has already been installed on your local machine. If this is not the case, you can download it from [Anaconda’s download page](https://www.anaconda.com/download). After installing it, you can verify it succeeded with the following command in a terminal:\n\n```bash\nconda --version\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/misc/023_840d7e7ee8_which-ide-is-recommended-for-machine-learning.md", "metadata": {"id": "840d7e7ee8", "question": "Which IDE is recommended for machine learning?", "sort_order": 23}, "content": "VSCode and Jupyter."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/misc/024_19e1e5c938_how-to-use-wget-with-google-colab.md", "metadata": {"id": "19e1e5c938", "question": "How to use wget with Google Colab?", "sort_order": 24}, "content": "To use `wget` in Google Colab, follow these steps:\n\n1. **Install wget**: Ensure that `wget` is installed by running the following command:\n   \n   ```bash\n   !which wget\n   ```\n\n2. **Download Data**: Use `wget` to download files by specifying the URL and destination path:\n   \n   ```bash\n   !wget -P /content/drive/My\\ Drive/Downloads/ URL\n   ```"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/misc/025_24713fbf0d_features-in-scikit-learn.md", "metadata": {"id": "24713fbf0d", "question": "Features in scikit-learn?", "sort_order": 25}, "content": "Features (X) must always be formatted as a 2-D array to be accepted by scikit-learn. Use `reshape` to convert a 1D array to a 2D array.\n\n```python\n# Example of reshaping a 1D array to a 2D array\nimport numpy as np\n\n# 1D array\narray_1d = np.array([1, 2, 3, 4, 5])\n\n# Reshape to a 2D array\narray_2d = array_1d.reshape(-1, 1)\nprint(array_2d)\n```\n\nAdditionally, when filtering and selecting specific columns in a DataFrame, you can use:\n\n```python\n# Filtering the DataFrame\ndf[df['ocean_proximity'].isin(['<1H OCEAN', 'INLAND'])]\n\n# Select only the desired columns\nselected_columns = [\n    'latitude',\n    'longitude',\n    'housing_median_age',\n    'total_rooms',\n    'total_bedrooms',\n    'population',\n    'households',\n    'median_income',\n    'median_house_value'\n]\n\nfiltered_df = filtered_df[selected_columns]\n\n# Display the first few rows of the filtered DataFrame\nprint(filtered_df.head())\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/misc/026_e5ceadc2f4_matplotlib-when-i-plotted-using-matplotlib-to-chec.md", "metadata": {"id": "e5ceadc2f4", "question": "Matplotlib: When I plotted using Matplotlib to check if the median has a tail, I got the error \"FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\". How can I bypass this?", "sort_order": 26}, "content": "To resolve this issue, you can try the following methods:\n\n1. **Upgrade Pandas:**\n   \n   You can resolve this by installing the latest version of Pandas. Execute the following command in a Jupyter code cell:\n   \n   ```bash\n   !pip install --upgrade pandas\n   ```\n\n2. **Suppress Warnings:**\n\n   If you prefer not to change your Pandas version, you can suppress the warnings in your code:\n   \n   ```python\n   import warnings\n   import pandas as pd\n   \n   # Suppress FutureWarning messages\n   warnings.simplefilter(action='ignore', category=FutureWarning)\n   ```"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/misc/027_2490b9e786_reproducibility-in-different-os.md", "metadata": {"id": "2490b9e786", "question": "Reproducibility in different OS", "sort_order": 27}, "content": "When trying to rerun the Docker file in Windows, as opposed to developing in WSL/Linux, I encountered the following error:\n\n```bash\nWarning: Python 3.11 was not found on your system…\n\nNeither ‘pipenv’ nor ‘asdf’ could be found to install Python.\n\nYou can specify specific versions of Python with:\n\n$ pipenv –python path\\to\\python\n```\n\nThe solution was to add the Python 3.11 installation folder to the PATH, restart the system, and run the Docker file again. This solved the error."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/misc/028_0b20afa7bb_deploying-to-digital-ocean.md", "metadata": {"id": "0b20afa7bb", "question": "Deploying to Digital Ocean", "sort_order": 28}, "content": "You may quickly deploy your project to DigitalOcean App Cloud. The process is relatively straightforward. The deployment costs about 5 USD/month. The container needs to be up until the end of the project evaluation.\n\nSteps:\n\n1. Register in DigitalOcean.\n2. Go to Apps -> Create App.\n3. Choose GitHub as a service provider.\n4. Edit Source Directory (if your project is not in the repo root).\n5. **IMPORTANT**: Go to settings -> App Spec and edit the Dockerfile path so it looks like `./project/Dockerfile` path relative to your repo root.\n6. Remember to add model files if they are not built automatically during the container build process."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/misc/029_3cc0ce528e_is-it-best-to-train-your-model-only-on-the-most-im.md", "metadata": {"id": "3cc0ce528e", "question": "Is it best to train your model only on the most important features?", "sort_order": 29}, "content": "Not necessarily. While some features may show higher importance, it's essential to consider the predictive value of all features. Here are a few guidelines:\n\n- **Evaluate Predictive Value**: Include features that offer additional predictive value. Test your model with and without certain features. If excluding a feature decreases performance, it should be retained.\n- **Correlation Consideration**: Some important features might be highly correlated with others. It may be fine to drop some correlated features if they do not improve model performance.\n- **Feature Selection Algorithms**: Consider using feature selection methods like L1 regularization (Lasso), which implicitly selects features by shrinking some weights to zero.\n\nRefer to the lessons in week 3 of the churn prediction project for more insights, especially around feature importance for categorical values. Specifically, lesson 3.6 discusses mutual info scores, and lesson 3.10 demonstrates training a Logistic Regression model on all categorical variables."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/misc/030_7f18a34896_how-can-i-work-with-very-large-datasets-eg-the-new.md", "metadata": {"id": "7f18a34896", "question": "How can I work with very large datasets, e.g. the New York Yellow Taxi dataset, with over a million rows?", "sort_order": 30}, "content": "You can consider several different approaches:\n\n- **Sampling**: In the exploratory phase, you can use random samples of the data.\n\n- **Chunking**: When you do need all the data, you can read and process it in chunks that do fit in the memory.\n\n- **Optimizing data types**: Pandas’ automatic data type inference (when reading data in) might result in, e.g., `float64` precision being used to represent integers, which wastes space. You might achieve substantial memory reduction by optimizing the data types.\n\n- **Using Dask**: An open-source Python project which parallelizes Numpy and Pandas.\n\nSee, e.g., [this blog on Vantage AI](https://www.vantage-ai.com/en/blog/4-strategies-how-to-deal-with-large-datasets-in-pandas)"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/misc/031_165122776e_can-i-do-the-course-in-other-languages-like-r-or-s.md", "metadata": {"id": "165122776e", "question": "Can I do the course in other languages, like R or Scala?", "sort_order": 31}, "content": "Technically, yes. Advisable? Not really. Here are the reasons:\n\n- Some homework assignments require specific Python library versions.\n- Answers may not align with multiple-choice questions if using languages other than Python 3.10 (the recommended version for the 2023 cohort).\n- For midterms or capstones, your peer-reviewers may not know these other languages, which could lead to issues in scoring and feedback.\n- While you can create a separate repository using the course lessons in other languages for personal learning, it is not recommended for official submissions."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/misc/032_29ef71cc59_is-use-of-libraries-like-fastaihttpfastai-or-huggi.md", "metadata": {"id": "29ef71cc59", "question": "Is use of libraries like [fast.ai](http://fast.ai/) or Huggingface allowed in the capstone and competition, or are they considered to be \"too much help\"?", "sort_order": 32}, "content": "Yes, it’s allowed."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/misc/033_c04524a66d_docker-flask-image-was-built-and-tested-successful.md", "metadata": {"id": "c04524a66d", "question": "Docker: Flask image was built and tested successfully, but tensorflow serving image was built and unable to test successfully. What could be the problem?", "sort_order": 33}, "content": "The TF and TF Serving versions have to match.\n\nFor Module 10.3, if you are on Apple Silicon and you encounter the following error when trying to run TF-Serving locally with Docker:\n\n```bash\n/usr/bin/tf_serving_entrypoint.sh: line 3:     7 Illegal instruction     tensorflow_model_server --port=8500 --rest_api_port=8501 --model_name=${MODEL_NAME} --model_base_path=${MODEL_BASE_PATH}/${MODEL_NAME} \"$@\"\n```\n\nYou may find a solution in this [GitHub comment](https://github.com/tensorflow/serving/issues/1816#issuecomment-2445056791).\n\nDocker release 4.35.0 for Mac introduces Docker VMM Beta, a replacement for the Apple Virtualisation Framework using Rosetta. You can now run the native TF Serving image."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/misc/034_d767e50478_any-advice-for-adding-the-machine-learning-zoomcam.md", "metadata": {"id": "d767e50478", "question": "Any advice for adding the Machine Learning Zoomcamp experience to your LinkedIn profile?", "sort_order": 34}, "content": "I've seen LinkedIn users list DataTalksClub as Experience with titles such as:\n\n- Machine Learning Fellow\n- Machine Learning Student\n- Machine Learning Participant\n- Machine Learning Trainee\n\nIt is advised not to list this experience as an official job or internship since DataTalksClub did not hire or financially compensate you.\n\nConsider incorporating the experience in the following sections:\n\n- Organizations\n- Projects\n- Skills\n- Featured\n- Original posts\n- Certifications\n- Courses\n\nInteresting suggestion: Add the link of your project to your CV as a showcase and make posts to show your progress."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/misc/035_b75fa5280e_how-to-install-extras-packages-on-google-colab-or.md", "metadata": {"id": "b75fa5280e", "question": "How to install extras packages on Google Colab or Kaggle?", "sort_order": 35}, "content": "To install extra packages on Google Colab or Kaggle, you can use the following methods:\n\n1. **Using PIP:**\n   \n   Execute the following command in a cell:\n   \n   ```bash\n   !pip install tensorflow[and-cuda]==2.14\n   ```\n\n2. **Using Conda:**\n   \n   You can also use Conda commands. For example:\n   \n   ```bash\n   !conda install pandas --yes\n   ```\n   \n   The option `--yes` allows the installation to proceed automatically when you see the \"Proceed ([y]/n)?\" message."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/misc/036_e53752c5b7_if-you-are-working-in-the-terminal-on-your-compute.md", "metadata": {"id": "e53752c5b7", "question": "If you are working in the terminal on your computer in WSL and you want to go to the directory in Explorer to upload to GitHub, what command should you use?", "sort_order": 36}, "content": "Use the following command:\n\n```bash\nexplorer.exe .\n```\n\nThis command opens the current directory in Windows Explorer.\n\nAlternatively, you can sync through VSCode to GitHub."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-1-homework/001_b7e73fcee7_floating-point-precision.md", "metadata": {"id": "b7e73fcee7", "question": "Floating Point Precision", "sort_order": 1}, "content": "I was doing Question 7 from Week 1 Homework and with step 6: Invert XTX. I created the inverse. Now, an inverse when multiplied by the original matrix should return an identity matrix. But when I multiplied the inverse with the original matrix, it gave a matrix like this:\n\nInverse * Original:\n\n```\n[[ 1.00000000e+00 -1.38777878e-16]\n [ 3.16968674e-13  1.00000000e+00]]\n```\n\nSolution:\n\nIt's because floating point math doesn't work well on computers as shown here: [https://stackoverflow.com/questions/588004/is-floating-point-math-broken](https://stackoverflow.com/questions/588004/is-floating-point-math-broken)"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-1-homework/002_b72a4be56d_how-to-avoid-value-errors-with-array-shapes-in-hom.md", "metadata": {"id": "b72a4be56d", "question": "How to avoid Value errors with array shapes in homework?", "sort_order": 2}, "content": "First of all, use `np.dot` for matrix multiplication. When you perform matrix-matrix multiplication, remember that the order of multiplication is crucial and affects the result.\n\n**Dimension Mismatch**\n\nTo perform matrix multiplication, the number of columns in the first matrix should match the number of rows in the second matrix. Rearrange the order to satisfy this condition."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-1-homework/003_4115da8cbc_homework-q5-how-to-replace-nans-with-the-average.md", "metadata": {"id": "4115da8cbc", "question": "Homework Q5: How to replace NaNs with the average?", "sort_order": 3}, "content": "You would first get the average of the column and save it to a variable, then replace the NaN values with the average variable.\n\nThis method is called imputing - when you have NaN/null values in a column, but you do not want to get rid of the row because it has valuable information contributing to other columns."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-1-homework/004_22e80c8b22_homework-q5-why-the-mode-returns-a-series-instead.md", "metadata": {"id": "22e80c8b22", "question": "Homework Q5: Why the mode returns a Series instead of a single value?", "sort_order": 4}, "content": "When you calculate the mode using the `mode()` function in pandas, the function always returns a Series. This design choice allows `mode()` to handle cases where there may be multiple modes (i.e., multiple values with the same highest frequency). Even when there is only one mode, the function will still return a Series with that single value.\n\nIf you are certain that your column has only one mode and you want to extract it as a single value, you can access the first element of the Series returned by `mode()`:\n\n```python\nsingle_mode_value = your_dataframe['your_column'].mode()[0]\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-1-homework/005_061c36bce4_question-7-mathematical-formula-for-linear-regress.md", "metadata": {"id": "061c36bce4", "question": "Question 7: Mathematical formula for linear regression", "sort_order": 5}, "content": "In Question 7, we are asked to calculate the following:\n\n- `X^T X`\n- `(X^T X)^{-1}`\n- `w = (X^T X)^{-1} X^T y`\n\nThe initial problem `w = X^{-1} y` can be solved by this, where a matrix `X` is multiplied by some unknown weights `w` resulting in the target `y`.\n\nAdditional Reading and Videos\n\n- [Ordinary least squares](https://en.wikipedia.org/wiki/Ordinary_least_squares)\n- [Multiple Linear Regression in Matrix Form](https://www.youtube.com/watch?v=jZ_Hq-7ifk8)\n- [Pseudoinverse Solution to OLS](https://en.wikipedia.org/wiki/Ordinary_least_squares)"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-1-homework/006_6a9d87d7d1_homework-q7-final-multiplication-not-having-5-colu.md", "metadata": {"id": "6a9d87d7d1", "question": "Homework Q7: Final multiplication not having 5 column", "sort_order": 6}, "content": "This is most likely because you interchanged the first step of the multiplication.\n\nEnsure you use:\n\n- Correct: \n  ```\n  XTX = XT X\n  ```\n- Instead of incorrect: \n  ```\n  XTX = X XT\n  ```"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-1-homework/007_3ef50b0d88_homework-q7-multiplication-operators.md", "metadata": {"id": "3ef50b0d88", "question": "Homework Q7: Multiplication operators.", "sort_order": 7}, "content": "Matrix multiplication, such as matrix-matrix or matrix-vector multiplication, is often represented using the `*` operator. However, in NumPy, it is performed using the `@` operator or `np.matmul()`. The `*` operator in NumPy is used for element-wise multiplication, also known as the [Hadamard product](https://en.wikipedia.org/wiki/Hadamard_product_(matrices)).\n\nFor matrix-matrix multiplication, using the `@` operator or `np.matmul()` is preferred, as noted in the [NumPy documentation](https://numpy.org/doc/stable/reference/generated/numpy.dot.html#numpy.dot).\n\nWhen multiplying by a scalar, it's preferred to use `numpy.multiply()` or the `*` operator.\n\nReferences:\n- [numpy.dot()](https://numpy.org/doc/stable/reference/generated/numpy.dot.html)\n- [ndarray.dot()](https://numpy.org/doc/1.21/reference/generated/numpy.ndarray.dot.html)"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-1/001_6698c6cbd4_wget-is-not-recognized-as-an-internal-or-external.md", "metadata": {"id": "6698c6cbd4", "question": "wget is not recognized as an internal or external command", "sort_order": 1}, "content": "If you encounter the error \"wget is not recognized as an internal or external command\", you need to install it.\n\n- **On Ubuntu, run:**\n  \n  ```bash\n  sudo apt-get install wget\n  ```\n\n- **On Windows, you can use Chocolatey:**\n  \n  ```bash\n  choco install wget\n  ```\n  \n  Or download a binary [from here](http://gnuwin32.sourceforge.net/packages/wget.htm) and add it to your PATH (e.g., `C:/tools/`).\n\n- **On Mac, use Homebrew:**\n  \n  ```bash\n  brew install wget\n  ```\n\nAlternatively, you can use Python libraries:\n\n- **Python `wget` library:**\n\n  Install it first:\n  \n  ```bash\n  pip install wget\n  ```\n\n  Then, in your Python code:\n  \n  ```python\n  import wget\n\n  wget.download(\"URL\")\n  ```\n\n- **Using `pandas` to read a CSV directly from a URL:**\n\n  ```python\n  import pandas as pd\n  \n  url = \"https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\"\n  \n  df = pd.read_csv(url)\n  ```\n\n  Valid URL schemes include http, ftp, s3, gs, and file.\n\n- **Bypassing HTTPS checks (if needed):**\n\n  ```python\n  import ssl\n  \n  ssl._create_default_https_context = ssl._create_unverified_context\n  ```\n\n- **Using Python's `urllib` for downloading files:**\n\n  ```python\n  import urllib.request\n  \n  url = \"https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\"\n  \n  urllib.request.urlretrieve(url, \"housing.csv\")\n  ```\n\n  The `urlretrieve()` function allows you to download files from URLs and save them locally. It is part of the standard Python library `urllib.request`, available on all devices and platforms."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-1/002_f3a06ec752_downloading-a-csv-file-inside-notebook.md", "metadata": {"id": "f3a06ec752", "question": "Downloading a csv file inside notebook", "sort_order": 2}, "content": "The best way is to use pandas and give it the URL directly:\n\n```python\nurl = 'https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv'\ndf = pd.read_csv(url)\n```\n\nYou can also execute cmd/bash commands inside Jupyter:\n\n```bash\n!wget https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\n```\n\nThe exclamation mark `!` lets you execute shell commands inside your notebooks. This works for shell commands such as `ls`, `cp`, `mkdir`, `mv`, etc.\n\nFor instance, if you then want to move your data into a data directory alongside your notebook-containing directory, you could execute the following:\n\n```bash\n!mkdir -p ../data/\n!mv housing.csv ../data/\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-1/003_5ea323b01d_windows-wsl-and-vs-code.md", "metadata": {"id": "5ea323b01d", "question": "Windows: WSL and VS Code", "sort_order": 3}, "content": "If you have a Windows 11 device and would like to use the built-in WSL to access Linux, you can use the Microsoft Learn link [Set up a WSL development environment | Microsoft Learn](https://learn.microsoft.com/en-us/windows/wsl/setup/environment).\n\nTo connect this to VS Code, download the Microsoft verified VS Code extension ‘WSL’. This will allow you to remotely connect to your WSL Ubuntu instance as if it were a virtual machine."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-1/004_f3423f5014_uploading-the-homework-to-github.md", "metadata": {"id": "f3423f5014", "images": [{"description": "image #1", "id": "image_1", "path": "images/machine-learning-zoomcamp/image_b1bb532e.png"}, {"description": "image #2", "id": "image_2", "path": "images/machine-learning-zoomcamp/image_5921040c.jpg"}, {"description": "image #3", "id": "image_3", "path": "images/machine-learning-zoomcamp/image_c812cf91.png"}], "question": "Uploading the homework to Github", "sort_order": 4}, "content": "If you encounter the following error when trying to use Git for the first time:\n\n```bash\nerror: src refspec master does not match any\nerror: failed to push some refs to 'https://github.com/XXXXXX/1st-Homework.git'\n```\n\nSolution:\n\n1. Make an initial commit using:\n   \n   ```bash\n   git commit -m \"initial commit\"\n   ```\n\n2. Push to the main branch instead:\n   \n   ```bash\n   git push origin main\n   ```\n\nFor a comprehensive guide on using GitHub, visit [GitHub Quickstart](https://dennisivy.com/github-quickstart).\n\nYou can also use GitHub's \"upload file\" feature, or share your Google Colab notebooks directly to GitHub:\n\n<{IMAGE:image_1}>\n\n<{IMAGE:image_2}>\n\n<{IMAGE:image_3}>"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-1/005_0be9e7cc3d_singular-matrix-error.md", "metadata": {"id": "0be9e7cc3d", "question": "Singular Matrix Error", "sort_order": 5}, "content": "I'm trying to invert the matrix but I got an error that the matrix is singular.\n\nThe singular matrix error is caused because not every matrix can be inverted. In particular, it happens when dealing with multiplication (using the method `.dot`) since multiplication is not commutative. `X.dot(Y)` is not necessarily equal to `Y.dot(X)`. Respect the order; otherwise, you get the wrong matrix."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-1/006_694ecdf4e8_conda-is-not-an-internal-command.md", "metadata": {"id": "694ecdf4e8", "question": "Conda is not an internal command", "sort_order": 6}, "content": "If you encounter an issue where the command:\n\n```bash\nconda create -n ml-zoomcamp python=3.13\n```\n\ndoesn't work, try the following solutions:\n\n1. **Use Anaconda Prompt:**\n   - If you are on Windows, use Anaconda's own terminal called \"Anaconda Prompt\". This should automatically configure your environment to recognize Conda commands.\n\n2. **Install Anaconda or Miniconda:**\n   - If you do not have Anaconda or Miniconda installed, download and install one of them first.\n\nNote: Any version of Python starting from 3.10 should be suitable."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-1/007_5e953f0e8e_read-in-the-file-in-windows-os.md", "metadata": {"id": "5e953f0e8e", "question": "Read-in the File in Windows OS", "sort_order": 7}, "content": "How do I read the dataset with Pandas in Windows?\n\nI used the code below but it's not working:\n\n```python\ndf = pd.read_csv('C:\\Users\\username\\Downloads\\data.csv')\n```\n\nUnlike Linux/Mac OS, Windows uses the backslash (`\\`) to navigate the files, which causes a conflict with Python. In Python, the `\\` is used for escape sequences, e.g., `\\n` for a new line or `\\t` for a tab. To avoid this issue, add an `r` before the file path to treat it as a raw string:\n\n```python\ndf = pd.read_csv(r'C:\\Users\\username\\Downloads\\data.csv')\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-1/008_cc0048eefb_403-forbidden-error-message-when-you-try-to-push-t.md", "metadata": {"id": "cc0048eefb", "question": "'403 Forbidden' error message when you try to push to a GitHub repository", "sort_order": 8}, "content": "To resolve a '403 Forbidden' error when pushing to a GitHub repository, follow these steps:\n\n1. Check the current remote URL configuration by running:\n\n   ```bash\n   git config -l | grep url\n   ```\n\n   The output should be similar to:\n\n   ```\n   remote.origin.url=https://github.com/github-username/github-repository-name.git\n   ```\n\n2. Change the URL format to include your GitHub username:\n\n   ```bash\n   git remote set-url origin \"https://github-username@github.com/github-username/github-repository-name.git\"\n   ```\n\n3. Verify the change is reflected using the command in step 1. Make sure the URL is correctly updated."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-1/009_261ca9cbab_git-fatal-authentication-failed-for-httpsgithubcom.md", "metadata": {"id": "261ca9cbab", "question": "Git: Fatal: Authentication failed for https://github.com/username", "sort_order": 9}, "content": "I encountered a problem when trying to push code from Git Bash:\n\n```\nremote: Support for password authentication was removed on August 13, 2021.\nremote: Please see https://docs.github.com/en/get-started/getting-started-with-git/about-remote-repositories#cloning-with-https-urls for information on currently recommended modes of authentication.\nfatal: Authentication failed for 'https://github.com/username'\n```\n\n**Solution:**\n\n1. Create a personal access token from your GitHub account.\n2. Use this token to authenticate when you push your changes.\n\nFor more details, see the documentation on [generating a new SSH key and adding it to the SSH agent](https://docs.github.com/en/authentication/connecting-to-github-with-ssh/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent)."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-1/010_ba250275a3_kaggle-wget-unable-to-resolve-host-address-rawgith.md", "metadata": {"id": "ba250275a3", "images": [{"description": "image #1", "id": "image_1", "path": "images/machine-learning-zoomcamp/image_16b33640.png"}], "question": "Kaggle: wget: unable to resolve host address raw.githubusercontent.com", "sort_order": 10}, "content": "In Kaggle, when you attempt to `!wget` a dataset from GitHub or any other public repository, you might encounter the following error:\n\n```\n--2022-09-17 16:55:24--  https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... failed: Temporary failure in name resolution.\nwget: unable to resolve host address 'raw.githubusercontent.com'\n```\n\n### Solution:\n\n- In your Kaggle notebook settings, enable internet access for your session. This option is found in the settings panel on the right-hand side of the Kaggle screen.\n- You will need to verify your phone number to confirm you are not a bot.\n\n<{IMAGE:image_1}>"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-1/011_bad28cd4e0_setting-up-an-environment-using-vs-code.md", "metadata": {"id": "bad28cd4e0", "question": "Setting up an environment using VS Code", "sort_order": 11}, "content": "I found this video quite helpful: [Creating Virtual Environment for Python from VS Code](https://www.youtube.com/watch?v=8h9w0meM8i4)\n\n**Native Jupyter Notebooks Support in VS Code**\n\nIn VS Code, you can have native Jupyter Notebooks support, i.e., you do not need to open a web browser to code in a Notebook. If you have port forwarding enabled, run a `jupyter notebook` command from a remote machine, and have a remote connection configured in `.ssh/config` (as Alexey’s [video](https://www.youtube.com/watch?v=IXSiYkP23zo) suggests), VS Code can execute remote Jupyter Notebook files on a remote server from your local machine: [Visual Studio Code Jupyter Notebooks](https://code.visualstudio.com/docs/datascience/jupyter-notebooks).\n\n**Git Support in VS Code**\n\nYou can work with GitHub from VS Code. Staging and commits are easy from the VS Code’s UI:\n\n[Visual Studio Code Source Control Overview](https://code.visualstudio.com/docs/sourcecontrol/overview)"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-1/012_a8563f2c5c_port-forwarding-with-ssh.md", "metadata": {"id": "a8563f2c5c", "question": "Port-Forwarding with SSH", "sort_order": 12}, "content": "If you prefer using the terminal for port forwarding, configure it in your SSH config file.\n\n1. Open your SSH config file:\n   ```bash\n   nano ~/.ssh/config\n   ```\n\n2. Add the following line to forward your Jupyter server:\n   ```\n   LocalForward 8888 localhost:8888\n   ```"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-1/013_7bcbd64ac9_conda-environment-setup-do-we-need-to-run-conda-cr.md", "metadata": {"id": "7bcbd64ac9", "question": "Conda Environment Setup: Do we need to run 'conda create' and 'conda activate' every time?", "sort_order": 13}, "content": "To set up a Conda environment for the project:\n\n- **Initial Setup**: Run the following command only once to create the environment:\n  \n  ```bash\n  conda create -n ml-zoomcamp\n  ```\n\n- **Activating Environment**: Each time you want to work on the project, activate the environment:\n  \n  ```bash\n  conda activate ml-zoomcamp\n  ```\n\n- **Exporting Environment**: To export your existing environment to a YAML file:\n  \n  ```bash\n  conda env export > environment.yml\n  ```\n\n- **Recreating Environment**: Use the YAML file to recreate the environment:\n  \n  ```bash\n  conda env create -f environment.yml\n  ```"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-1/014_e74369cf00_what-does-pandasdataframeinfo-do.md", "metadata": {"id": "e74369cf00", "question": "What does pandas.DataFrame.info() do?", "sort_order": 14}, "content": "It prints the information about the dataset, including:\n\n- Index datatype\n- Number of entries\n- Column information with not-null count and datatype\n- Memory usage by the dataset\n\nWe use it as:\n\n```python\ndf.info()\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-1/015_fa28f60a1b_nameerror-name-np-is-not-defined.md", "metadata": {"id": "fa28f60a1b", "question": "NameError: name 'np' is not defined", "sort_order": 15}, "content": "If you're using `numpy` or `pandas`, make sure to import the libraries before using them:\n\n```python\nimport pandas as pd\nimport numpy as np\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-1/016_eecebb4f2f_how-to-select-column-by-dtype.md", "metadata": {"id": "eecebb4f2f", "question": "How to select column by dtype", "sort_order": 16}, "content": "To select columns by data type, you can use the following methods:\n\n- To get columns with numeric data:\n\n  ```python\n  df.select_dtypes(include=np.number).columns.tolist()\n  ```\n  \n- To get columns with object (string) data:\n\n  ```python\n  df.select_dtypes(include='object').columns.tolist()\n  ```"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-1/017_cda76de4e9_how-to-identify-the-shape-of-dataset-in-pandas.md", "metadata": {"id": "cda76de4e9", "question": "How to identify the shape of dataset in Pandas", "sort_order": 17}, "content": "To identify the shape of a dataset in Pandas, you can use the `.shape` attribute:\n\n- `df.shape`: Returns a tuple representing the dimensionality of the DataFrame.\n- `df.shape[0]`: Returns the number of rows.\n- `df.shape[1]`: Returns the number of columns.\n\nYou can also use the built-in `len` function to find the total number of rows:\n\n```python\nlen(df)\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-1/018_a5caeac2b7_error-launching-jupyter-notebook.md", "metadata": {"id": "a5caeac2b7", "question": "Error launching Jupyter notebook", "sort_order": 18}, "content": "If you encounter the error below when launching a Jupyter notebook in a new environment:\n\n```bash\nImportError: cannot import name 'contextfilter' from 'jinja2' (anaconda\\lib\\site-packages\\jinja2\\__init__.py)\n```\n\nFollow these steps:\n\n1. Switch to the main environment.\n2. Run the following command:\n\n   ```bash\n   pip install nbconvert --upgrade\n   ```"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-1/019_16af61862a_wget-hangs-on-macos-ventura-m1.md", "metadata": {"id": "16af61862a", "question": "wget hangs on MacOS Ventura M1", "sort_order": 19}, "content": "Executing the following command hangs on MacOS Ventura M1:\n\n```bash\nwget https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\n```\n\nIf you encounter this and see IPv6 addresses in the terminal, follow these steps:\n\n1. Go to **System Settings**.\n2. Select **Network**.\n3. Choose your network connection and click **Details**.\n4. Set **Configure IPv6** to **Manually**.\n5. Click **OK**.\n6. Try the command again."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-1/020_ea4dbd6c83_using-macos-and-having-trouble-with-wget.md", "metadata": {"id": "ea4dbd6c83", "question": "Using macOS and having trouble with WGET", "sort_order": 20}, "content": "Wget doesn't ship with macOS, but you can use `curl` as an alternative.\n\nExample command:\n\n```bash\ncurl -o ./housing.csv https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\n```\n\n### Explanation:\n\n- **curl**: A utility for retrieving information from the internet.\n- **-o**: Specifies the output filename for the file being downloaded.\n- **filename**: Your choice for naming the file.\n- **URL**: The web address from which `curl` will download the data and save it using the specified filename.\n\nFor more information, you can refer to the [Curl Documentation](https://curl.se/docs/manpage.html)"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-1/021_59fc88dea8_how-to-output-only-a-certain-number-of-decimal-pla.md", "metadata": {"id": "59fc88dea8", "question": "How to output only a certain number of decimal places", "sort_order": 21}, "content": "You can use the `round()` function or f-strings:\n\n- **Using `round()` function:**\n  ```python\n  round(number, 4)  # This will round number up to 4 decimal places\n  ```\n\n- **Using f-strings:**\n  ```python\n  print(f'Average mark for the Homework is {avg:.3f}')  # Formats the number to 3 decimal places\n  ```\n\n- **Using `pandas.Series.round` if you need to round values in a whole Series:**\n  \n  See the documentation for more information:\n  [pandas.Series.round](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.round.html#pandas.Series.round)"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-1/022_5ff9126a13_cant-get-juypter-running-locally-on-your-machine.md", "metadata": {"id": "5ff9126a13", "question": "Can’t get Jupyter running locally on your machine?", "sort_order": 22}, "content": "If you’re struggling to get a Jupyter notebook running locally on your machine or have other issues (like only having a cellphone available), consider using the following online platforms that don't require installation:\n\n- [JupyterLab](https://jupyter.org/try-jupyter/lab/)\n- [JupyterLite](https://jupyterlite.github.io/demo/lab/index.html)\n- [Replit](https://replit.com/)\n- [Google Colab](https://colab.research.google.com/)"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-1/023_68af9cffed_start-module-1-now.md", "metadata": {"id": "68af9cffed", "question": "Can I start Module 1 now and move ahead faster?", "sort_order": 23}, "content": "Yes. Lessons are already available. You can proceed to later modules without waiting for official start dates."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-10/001_0a166909a0_what-tools-are-recommended-for-setting-up-a-local.md", "metadata": {"id": "0a166909a0", "question": "What tools are recommended for setting up a local Kubernetes environment for model deployment practice?", "sort_order": 1}, "content": "Several tools can help set up a local Kubernetes environment:\n\n- **Kind**: Runs Kubernetes clusters in Docker containers, suitable for testing and development.\n- **Minikube**: Runs a single-node Kubernetes cluster on your local machine.\n- **K3s**: A lightweight Kubernetes distribution ideal for local development.\n- **MicroK8s**: A minimal Kubernetes distribution for local development.\n- **Docker Desktop**: Includes a standalone Kubernetes server and client for development."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-10/002_getting-allocator-ran-out-of.md", "metadata": {"id": "4693802714", "question": "Allocator (GPU_0_bfc) ran out of memory", "sort_order": 2}, "content": "If you are running TensorFlow on your own machine and you start getting the following errors:\n\n```\nAllocator (GPU_0_bfc) ran out of memory trying to allocate 6.88GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n```\n\nTry adding this code in a cell at the beginning of your notebook:\n\n```python\nconfig = tf.compat.v1.ConfigProto()\nconfig.gpu_options.allow_growth = True\nsession = tf.compat.v1.Session(config=config)\n```\n\nAfter doing this, most issues should be resolved. Occasionally, the error may still appear during high-demand epochs, but re-running the code should typically resolve it."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-10/003_c25b3cc5fe_problem-with-recent-version-of-protobuf.md", "metadata": {"id": "c25b3cc5fe", "question": "Problem with recent version of protobuf", "sort_order": 3}, "content": "In session 10.3, when creating the virtual environment with pipenv and trying to run the script `gateway.py`, you might encounter the following error:\n\n```plaintext\nTypeError: Descriptors cannot not be created directly.\n\nIf this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\n```\n\nIf you cannot immediately regenerate your protos, consider these workarounds:\n\n1. **Downgrade the protobuf package** to 3.20.x or lower.\n2. **Set the environment variable**:\n   \n   ```bash\n   PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python\n   ```\n   \n   This will use pure-Python parsing but may be slower.\n\nFor more information, visit [developers.google.com](https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates).\n\nThis issue occurs with newer versions of protobuf. As a workaround, you can fix the protobuf version to an older one. Here's a command that addresses this issue:\n\n```bash\npipenv install --python 3.9.13 requests grpcio==1.42.0 flask gunicorn \\\nkeras-image-helper tensorflow-protobuf==2.7.0 protobuf==3.19.6\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-10/004_12d79208b3_wsl-cannot-connect-to-docker-daemon.md", "metadata": {"id": "12d79208b3", "images": [{"description": "image #1", "id": "image_1", "path": "images/machine-learning-zoomcamp/image_41f796fe.png"}], "question": "WSL: Cannot Connect To Docker Daemon", "sort_order": 4}, "content": "Due to machine uncertainties, you might encounter the following error when trying to run a Docker command:\n\n```bash\nCannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?\n```\n\n**Solution:**\n\nThe issue may arise if Docker Desktop is not correctly connecting to the WSL Linux distribution. To resolve this:\n\n1. Open Docker Desktop settings.\n2. Navigate to the \"Resources\" section.\n3. Click on \"WSL Integration.\"\n\n   <{IMAGE:image_1}>\n\n4. Enable additional distros, even if it matches the default WSL distro.\n\nThat's all you need to do."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-10/005_b2e624ea5a_hpa-instance-doesnt-run-properly.md", "metadata": {"id": "b2e624ea5a", "question": "HPA instance doesn’t run properly", "sort_order": 5}, "content": "If the HPA instance does not run correctly even after installing the latest version of Metrics Server from the `components.yaml` manifest with:\n\n```bash\nkubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml\n```\n\nAnd the targets still appear as `<unknown>`, follow these steps:\n\n1. Run the following command to edit the deployment:\n   \n   ```bash\n   kubectl edit deploy -n kube-system metrics-server\n   ```\n   \n2. Search for the line:\n   \n   ```yaml\n   args:\n   - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname\n   ```\n   \n3. Add the following line in the middle:\n   \n   ```yaml\n   - --kubelet-insecure-tls\n   ```\n   \n   So it looks like this:\n\n   ```yaml\n   args:\n   - --kubelet-insecure-tls\n   - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname\n   ```\n\n4. Save the changes and run:\n\n   ```bash\n   kubectl get hpa\n   ```"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-10/006_f8533d5ffd_hpa-instance-doesnt-run-properly-easier-solution.md", "metadata": {"id": "f8533d5ffd", "question": "HPA instance doesn’t run properly (easier solution)", "sort_order": 6}, "content": "If the HPA instance does not run correctly even after installing the latest version of Metrics Server from the components.yaml manifest with:\n\n```bash\nkubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml\n```\n\nAnd the targets still appear as `<unknown>`, run the following command:\n\n```bash\nkubectl apply -f https://raw.githubusercontent.com/Peco602/ml-zoomcamp/main/10-kubernetes/kube-config/metrics-server-deployment.yaml\n```\n\nThis uses a metrics server deployment file already embedding the `--kubelet-insecure-tls` option."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-10/007_bbced12feb_pods-not-starting.md", "metadata": {"id": "bbced12feb", "question": "Pods not starting", "sort_order": 7}, "content": "This issue can be caused by several factors:\n\n- **Resource Allocation:** Ensure that your Pods have enough CPU and memory resources allocated. If resources are too low, the Kubernetes scheduler might fail to schedule your Pods.\n\n- **Image Issues:** Verify that the Docker image specified for your Pod is correctly built and accessible. If the image cannot be pulled from the repository, the Pod won’t start."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-10/008_bcedbb12fe_could-not-install-packages-due-to-an-oserror-winer.md", "metadata": {"id": "bcedbb12fe", "question": "Could not install packages due to an OSError: [WinError 5] Access is denied", "sort_order": 8}, "content": "When I ran the command:\n\n```bash\npip install grpcio==1.42.0 tensorflow-serving-api==2.7.0\n```\n\nto install the libraries on a Windows machine, I encountered the following error:\n\n```\nERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\Users\\Asia\\anaconda3\\Lib\\site-packages\\google\\protobuf\\internal\\_api_implementation.cp39-win_amd64.pyd'\nConsider using the `--user` option or check the permissions.\n```\n\nI was able to successfully install the libraries using the following command:\n\n```bash\npip --user install grpcio==1.42.0 tensorflow-serving-api==2.7.0\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-10/009_47408c61b9_typeerror-descriptors-cannot-not-be-created-direct.md", "metadata": {"id": "47408c61b9", "question": "TypeError: Descriptors cannot not be created directly.", "sort_order": 9}, "content": "You may encounter the following error when running `gateway.py`:\n\n```\nTypeError: Descriptors cannot not be created directly.\n```\n\nThis error appears in the following context:\n\n```\nFile \"C:\\Users\\Asia\\Data_Science_Code\\Zoompcamp\\Kubernetes\\gat.py\", line 9, in <module>\n  from tensorflow_serving.apis import predict_pb2\nFile \"C:\\Users\\Asia\\.virtualenvs\\Kubernetes-Ge6Ts1D5\\lib\\site-packages\\tensorflow_serving\\apis\\predict_pb2.py\", line 14, in <module>\n  from tensorflow.core.framework import tensor_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__pb2\nFile \"C:\\Users\\Asia\\.virtualenvs\\Kubernetes-Ge6Ts1D5\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_pb2.py\", line 14, in <module>\n  from tensorflow.core.framework import resource_handle_pb2 as tensorflow_dot_core_dot_framework_dot_resource__handle__pb2\nFile \"C:\\Users\\Asia\\.virtualenvs\\Kubernetes-Ge6Ts1D5\\lib\\site-packages\\tensorflow\\core\\framework\\resource_handle_pb2.py\", line 14, in <module>\n  from tensorflow.core.framework import tensor_shape_pb2 as tensorflow_dot_core_dot_framework_dot_tensor__shape__pb2\nFile \"C:\\Users\\Asia\\.virtualenvs\\Kubernetes-Ge6Ts1D5\\lib\\site-packages\\tensorflow\\core\\framework\\tensor_shape_pb2.py\", line 36, in <module>\n  _descriptor.FieldDescriptor(\nFile \"C:\\Users\\Asia\\.virtualenvs\\Kubernetes-Ge6Ts1D5\\lib\\site-packages\\google\\protobuf\\descriptor.py\", line 560, in __new__\n  _message.Message._CheckCalledFromGeneratedFile()\n\nTypeError: Descriptors cannot not be created directly.\n```\n\nThis message indicates that your generated protobuf code is out of date, and must be regenerated using `protoc >= 3.19.0`.\n\n\nTo resolve the issue, you have several options:\n\n1. **Regenerate your Protocol Buffers:** If possible, regenerate your .proto files using `protoc >= 3.19.0`.\n\n2. **Downgrade the protobuf package:**\n   \n   Downgrade the protobuf package to version 3.20.x or lower.\n\n   ```bash\n   pipenv install protobuf==3.20.1\n   ```\n   \n3. **Use a different implementation:**\n   \n   Set the environment variable to use a slower, pure-Python implementation:\n\n   ```bash\n   set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python\n   ```\n\nThe issue can often be resolved by downgrading `protobuf` to version `3.20.1`. This was confirmed to work in the described scenario."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-10/010_02bb92c002_how-to-install-easily-kubectl-on-windows.md", "metadata": {"id": "02bb92c002", "question": "How to install easily kubectl on Windows?", "sort_order": 10}, "content": "To install kubectl on Windows using PowerShell in VSCode, follow these steps:\n\n1. **Download kubectl with curl**\n   - Use the following command lines as per the [Kubernetes documentation](https://kubernetes.io/docs/tasks/tools/install-kubectl-windows/#install-kubectl-binary-with-curl-on-windows).\n\n2. **Copy the Executable**\n   - At step 3 of the tutorial, copy the `kubectl.exe` file to a specific folder on your C drive.\n\n3. **Update the System PATH**\n   - Add the folder path to the PATH in your environment variables.\n\nYou can also install `kind` similarly using the curl command on Windows by specifying a folder that will be added to the PATH environment variable.\n\nFor detailed guidance, refer to this [Medium tutorial](https://medium.com/@ggauravsigra/install-kubectl-on-windows-af77da2e6fff)."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-10/011_5b78a9f80a_install-kind-through-choco-library.md", "metadata": {"id": "5b78a9f80a", "question": "Install kind through choco library", "sort_order": 11}, "content": "First, launch a PowerShell terminal with administrator privileges.\n\nTo install the choco library, use the following command in PowerShell:\n\n```powershell\nSet-ExecutionPolicy Bypass -Scope Process -Force; [System.Net.ServicePointManager]::SecurityProtocol = [System.Net.ServicePointManager]::SecurityProtocol -bor 3072; iex ((New-Object System.Net.WebClient).DownloadString('https://chocolatey.org/install.ps1'))\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-10/012_92f8926c92_install-kind-via-go-package.md", "metadata": {"id": "92f8926c92", "question": "Install Kind via Go package", "sort_order": 12}, "content": "If you are having challenges installing Kind through the Windows Powershell or Choco Library, you can install Kind through Go.\n\n1. **Download and Install Go**: [https://go.dev/doc/install](https://go.dev/doc/install)\n\n2. **Confirm installation**:\n   ```bash\n   go version\n   ```\n3. **Install Kind**:\n   ```bash\n   go install sigs.k8s.io/kind@v0.20.0\n   ```\n4. **Confirm Kind installation**:\n   ```bash\n   kind --version\n   ```\n\nIt works perfectly."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-10/013_196b1a962a_the-connection-to-the-server-localhost8080-was-ref.md", "metadata": {"id": "196b1a962a", "question": "The connection to the server localhost:8080 was refused - did you specify the right host or port?", "sort_order": 13}, "content": "I encountered an issue where `kubectl` wasn't working, and I received the following error when trying to execute a command:\n\n```bash\nkubectl get service\n\nThe connection to the server localhost:8080 was refused - did you specify the right host or port?\n```\n\nHere is the solution that worked for me:\n\n1. Delete the existing cluster:\n   \n   ```bash\n   kind delete cluster\n   ```\n\n2. Remove the Kubernetes configuration directory:\n\n   ```bash\n   rm -rf ~/.kube\n   ```\n\n3. Create a new cluster:\n\n   ```bash\n   kind create cluster\n   ```\n\nAfter performing these steps, the command worked successfully:\n\n```bash\nkubectl get service\n\nNAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE\nkubernetes   ClusterIP   10.96.0.1    <none>        443/TCP   53s\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-10/014_34d0aaa13e_docker-running-out-of-storage-after-building-many.md", "metadata": {"id": "34d0aaa13e", "question": "Docker: Running out of storage after building many docker images", "sort_order": 14}, "content": "Due to experimenting extensively, I ran out of storage on my 30-GB AWS instance. Removing empty directories did not resolve the issue as those primarily contained code, which did not occupy much space.\n\n\n1. **Check Existing Images:**\n   - Use the following command to list all Docker images:\n     \n     ```bash\n     docker images\n     ```\n   \n   - This showed over 20 GBs of superseded or duplicate models.\n\n2. **Remove Unnecessary Images:**\n   - Remove unwanted images using:\n     \n     ```bash\n     docker rmi <image_id>\n     ```\n   \n   - However, this did not free up space as anticipated.\n\n3. **Free Up Space:**\n   - To actually free up storage, execute:\n     \n     ```bash\n     docker system prune\n     ```\n\n\nFor more details on why this happens, see: [Stack Overflow Discussion](https://stackoverflow.com/questions/36799718/why-removing-docker-containers-and-images-does-not-free-up-storage-space-on-wind)"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-10/015_ed397a51ef_homework-in-hw10-q6-what-does-it-mean-correct-valu.md", "metadata": {"id": "ed397a51ef", "question": "Homework: In HW10 Q6 what does it mean “correct value for CPU and memory”? Aren’t they arbitrary?", "sort_order": 15}, "content": "Yes, the question requires you to specify values for CPU and memory in the yaml file. However, the part of the question regarding the form only refers to the port, which does have a defined correct value for this specific homework."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-10/016_1c65bb4c0c_why-do-cpu-values-for-kubernetes-deploymentyaml-lo.md", "metadata": {"id": "1c65bb4c0c", "question": "Why do CPU values for Kubernetes deployment.yaml look like \"100m\" and \"500m\"? What does \"m\" mean?", "sort_order": 16}, "content": "In Kubernetes resource specifications, such as CPU requests and limits, the \"m\" stands for milliCPU, which is a unit of computing power. It represents one thousandth of a CPU core.\n\n- `cpu: \"100m\"` means the container is requesting 100 milliCPUs, which is equivalent to 0.1 CPU core.\n- `cpu: \"500m\"` means the container has a CPU limit of 500 milliCPUs, which is equivalent to 0.5 CPU core.\n\nThese values are specified in milliCPUs to allow fine-grained control over CPU resources. It allows you to express CPU requirements and limits in a more granular way, especially in scenarios where your application might not need a full CPU core."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-10/017_fd3a06c6ee_kind-cannot-load-docker-image.md", "metadata": {"id": "fd3a06c6ee", "question": "Kind: cannot load docker image", "sort_order": 17}, "content": "**Problem:** Failing to load docker-image to cluster (when you've named a cluster)\n\n```bash\nkind load docker-image zoomcamp-10-model:xception-v4-001\n\nERROR: no nodes found for cluster \"kind\"\n```\n\n**Solution:** Specify the cluster name with `-n`\n\n```bash\nkind -n clothing-model load docker-image zoomcamp-10-model:xception-v4-001\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-10/018_23f5be95ef_python-fastapi-deployment.md", "metadata": {"id": "23f5be95ef", "question": "Which language and framework are used for deployment?", "sort_order": 18}, "content": "Python with FastAPI (updated from Flask). You’ll also package/serve models, containerize, and deploy to cloud/Kubernetes."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-10/018_4076e3f3a6_kind-is-not-recognized-as-an-internal-or-external.md", "metadata": {"id": "4076e3f3a6", "question": "'kind' is not recognized as an internal or external command, operable program or batch file. (In Windows)", "sort_order": 18}, "content": "**Problem:**\n\nI downloaded `kind` using the following command:\n\n```bash\ncurl.exe -Lo kind-windows-amd64.exe https://kind.sigs.k8s.io/dl/v0.17.0/kind-windows-amd64\n```\n\nWhen I try to run:\n\n```bash\nkind --version\n```\n\nI receive the error:\n\n```plaintext\n'kind' is not recognized as an internal or external command, operable program or batch file.\n```\n\n**Solution:**\n\n1. The default name of the executable is `kind-windows-amd64.exe`. Rename this file to `kind.exe`.\n2. Place `kind.exe` in a specific folder.\n3. Add this folder to the `PATH` environment variable."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-10/019_60b2f26382_running-kind-on-linux-with-rootless-docker-or-root.md", "metadata": {"id": "60b2f26382", "question": "Running kind on Linux with Rootless Docker or Rootless Podman", "sort_order": 19}, "content": "Using kind with Rootless Docker or Rootless Podman requires some changes on the system (Linux). See [kind – Rootless (k8s.io)](https://kind.sigs.k8s.io/docs/user/rootless/)."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-10/020_e5c67e3439_kubernetes-dashboard.md", "metadata": {"id": "e5c67e3439", "question": "Kubernetes-dashboard", "sort_order": 20}, "content": "[Deploy and Access the Kubernetes Dashboard](https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/)"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-10/021_afc2e6e77d_correct-aws-cli-version-for-eksctl.md", "metadata": {"id": "afc2e6e77d", "question": "Correct AWS CLI version for eksctl", "sort_order": 21}, "content": "Ensure you are using AWS CLI v2. You can check your current version with the following command:\n\n```bash\naws --version\n```\n\nFor more details, refer to the [AWS CLI v2 migration instructions](https://docs.aws.amazon.com/cli/latest/userguide/cliv2-migration-instructions.html)."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-10/022_b062fc29e5_typeerror-__init__-got-an-unexpected-keyword-argum.md", "metadata": {"id": "b062fc29e5", "question": "TypeError: __init__() got an unexpected keyword argument 'unbound_message' while importing Flask", "sort_order": 22}, "content": "In video 10.3, while testing a Flask service, the following error occurred:\n\n```\nTypeError: __init__() got an unexpected keyword argument 'unbound_message'\n```\n\nThis error was encountered when running `docker run ...` in one terminal and then executing `python gateway.py` in another terminal.\n\n\n\nThis issue is related to the versions of Flask and Werkzeug.\n\nTo debug the issue:\n\n1. Run `pip freeze > requirements.txt` to check the installed versions of Flask and Werkzeug.\n   - Example output:\n     ```\n     Flask==2.2.2\n     Werkzeug==2.2.2\n     ```\n2. The error occurs when using an old version of Werkzeug (2.2.2) with a new version of Flask (2.2.2).\n3. To resolve, pin the version of Flask to an older version:\n   \n   ```bash\n   pipenv install Flask==2.1.3\n   ```\n\nThis should resolve the compatibility issue."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-10/023_b612fa07ec_command-aws-ecr-get-login-no-include-email-returns.md", "metadata": {"id": "b612fa07ec", "question": "Command aws ecr get-login --no-include-email returns \"aws: error: argument operation: Invalid choice…\"", "sort_order": 23}, "content": "As per AWS documentation:\n\nYou need to execute the following command:\n\n```bash\naws ecr get-login-password --region <region> | docker login --username AWS --password-stdin <aws_account_id>.dkr.ecr.<region>.amazonaws.com\n```\n\n- Replace `<region>` and `<aws_account_id>` with your specific details.\n\nAlternatively, you can run the following command without changing anything, given you have a default region configured:\n\n```bash\naws ecr get-login-password --region $(aws configure get region) | docker login --username AWS --password-stdin \"$(aws sts get-caller-identity --query \"Account\" --output text).dkr.ecr.$(aws configure get region).amazonaws.com\"\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-10/024_error-downloading-tensorflow.md", "metadata": {"id": "1322100525", "question": "Error downloading tensorflow/serving:2.7.0 on Apple M1 Mac", "sort_order": 24}, "content": "While trying to run the Docker code on M1:\n\n```bash\ndocker run --platform linux/amd64 \\\n   -it --rm \\\n   -p 8500:8500 \\\n   -v $(pwd)/clothing-model:/models/clothing-model/1 \\\n   -e MODEL_NAME=\"clothing-model\" \\\n   tensorflow/serving:2.7.0\n```\n\nIt outputs the error:\n\n```\nStatus: Downloaded newer image for tensorflow/serving:2.7.0\n[libprotobuf FATAL external/com_google_protobuf/src/google/protobuf/generated_message_reflection.cc:2345] CHECK failed: file != nullptr:\nterminate called after throwing an instance of 'google::protobuf::FatalException'\nwhat():  CHECK failed: file != nullptr:\nqemu: uncaught target signal 6 (Aborted) - core dumped\n/usr/bin/tf_serving_entrypoint.sh: line 3:     8 Aborted                 tensorflow_model_server --port=8500 --rest_api_port=8501 --model_name=${MODEL_NAME} --model_base_path=${MODEL_BASE_PATH}/${MODEL_NAME} \"$@\"\n```\n\nSolution:\n\n1. Pull the alternative Docker image:\n   \n   ```bash\n   docker pull emacski/tensorflow-serving:latest\n   ```\n\n2. Run the container with the alternative image:\n   \n   ```bash\n   docker run -it --rm \\\n   -p 8500:8500 \\\n   -v $(pwd)/clothing-model:/models/clothing-model/1 \\\n   -e MODEL_NAME=\"clothing-model\" \\\n   emacski/tensorflow-serving:latest-linux_arm64\n   ```\n   \nSee more here: [GitHub Repository](https://github.com/emacski/tensorflow-serving-arm)"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-10/025_930f795453_illegal-instruction-error-when-running-tensorflows.md", "metadata": {"id": "930f795453", "images": [{"description": "image #1", "id": "image_1", "path": "images/machine-learning-zoomcamp/image_51551549.png"}], "question": "Illegal instruction error when running tensorflow/serving image on Mac M2 Apple Silicon (potentially on M1 as well)", "sort_order": 25}, "content": "Problem:\n\nWhile trying to run the following Docker code on Mac M2 Apple Silicon:\n\n```bash\ndocker run --platform linux/amd64 -it --rm \\\n-p 8500:8500 \\\n-v $(pwd)/clothing-model:/models/clothing-model/1 \\\n-e MODEL_NAME=\"clothing-model\" \\\ntensorflow/serving\n```\n\nYou get an error:\n\n```bash\n/usr/bin/tf_serving_entrypoint.sh: line 3:     7 Illegal instruction     tensorflow_model_server --port=8500 --rest_api_port=8501 --model_name=${MODEL_NAME} --model_base_path=${MODEL_BASE_PATH}/${MODEL_NAME} \"$@\"\n```\n\nSolution:\n\n1. **Use Bitnami TensorFlow-Serving Base Image**\n   \n   Launch it either using `docker run`:\n   \n   ```bash\n   docker run -d \\\n   --name tf_serving \\\n   -p 8500:8500 \\\n   -p 8501:8501 \\\n   -v $(pwd)/clothing-model:/bitnami/model-data/1 \\\n   -e TENSORFLOW_SERVING_MODEL_NAME=clothing-model \\\n   bitnami/tensorflow-serving:2\n   ```\n   \n   Or use the following `docker-compose.yaml`:\n   \n   ```yaml\n   version: '3'\n\n   services:\n     tf_serving:\n       image: bitnami/tensorflow-serving:2\n       volumes:\n         - ${PWD}/clothing-model:/bitnami/model-data/1\n       ports:\n         - 8500:8500\n         - 8501:8501\n       environment:\n         - TENSORFLOW_SERVING_MODEL_NAME=clothing-model\n   ```\n   \n   And run it with:\n   \n   ```bash\n   docker compose up\n   ```\n\n2. **Alternative since Oct 2024:**\n\n   Beta release of Docker VMM - the more performant alternative to Apple Virtualization Framework on macOS (requires Apple Silicon and macOS 12.5 or later). [Docker VMM Documentation](https://docs.docker.com/desktop/features/vmm/)\n\n<{IMAGE:image_1}>"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-10/026_82bef32bca_hpa-cpu-metrics-dont-show.md", "metadata": {"id": "82bef32bca", "question": "HPA: CPU metrics don't show", "sort_order": 26}, "content": "CPU metrics show \"Unknown\"\n\n```\nNAME         REFERENCE           TARGETS         MINPODS   MAXPODS   REPLICAS   AGE\n\ncredit-hpa   Deployment/credit   <unknown>/20%   1         3         1          18s\n\nFailedGetResourceMetric       2m15s (x169 over 44m)  horizontal-pod-autoscaler  failed to get cpu utilization: unable to get metrics for resource cpu: unable to fetch metrics from resource metrics API:\n```\n\n\n1. Delete HPA:\n   ```bash\n   kubectl delete hpa credit-hpa\n   ```\n\n2. Apply the metrics server configuration:\n   ```bash\n   kubectl apply -f https://raw.githubusercontent.com/pythianarora/total-practice/master/sample-kubernetes-code/metrics-server.yaml\n   ```\n\n3. Recreate the HPA.\n\nThis should solve the CPU metrics report issue."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-10/027_c9f27cb21a_hw10-autoscaling-optional-command-does-not-work.md", "metadata": {"id": "c9f27cb21a", "question": "HW10 Autoscaling (optional) command does not work", "sort_order": 27}, "content": "The following command encountered issues:\n\n```bash\nkubectl autoscale deployment subscription --name subscription-hpa --cpu-percent=20 --min=1 --max=3\n```\n\nError logs indicated certificate validation issues due to the server's certificate lacking a valid Subject Alternative Name (SAN) for the node's IP address.\n\nSuggested Steps:\n\n1. Run the following command to skip TLS verification:\n   \n   ```bash\n   kubectl patch deployment metrics-server -n kube-system --type='json' -p='[{\"op\": \"add\", \"path\": \"/spec/template/spec/containers/0/args/-\", \"value\": \"--kubelet-insecure-tls\"}]'\n   ```\n\n2. Restart the deployment:\n   \n   ```bash\n   kubectl rollout restart deployment metrics-server -n kube-system\n   ```\n\nNote: Avoiding TLS certificate validation is not recommended for production systems, but may suffice for this case."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-11/001_da9f03ae67_kserve-module-optional.md", "metadata": {"id": "da9f03ae67", "question": "Is the KServe module required?", "sort_order": 1}, "content": "No. It’s **optional and outdated**. If time-constrained, prioritize Kubernetes."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-11/002_7e7931bff6_errors-with-istio-during-installation.md", "metadata": {"id": "7e7931bff6", "question": "Errors with istio during installation", "sort_order": 2}, "content": "Running the following command:\n\n```bash\ncurl -s \"https://raw.githubusercontent.com/kserve/kserve/release-0.9/hack/quick_install.sh\" | bash\n```\n\nFails with errors due to Istio failing to update resources when using `kubectl` version greater than 1.25.0.\n\nCheck your `kubectl` version with:\n\n```bash\nkubectl version\n```\n\n\n1. Download the \"quick_install.bash\" script without executing it:\n   \n   ```bash\n   curl -O https://raw.githubusercontent.com/kserve/kserve/release-0.9/hack/quick_install.sh\n   ```\n\n2. Edit the downloaded script to update the versions of Istio and Knative according to the [recommended version matrix on the KServe website](https://kserve.github.io/website/master/admin/serverless/serverless/#recommended-version-matrix).\n\n3. Run the modified bash script.\n\nBy following these steps, you should avoid the installation errors related to Istio."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-2-homework/001_3ff258b230_homework-q4-is-r-same-as-alpha-in-scikit-learn-rid.md", "metadata": {"id": "3ff258b230", "question": "Homework Q4: Is r same as alpha in Scikit-Learn Ridge?", "sort_order": 1}, "content": "In the context of regression, particularly with regularization:\n\n- `r` typically represents the regularization parameter in some algorithms. It controls the strength of the penalty applied to the coefficients of the regression model to prevent overfitting.\n\n- In `sklearn.Ridge()`, the parameter `alpha` serves the same purpose as `r`. It specifies the amount of regularization applied to the model. A higher value of `alpha` increases the amount of regularization, which can reduce model complexity and improve generalization.\n\n`r` and `alpha` are both regularization parameters and control the \"strength\" of regularization. Increasing these values will lead to stronger regularization. However, the mathematical implementation differs:\n\n- **`sklearn.Ridge()`**:\n\n  ```\n  ||y - Xw||^2_2 + alpha * ||w||^2_2\n  ```\n\n- **Lesson's Notebook (`train_linear_regression_reg` function)**:\n  \n  ```python\n  XTX = XTX + r * np.eye(XTX.shape[0])\n  ```\n  \n  `r` adds “noise” to the main diagonal to prevent multicollinearity, which “breaks” finding the inverse matrix.\n\nFor further reference, see the [sklearn.Ridge documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html) and the [lesson’s notebook](https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/02-regression/notebook.ipynb)."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-2-homework/002_8af59cb98f_homework-the-answer-i-get-for-one-of-the-homework.md", "metadata": {"id": "8af59cb98f", "question": "Homework: The answer I get for one of the homework questions doesn't match any of the options. What should I do?", "sort_order": 2}, "content": "That’s normal. We all have different environments: our computers have different versions of OS and different versions of libraries — even different versions of Python.\n\nIf it’s the case, just select the option that’s closest to your answer."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-2-homework/003_af50bd4d07_homework-q3-meaning-of-mean.md", "metadata": {"id": "af50bd4d07", "question": "Homework: Q3: Meaning of mean", "sort_order": 3}, "content": "In question 3 of HW02 it is mentioned: ‘For computing the mean, use the training only’. What does that mean?\n\nIt means that you should use only the training data set for computing the mean, not the validation or test data set. This is how you can calculate the mean:\n\n```python\n# Calculate mean for a specific column in the training data\nmean_value = df_train['column_name'].mean()\n```\n\nAnother option:\n\n```python\n# Get descriptive statistics, including the mean\nstats = df_train['column_name'].describe()\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-2-homework/004_9346394db7_caution-for-applying-log-transformation-in-week-2.md", "metadata": {"id": "9346394db7", "images": [{"description": "image #1", "id": "image_1", "path": "images/machine-learning-zoomcamp/image_5b86f554.png"}], "question": "Caution for applying log transformation in Week-2 2023 cohort homework", "sort_order": 4}, "content": "The instruction for applying log transformation to the ‘median_house_value’ variable is provided before Q3 in the homework for Week-2 under the ‘Prepare and split the dataset’ heading.\n\n<{IMAGE:image_1}>\n\nHowever, this instruction is absent in the subsequent questions of the homework, and you might encounter issues like a huge RMSE. Remember to apply log transformation to the target variable for each question."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-2-homework/005_bdcc3ae17b_homework-is-the-rmse-result-close-to-the-options.md", "metadata": {"id": "bdcc3ae17b", "question": "Homework: is the RMSE result close to the options?", "sort_order": 5}, "content": "My result is about 12.4 different from the closest option. For previous questions, my answers were close, so I'm unsure why there's a large discrepancy for question 6.\n\nFor questions 5 and 6, please ensure you reinitialize with:\n\n```python\nIdx = np.arange(n)\n```\n\nThis should be done for each iteration of `r` in question 5 and also for question 6."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-2/001_a654b0e217_how-to-avoid-accidentally-pushing-csv-files.md", "metadata": {"id": "a654b0e217", "question": "How to avoid accidentally pushing CSV files", "sort_order": 1}, "content": "To avoid accidentally pushing CSV files (or any specific file type) to a Git repository, you can use a `.gitignore` file.\n\n- Add a rule to ignore CSV files by including:\n  \n  ```\n  *.csv\n  ```\n\n- If the CSV files have already been committed, you can remove them from Git tracking but keep them locally by using the command:\n\n  ```bash\n  git rm --cached filename.csv\n  ```"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-2/002_ad33c1d6f5_checking-long-tail-of-data.md", "metadata": {"id": "ad33c1d6f5", "question": "Checking long tail of data", "sort_order": 2}, "content": "To analyze the long tail of data, you can use a histogram or check skewness and descriptive statistics.\n\n#### Using Histogram\n\n```python\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Load the data\nurl = 'https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv'\ndf = pd.read_csv(url)\n\n# EDA\nsns.histplot(df['median_house_value'], kde=False)\nplt.show()\n```\n\n#### Check Skewness and Descriptive Statistics\n\n```python\n# Describe the 'median_house_value'\nprint(df['median_house_value'].describe())\n\n# Calculate the skewness of the 'median_house_value' variable\nskewness = df['median_house_value'].skew()\n\n# Print the skewness value\nprint(\"Skewness of 'median_house_value':\", skewness)\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-2/003_0a7e9c723a_linalgerror-singular-matrix.md", "metadata": {"id": "0a7e9c723a", "question": "LinAlgError: Singular matrix", "sort_order": 3}, "content": "It’s possible that when you follow the videos, you’ll get a Singular Matrix error. This will be explained in the Regularization video. Don’t worry, it’s normal to encounter this.\n\nYou might also receive this error if you invert matrix `X` more than once in your code."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-2/004_ba636cdb18_getting-nans-after-applying-mean.md", "metadata": {"id": "ba636cdb18", "question": "Getting NaNs after applying .mean()", "sort_order": 4}, "content": "I was using for loops to apply RMSE to a list of `y_val` and `y_pred`. However, the resulting RMSE was all NaN.\n\nI discovered that the issue occurred during the mean calculation step in the RMSE function, after squaring the error. There were NaNs in the array, which I traced back to the initial data splitting step. I had only used `fillna(0)` on the training data, not on the validation and test data. \n\nThe problem was resolved by applying `fillna(0)` to all datasets (train, validation, and test). My for loops now successfully compute RMSE for all seed values."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-2/005_17d0c79f61_target-variable-transformation.md", "metadata": {"id": "17d0c79f61", "question": "Target variable transformation", "sort_order": 5}, "content": "Why should we transform the target variable to logarithm distribution? Do we do this for all machine learning projects?\n\nOnly if you see that your target is highly skewed. The easiest way to evaluate this is by plotting the distribution of the target variable.\n\nTransforming to a logarithmic scale can help address skewness and improve the distribution of your data set.\n\nFor more information, you can refer to [Skewness on Wikipedia](https://en.wikipedia.org/wiki/Skewness)."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-2/006_0a4f9065b1_loading-the-dataset-directly-through-kaggle-notebo.md", "metadata": {"id": "0a4f9065b1", "question": "Loading the dataset directly through Kaggle Notebooks", "sort_order": 6}, "content": "To load a dataset in Kaggle Notebooks, you can use the following command. Remember that the `!` before `wget` is essential.\n\n```bash\n!wget https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv\n```\n\nOnce the dataset is loaded onto the Kaggle Notebook server, it can be read using the following pandas command:\n\n```python\ndf = pd.read_csv('housing.csv')\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-2/007_ab0c17d01c_filter-a-dataset-by-using-its-values.md", "metadata": {"id": "ab0c17d01c", "question": "Filter a dataset by using its values", "sort_order": 7}, "content": "We can filter a dataset by using its values as shown below:\n\n```python\n# Using OR condition\n df = df[(df['ocean_proximity'] == '<1H OCEAN') | (df['ocean_proximity'] == 'INLAND')]\n```\n\nYou can use `|` for 'OR', and `&` for 'AND'.\n\nAlternative method:\n\n```python\n# Using isin()\n df = df[df['ocean_proximity'].isin(['<1H OCEAN', 'INLAND'])]\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-2/008_9fbb36bb16_alternative-way-to-load-the-data-using-requests.md", "metadata": {"id": "9fbb36bb16", "question": "Alternative way to load the data using requests", "sort_order": 8}, "content": "Here's another way to load a dataset using the `requests` library:\n\n```python\nimport requests\n\nurl = 'https://raw.githubusercontent.com/alexeygrigorev/datasets/master/housing.csv'\n\nresponse = requests.get(url)\n\nif response.status_code == 200:\n    with open('housing.csv', 'wb') as file:\n        file.write(response.content)\nelse:\n    print(\"Download failed.\")\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-2/009_a0b5f19b14_null-column-is-appearing-even-if-i-applied-fillna.md", "metadata": {"id": "a0b5f19b14", "question": "Null column is appearing even if I applied .fillna()", "sort_order": 9}, "content": "When creating a duplicate of your dataframe, if you do the following:\n\n```python\nX_train = df_train\nX_val = df_val\n```\n\nYou're still referencing the original variable. This is called a shallow copy. To ensure that no references are attaching both variables and to keep a copy of the data, create a deep copy:\n\n```python\nX_train = df_train.copy()\nX_val = df_val.copy()\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-2/010_aac6525676_can-i-use-scikit-learns-train_test_split-for-this.md", "metadata": {"id": "aac6525676", "question": "Can I use Scikit-Learn’s train_test_split for this week?", "sort_order": 10}, "content": "Yes, you can. Here we implement it ourselves to better understand how it works, but later we will only rely on Scikit-Learn’s functions. If you want to start using it earlier — feel free to do it."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-2/011_5a2fac8521_can-i-use-linearregression-from-scikit-learn-for-t.md", "metadata": {"id": "5a2fac8521", "question": "Can I use LinearRegression from Scikit-Learn for this week?", "sort_order": 11}, "content": "Yes, you can. We will also do that next week, so don’t worry, you will learn how to do it."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-2/012_6cf4287216_using-scikit-learn-for-regression-with-and-without.md", "metadata": {"id": "6cf4287216", "question": "Using Scikit-Learn for regression with and without regularization", "sort_order": 12}, "content": "What are the equivalents in Scikit-Learn for linear regression with and without regularization used in week 2?\n\n**Without Regularization:**\n  \n```python\nsklearn.linear_model.LinearRegression\n```\n\n**With Regularization:**\n  \n```python\nsklearn.linear_model.Ridge\n```\n\nFor more information, you can refer to the Scikit-Learn documentation on linear models:\n\n[Scikit-Learn Linear Models](https://scikit-learn.org/stable/modules/linear_model.html)"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-2/013_631e3d040c_why-linear-regression-doesnt-provide-a-perfect-fit.md", "metadata": {"id": "631e3d040c", "images": [{"description": "image #1", "id": "image_1", "path": "images/machine-learning-zoomcamp/image_008f5d2a.png"}], "question": "Why linear regression doesn’t provide a “perfect” fit?", "sort_order": 13}, "content": "Linear regression often provides a good approximation of the underlying relationship but rarely achieves a \"perfect\" fit in real-world applications.\n\n**Q:** Why is `y_pred` different from `y`?\n\nIn lesson 2.8, the question arises: after training `X_train` to get the weights, shouldn't multiplying by `X_train` give exactly `y`?\n\n**A:** Linear regression is a simple model and should not fit 100%, as this would indicate overfitting. Consider a single feature `X`:\n\n<{IMAGE:image_1}>\n\nAs the model is linear, how would you draw a line to fit all the \"dots\"?\n\nYou could \"fit\" all the \"dots\" using something like `scipy.optimize.curve_fit` (non-linear least squares), but consider how it would perform on previously unseen data.\n\nRefer to: [scipy.optimize.curve_fit](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.curve_fit.html)"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-2/014_0f5754c053_random-seed-42.md", "metadata": {"id": "0f5754c053", "question": "Random seed 42", "sort_order": 14}, "content": "One of the questions on the homework calls for using a random seed of 42. When using 42, all my missing values ended up in my training dataframe and not my validation or test dataframes, why is that?\n\nThe purpose of the seed value is to randomly generate the proportion split. Using a seed of 42 ensures that all learners are on the same page by getting the same behavior (in this case, all missing values ending up in the training dataframe). If using a different seed value (e.g. 9), missing values will then appear in all other dataframes."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-2/015_6c5a79afd4_shuffling-the-initial-dataset-using-pandas-built-i.md", "metadata": {"id": "6c5a79afd4", "question": "Shuffling the initial dataset using pandas built-in function", "sort_order": 15}, "content": "It is possible to shuffle the dataset using the pandas built-in function [`pandas.DataFrame.sample`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sample.html). To shuffle the complete dataset and reset the index, use the following commands:\n\n- Set `frac=1` to return a shuffled version of the complete dataset.\n- Set `random_state=seed` for consistent randomization.\n\n```python\ndf_shuffled = df.sample(frac=1, random_state=seed)\ndf_shuffled.reset_index(drop=True, inplace=True)\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-2/016_3547bb9442_shuffling-data-using-numpys-generator-feature.md", "metadata": {"id": "3547bb9442", "question": "Shuffling data using Numpy’s Generator Feature", "sort_order": 16}, "content": "While the lectures have you use the shuffle function to shuffle the index of the dataframe, it no longer accepts random seed as a parameter. This is because Numpy converted this feature into its own \"[Generator](https://numpy.org/doc/stable/reference/random/generator.html) Class\". In order to assign the random generator a seed, you have to specify the object (`rng`) that you are going to utilize in your code:\n\n```python\n# Create index from range of values in array\nidx = np.arange(n)\n\n# Create random generator object and set seed\nrng = np.random.default_rng(random_seed)\n\n# Shuffle values using Generator object\nrng.shuffle(idx)\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-2/017_9e300560ca_when-should-we-transform-the-target-variable-to-lo.md", "metadata": {"id": "9e300560ca", "question": "When should we transform the target variable to logarithm distribution?", "sort_order": 17}, "content": "When the target variable has a long tail distribution, such as prices with a wide range, you can transform it using the `np.log1p()` method. However, be aware that this method will not work if your target variable contains negative values."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-2/018_1fda7c57b0_valueerror-shapes-not-aligned.md", "metadata": {"id": "1fda7c57b0", "question": "ValueError: shapes not aligned", "sort_order": 18}, "content": "```python\nX_train = prepare_X(df_train)\nw_0, w = train_linear_regression(X_train, y_train)\n\nX_val = prepare_X(df_val)\ny_pred = w_0 + X_val.dot(w)\n\nrmse(y_val, y_pred)\n```\n\nWe get:\n\n```\nValueError                                Traceback (most recent call last)\nInput In [132], in <cell line: 5>()\n      2 w_0, w = train_linear_regression(X_train, y_train)\n      4 X_val = prepare_X(df_val)\n----> 5 y_pred = w_0 + X_val.dot(w)\n      7 rmse(y_val, y_pred)\n\nValueError: shapes (4128,) and (1,) not aligned: 4128 (dim 0) != 1 (dim 0)\n```\n\nIf we try to perform an arithmetic operation between two arrays of different shapes or dimensions, it throws an error like operands could not be broadcast together with shapes. Broadcasting can occur in certain scenarios and will fail in others.\n\nTo solve this issue, you can use the `*` operator instead of the `dot()` method:\n\n```python\nX_train = prepare_X(df_train)\nw_0, w = train_linear_regression(X_train, y_train)\n\nX_val = prepare_X(df_val)\ny_pred = w_0 + (X_val * w)\n\nrmse(y_val, y_pred)\n```\n\nOutput:\n\n```\n0.5713144443358035\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-2/019_c6c716b9e7_how-to-copy-a-dataframe-without-changing-the-origi.md", "metadata": {"id": "c6c716b9e7", "question": "How to copy a dataframe without changing the original dataframe?", "sort_order": 19}, "content": "Copy a dataframe using:\n\n```python\nX_copy = X.copy()\n```\n\nThis creates a deep copy of the dataframe. If you use `X_copy = X`, it will create a \"view\" and any changes to `X_copy` will affect the original dataframe `X`. This is not a real copy."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-2/020_c19b485c07_what-is-standard-deviation.md", "metadata": {"id": "c19b485c07", "images": [{"description": "image #1", "id": "image_1", "path": "images/machine-learning-zoomcamp/image_6e82986e.png"}, {"description": "image #2", "id": "image_2", "path": "images/machine-learning-zoomcamp/image_88209e28.png"}], "question": "What is standard deviation?", "sort_order": 20}, "content": "<{IMAGE:image_1}>\n\nOne of the most important characteristics of the normal distribution is that mean = median = mode, meaning the most popular value, the mean of the distribution, and 50% of the sample are under the same value. This is equivalent to saying that the area under the curve (black) is the same on the left and on the right. The long tail (red curve) results from a few observations with high values, altering the behavior of the distribution. Consequently, the area is different on each side, and the mean, median, and mode become different. The mean is no longer representative, the range is larger, and the probability of being on the left or right is not the same.\n\nIn statistics, the standard deviation is a measure of the amount of variation or dispersion of a set of values. A low standard deviation indicates that the values tend to be close to the mean (also called the expected value) of the set, while a high standard deviation indicates that the values are spread out over a wider range. [Wikipedia](https://en.wikipedia.org/wiki/Standard_deviation) The formula to calculate standard deviation is:\n\n<{IMAGE:image_2}>"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-2/021_32f4f59df6_do-we-need-to-apply-regularization-techniques-alwa.md", "metadata": {"id": "32f4f59df6", "question": "Do we need to apply regularization techniques always? Or only in certain scenarios?", "sort_order": 21}, "content": "The application of regularization depends on the specific situation and problem. It is recommended to consider it when training machine learning models, especially with small datasets or complex models, to prevent overfitting. However, its necessity varies depending on the data quality and size. Evaluate each case individually to determine if it is needed."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-2/022_b1712cc9d5_shortcut-define-functions-for-faster-execution.md", "metadata": {"id": "b1712cc9d5", "question": "Shortcut: define functions for faster execution", "sort_order": 22}, "content": "Defining functions can speed up development significantly. You can create a function like `prepare_df(initial_df, seed, fill_na_type)` to prepare all three dataframes and y_vectors. The `fillna()` operation can be applied before splitting the `initial_df`.\n\nAdditionally, you can reuse functions such as `rmse()` and `train_linear_regression(X, y, r)` from the class notebook."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-2/023_e41007991b_warning-about-modifying-dataframes-inside-function.md", "metadata": {"id": "e41007991b", "question": "Warning about modifying Dataframes inside functions", "sort_order": 23}, "content": "When applying a function to a DataFrame, it is important to consider that if you do not want to alter the original DataFrame, you should create a copy of it first. Failing to do so may result in unintended modifications to the original dataset.\n\nTo preserve the integrity of your data, always use `df.copy()` before making any changes."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-2/024_8d98b7c098_find-standard-deviation-with-pandas.md", "metadata": {"id": "8d98b7c098", "question": "Find standard deviation with Pandas", "sort_order": 24}, "content": "To find the standard deviation of a list or series of data using Pandas, you can convert the list into a Pandas Series and use the `.std()` function. For example:\n\n```python\nimport pandas as pd\n\nx = [1, 2, 3, 4, 5]\nstandard_deviation = pd.Series(x).std()\nprint(standard_deviation)\n```\n\nThis will calculate the standard deviation of the list `x`."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-2/025_5020067c2b_standard-deviation-differences-in-numpy-and-pandas.md", "metadata": {"id": "5020067c2b", "images": [{"description": "image #1", "id": "image_1", "path": "images/machine-learning-zoomcamp/image_38e67375.png"}, {"description": "image #2", "id": "image_2", "path": "images/machine-learning-zoomcamp/image_8ad3d822.png"}], "question": "Standard Deviation Differences in Numpy and Pandas", "sort_order": 25}, "content": "Numpy and Pandas use different equations to compute the standard deviation. Numpy uses the population standard deviation by default, whereas Pandas uses the sample standard deviation.\n\n### Numpy\n\n<{IMAGE:image_1}>\n\n### Pandas\n\n<{IMAGE:image_2}>\n\nPandas computes the standard deviation using one degree of freedom by default. You can modify the degree of freedom in Numpy to achieve a similar result by using the `ddof` parameter:\n\n```python\nimport numpy as np\n\nnp.std(df.weight, ddof=1)\n```\n\nThe result will be similar if we set `ddof=1` in Numpy."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-2/026_d781b23627_standard-deviation-using-pandas-built-in-function.md", "metadata": {"id": "d781b23627", "question": "Standard deviation using Pandas built in Function", "sort_order": 26}, "content": "In pandas, you can use the built-in function `std()` to calculate the standard deviation. For example:\n\n- To get the standard deviation of a single column:\n\n  ```python\n  df['column_name'].std()\n  ```\n\n- To get the standard deviation of multiple columns:\n\n  ```python\n  df[['column_1', 'column_2']].std()\n  ```"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-2/027_19a42e2005_how-to-combine-train-and-validation-datasets.md", "metadata": {"id": "19a42e2005", "question": "How to combine train and validation datasets", "sort_order": 27}, "content": "Use `pandas.concat` function ([pandas documentation](https://pandas.pydata.org/docs/reference/api/pandas.concat.html)) to combine two dataframes. To combine two numpy arrays, use `numpy.concatenate` ([numpy documentation](https://numpy.org/doc/stable/reference/generated/numpy.concatenate.html)).\n\nThe code would be as follows:\n\n```python\ndf_train_combined = pd.concat([df_train, df_val])\ny_train = np.concatenate((y_train, y_val), axis=0)\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-2/028_c30519737d_understanding-rmse-and-how-to-calculate-rmse-score.md", "metadata": {"id": "c30519737d", "question": "Understanding RMSE and how to calculate RMSE score", "sort_order": 28}, "content": "The Root Mean Squared Error (RMSE) is one of the primary metrics to evaluate the performance of a regression model. It calculates the average deviation between the model's predicted values and the actual observed values, offering insight into the model's ability to accurately forecast the target variable. To calculate the RMSE score:\n\n1. Import the necessary libraries:\n   \n   ```python\n   import numpy as np\n   from sklearn.metrics import mean_squared_error\n   ```\n\n2. Calculate the Mean Squared Error (MSE):\n   \n   ```python\n   mse = mean_squared_error(actual_values, predicted_values)\n   ```\n\n3. Compute the RMSE:\n   \n   ```python\n   rmse = np.sqrt(mse)\n   print(\"Root Mean Squared Error (RMSE):\", rmse)\n   ```"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-2/029_077e2c96ad_deep-dive-into-normal-equation-for-regression.md", "metadata": {"id": "077e2c96ad", "question": "Deep dive into normal equation for regression", "sort_order": 29}, "content": "I found this video useful for understanding how we derive the normal form in linear regression: [Normal Equation Derivation for Regression](https://www.youtube.com/watch?v=g8qF61P741w)."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-2/030_1c1a0016fc_useful-resource-for-missing-data-treatment.md", "metadata": {"id": "1c1a0016fc", "question": "Useful Resource for Missing Data Treatment", "sort_order": 30}, "content": "[A Guide to Handling Missing Values in Python](https://www.kaggle.com/code/parulpandey/a-guide-to-handling-missing-values-in-python/notebook)"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-2/031_ae58046503_what-sklearn-version-is-alexey-using-in-the-youtub.md", "metadata": {"id": "ae58046503", "question": "What sklearn version is Alexey using in the YouTube videos?", "sort_order": 31}, "content": "Version 0.24.2 and Python 3.8.11."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-3-homework/001_71f360d8eb_what-data-should-we-use-for-correlation-matrix.md", "metadata": {"id": "71f360d8eb", "question": "What data should we use for correlation matrix?", "sort_order": 1}, "content": "Q2 asks about the correlation matrix and converting `median_house_value` from numeric to binary. Just to clarify, we are only dealing with `df_train`, not `df_train_full`, correct? The question explicitly mentions the train dataset.\n\nYes, it is only on `df_train`. The reason is that `df_train_full` also contains the validation dataset. At this stage, we don't want to make conclusions based on the validation data, since we want to test how we did without using that portion of the data."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-3-homework/002_16711f1a2f_feature-elimination.md", "metadata": {"id": "16711f1a2f", "question": "Feature elimination", "sort_order": 2}, "content": "For Q5 in homework, should we calculate the smallest difference in accuracy in real values (i.e. -0.001 is less than -0.0002) or in absolute values (i.e. 0.0002 is less than 0.001)?\n\nWe should select the “smallest” difference, and not the “lowest”, meaning we should reason in absolute values.\n\nIf the difference is negative, it means that the model actually became better when we removed the feature."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-3-homework/003_cbd581f803_how-to-select-the-alpha-parameter-in-q6.md", "metadata": {"id": "cbd581f803", "question": "How to select the alpha parameter in Q6", "sort_order": 3}, "content": "To select the alpha parameter, you need to find the RMSE for each alpha. If RMSE scores are equal, choose the lowest alpha."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-3-homework/004_4b0519f085_features-for-homework-q5.md", "metadata": {"id": "4b0519f085", "question": "Features for homework: Q5", "sort_order": 4}, "content": "Do we need to train the model only with the features: total_rooms, total_bedrooms, population, and households, or with all the available features, then remove each of the previous features one at a time to compare accuracy?\n\n1. Create a list of all features and evaluate the model to obtain the original accuracy.\n2. Remove one feature at a time.\n3. Train the model each time, calculate the accuracy, and find the difference between the original accuracy and the new accuracy.\n4. Identify which feature has the smallest absolute accuracy difference.\n\nWhile calculating differences between accuracy scores, use the smallest absolute difference. For example, if the differences are -4 and -2, the smallest absolute difference is `abs(-2)`. Use this value to determine the impact of the feature on accuracy."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-3-homework/005_40abba5fd9_homework-3-use-of-random-seed.md", "metadata": {"id": "40abba5fd9", "question": "Homework 3: Use of random seed", "sort_order": 5}, "content": "For the `test_train_split` question on week 3's homework, are we supposed to use 42 as the `random_state` in both splits or only the first one?\n\nAnswer: For both splits, `random_state = 42` should be used."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-3-homework/006_b5c44e4f07_homework-q6-choosing-smaller-c-that-leads-to-best.md", "metadata": {"id": "b5c44e4f07", "question": "Homework Q6: Choosing smaller C that leads to best accuracy", "sort_order": 6}, "content": "When searching for the best value of `C` that yields the highest accuracy, be mindful that you should be looking for the maximum accuracy, not the minimum.\n\nAlthough the goal is to find the smallest `C` value, ensure that it corresponds to the highest accuracy achieved. Always prioritize accuracy maximization while minimizing `C`."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-3-homework/007_1a96e5837b_homework-i-am-getting-10-as-accuracy-should-i-use.md", "metadata": {"id": "1a96e5837b", "question": "Homework: I am getting 1.0 as accuracy. Should I use the closest option?", "sort_order": 7}, "content": "If you are getting 1.0 as accuracy, it is possible that you have overfitted the model. Dropping the column `msrp/price` can help you solve this issue."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-3-homework/008_a7e230bfb4_homework-q6-train-a-regularized-logistic-regressio.md", "metadata": {"id": "a7e230bfb4", "question": "Homework Q6: Train a regularized logistic regression with C=0.0", "sort_order": 8}, "content": "This is not possible since the parameter `C` represents the inverse of the regularization strength. Setting `C` to 0 means infinite regularization. Attempting this with the scikit-learn module of Logistic Regression will result in a `ValueError`."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-3/001_80e735cca8_what-is-the-best-way-to-handle-missing-values-in-t.md", "metadata": {"id": "80e735cca8", "question": "What is the best way to handle missing values in the dataset before training a regression model?", "sort_order": 1}, "content": "You can handle missing values by:\n\n- Imputing the missing values with the mean, median, or mode.\n- Using algorithms that support missing values inherently (e.g., some tree-based methods).\n- Removing rows or columns with missing data, depending on the extent of missingness.\n- Utilizing feature engineering to derive new features from incomplete data."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-3/002_56018d6f07_error-could-not-convert-string-to-float-nissan.md", "metadata": {"id": "56018d6f07", "question": "Error: Could not convert string to float: 'Nissan'", "sort_order": 2}, "content": "The error message \"could not convert string to float: 'Nissan'\" typically occurs when a machine learning model or function is expecting numerical input but receives a string instead. In this case, it seems like the model is trying to convert the car brand 'Nissan' into a numerical value, which isn’t possible.\n\nTo resolve this issue, you can encode categorical variables like car brands into numerical values. One common method is one-hot encoding, which creates new binary columns for each category/label present in the original column.\n\nHere’s an example of how you can perform one-hot encoding using pandas:\n\n```python\nimport pandas as pd\n\n# Assuming 'data' is your DataFrame and 'brand' is the column with car brands\ndata_encoded = pd.get_dummies(data, columns=['brand'])\n```\n\nIn this code, `pd.get_dummies()` creates a new DataFrame where the `brand` column is replaced with binary columns for each brand (e.g., `brand_Nissan`, `brand_Toyota`, etc.). Each row in the DataFrame has a 1 in the column that corresponds to its brand and 0 in all other brand columns."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-3/003_b8b91119fe_homework-why-did-we-change-the-targets-to-binary-f.md", "metadata": {"id": "b8b91119fe", "question": "Homework: Why did we change the targets to binary format when calculating mutual information score in the homework?", "sort_order": 3}, "content": "Mutual Information score calculates the relationship between categorical or discrete variables. In the homework, the target, `median_house_value`, was continuous. Thus, we changed it to a binary format to make its values discrete (either 0 or 1).\n\nKeeping the target as a continuous variable would require the algorithm to divide it into bins, which would be highly subjective. This is why continuous variables are not used for mutual information score calculation."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-3/004_abfdc60e4a_how-do-you-find-the-correlation-matrix.md", "metadata": {"id": "abfdc60e4a", "question": "How do you find the correlation matrix?", "sort_order": 4}, "content": "First, you have to consider whether the data is numerical or categorical. If it’s numerical, you can correlate it directly. If it’s categorical, you can find the correlations indirectly by vectorizing the data using One-Hot encoding or a similar method.\n\nTo determine if data is numerical, check the `dtypes` of the DataFrame. Data types such as integer and float are numerical, while types such as objects are categorical. You can correlate the numerical data by specifying which columns are numerical and using that as input to a correlation matrix.\n\nExample:\n\n```python\nnumerical = ['tenure', 'monthlycharges', 'totalcharges']\n\ncorrelation_matrix = df[numerical].corr()\nprint(correlation_matrix)\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-3/005_06248fbd4b_coloring-the-background-of-the-pandasdataframecorr.md", "metadata": {"id": "06248fbd4b", "images": [{"description": "image #1", "id": "image_1", "path": "images/machine-learning-zoomcamp/image_db91a31a.png"}], "question": "Coloring the background of the pandas.DataFrame.corr correlation matrix directly", "sort_order": 5}, "content": "The background of any DataFrame, including the correlation matrix, can be colored based on its numerical values using the method [pandas.io.formats.style.Styler.background_gradient](https://pandas.pydata.org/docs/reference/api/pandas.io.formats.style.Styler.background_gradient.html).\n\nHere is an example of how to color the correlation matrix. A color map of choice can be passed; here, 'viridis' is used:\n\n- Ensure the DataFrame contains only numerical values before calling `corr`:\n\n```python\ncorr_mat = df_numerical_only.corr()\n\ncorr_mat.style.background_gradient(cmap='viridis')\n```\n\n- Here is an example of how the coloring will look using a DataFrame containing random values and applying `background_gradient` to it:\n\n```python\nnp.random.seed = 3\n\ndf_random = pd.DataFrame(data=np.random.random(3*3).reshape(3,3))\n\nprint(df_random.style.background_gradient(cmap='viridis'))\n```\n\n<{IMAGE:image_1}>"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-3/006_45482efa5e_identifying-highly-correlated-feature-pairs-easily.md", "metadata": {"id": "45482efa5e", "images": [{"description": "image #2", "id": "image_2", "path": "images/machine-learning-zoomcamp/image_6476267e.png"}], "question": "Identifying highly correlated feature pairs easily through unstack", "sort_order": 6}, "content": "To identify highly correlated feature pairs using unstack:\n\n```python\nimport pandas as pd\n\ndata_corr = pd.DataFrame(data_num.corr().round(3).abs().unstack().sort_values(ascending=False))\n\nprint(data_corr.head(10))\n```\n\nYou can also use seaborn to create a heatmap with the correlation:\n\n```python\nimport seaborn as sns\n\nsns.heatmap(\n    df[numerical_features].corr(),\n    annot=True,\n    square=True,\n    fmt=\".2g\",\n    cmap=\"crest\"\n)\n```\n\nTo refine your heatmap and plot only a triangle, with a blue to red color gradient, that will show every correlation between your numerical variables without redundant information:\n\n```python\n# Set figure size: modify it here or create new function arguments\nplt.figure(figsize=(12, 6))\n\n# define the mask to set the values in the upper triangle to True\nmask = np.triu(np.ones_like(dataframe.corr(numeric_only=True), dtype=bool))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\n\nheatmap = sns.heatmap(\n    dataframe.corr(numeric_only=True),\n    mask=mask,\n    cmap=cmap,\n    vmin=-1,\n    vmax=1,\n    annot=True,\n    linewidths=0.5,\n)\n\nheatmap.set_title(title, fontdict={\"fontsize\": 18}, pad=16)\nplt.show()\n```\n\nWhich outputs, in the case of a churn dataset:\n\n<{IMAGE:image_2}>"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-3/007_0c6414cf51_what-data-should-be-used-for-eda.md", "metadata": {"id": "0c6414cf51", "question": "What data should be used for EDA?", "sort_order": 7}, "content": "It's indeed good practice to only rely on the train dataset for EDA. Including validation might be okay. But we aren't supposed to touch the test dataset; even just looking at it isn't a good idea. We indeed pretend that this is the future unseen data."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-3/008_356445714b_dictvectorizer-fitting-on-validation-data.md", "metadata": {"id": "356445714b", "question": "DictVectorizer: Fitting on validation data", "sort_order": 8}, "content": "Validation datasets are used to optimize models by providing an estimate of performance on unseen data. Understanding how to properly use the `DictVectorizer` class is crucial for maintaining this separation between training and validation.\n\n- **Fitting on Training Data**: The `fit` method of `DictVectorizer` analyzes the training dataset to determine how to map dictionary values. Categorical features are one-hot encoded, while numeric features remain unchanged.\n- **Avoid Fitting on Validation Data**: Applying the `fit` method to validation data can lead to information leakage, as it exposes the model to data it should not see during training.\n- **Appropriate Usage**:\n  1. Use `fit_transform` on the training dataset.\n  2. Use `transform` only on validation and test datasets.\n\nBy following these practices, the model's performance on new data can be more accurately assessed."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-3/009_c928a85179_futurewarning-function-get_feature_names-is-deprec.md", "metadata": {"id": "c928a85179", "question": "FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2", "sort_order": 9}, "content": "In newer versions of scikit-learn, the method has been replaced by `get_feature_names_out()`.\n\nInstead, use the method `.get_feature_names_out()` from the `DictVectorizer` function to resolve the warning.\n\n```python\n# Example usage\nfrom sklearn.feature_extraction import DictVectorizer\n\n# Initialize the vectorizer\nvectorizer = DictVectorizer()\n\n# After fitting the vectorizer\nvectorizer.fit_transform(...)\n\n# Get feature names\nfeature_names = vectorizer.get_feature_names_out()\n```\n\nNote: The warning indicates that `get_feature_names` will be removed, so switching to `get_feature_names_out` is recommended even though the warning itself won't cause issues yet."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-3/010_b27e47d7ca_logistic-regression-crashing-jupyter-kernel.md", "metadata": {"id": "b27e47d7ca", "question": "Logistic regression crashing Jupyter kernel", "sort_order": 10}, "content": "Fitting the logistic regression takes a long time or the kernel crashes when calling `predict()` with the fitted model.\n\nEnsure that the target variable for the logistic regression is binary."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-3/011_ef52b72189_understanding-ridge.md", "metadata": {"id": "ef52b72189", "question": "Understanding Ridge", "sort_order": 11}, "content": "Ridge regression is a linear regression technique used to mitigate the problem of multicollinearity (when independent variables are highly correlated) and prevent overfitting in predictive modeling. It adds a regularization term to the linear regression cost function, penalizing large coefficients.\n\n- **sag Solver**: The sag solver stands for \"Stochastic Average Gradient.\" It's particularly suitable for large datasets, as it optimizes the regularization term using stochastic gradient descent (SGD). sag can be faster than some other solvers for large datasets.\n\n- **Alpha**: The alpha parameter controls the strength of the regularization in Ridge regression. A higher alpha value leads to stronger regularization, which means the model will have smaller coefficient values, reducing the risk of overfitting.\n\n```python\nfrom sklearn.linear_model import Ridge\n\nridge = Ridge(alpha=alpha, solver='sag', random_state=42)\n\nridge.fit(X_train, y_train)\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-3/012_947bf26d84_pandasget_dummies-and-dictvectorizersparsefalse-pr.md", "metadata": {"id": "947bf26d84", "images": [{"description": "image #1", "id": "image_1", "path": "images/machine-learning-zoomcamp/image_e1899130.png"}], "question": "pandas.get_dummies() and DictVectorizer(sparse=False) produce the same type of one-hot encodings:", "sort_order": 12}, "content": "<{IMAGE:image_1}>\n\n`DictVectorizer(sparse=True)` produces [CSR](https://en.wikipedia.org/wiki/Sparse_matrix#Compressed_sparse_row_(CSR,_CRS_or_Yale_format)) format, which is both more memory efficient and converges better during fit().\n\nIt stores non-zero values and indices instead of adding a column for each class of each feature, which can result in large numbers of columns (e.g., models of cars).\n\nUsing \"sparse\" format is slower (around 6-8 minutes for Q6 task - Linear/Ridge Regression) for a high number of classes (like car models) and produces slightly worse results in both Logistic and Linear/Ridge Regression.\n\nIt also generates convergence warnings for Linear/Ridge Regression."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-3/013_feed303fc9_convergencewarning-the-max_iter-was-reached.md", "metadata": {"id": "feed303fc9", "question": "ConvergenceWarning: The max_iter was reached", "sort_order": 13}, "content": "If you're encountering the following warning:\n\nConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n\nThis usually happens because the solver the model uses\nis sensitive to feature scales.\n\nYou can do the following to address it:\n\n\n1. **Normalize Numerical Features**  \n   - Scale your numerical features using techniques like `StandardScaler` or `MinMaxScaler`.  \n   - This ensures that all numerical features are on a similar scale, which helps the solver converge.  \n\n2. **Encode Categorical Features**  \n   - Apply `OneHotEncoder` (OHE) to categorical features to represent them as binary vectors.  \n   - Use `sparse=False` when necessary to return a dense array.  \n\n3. **Separate and Combine Features**  \n   - Process numerical and categorical features separately (scaling for numerical, OHE for categorical).  \n   - Combine them afterward into a single feature matrix (`X_train`) to use as input for Ridge regression.  \n\n4. **Experiment with Different Scalers**  \n   - If issues persist, try different scalers as Ridge can behave differently depending on feature scaling.  \n\nBy following these steps, you can reduce convergence errors and improve model stability. For a detailed example, see this notebook: [notebook-scaling-ohe.ipynb](https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/03-classification/notebook-scaling-ohe.ipynb)."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-3/014_0d7b521e27_sparse-matrix-compared-to-dense-matrix.md", "metadata": {"id": "0d7b521e27", "question": "Sparse matrix compared to dense matrix", "sort_order": 14}, "content": "A sparse matrix is more memory-efficient because it only stores the non-zero values and their positions in memory. This is particularly useful when working with large datasets with many zero or missing values.\n\nThe default `DictVectorizer` configuration is a sparse matrix. For Week 3, Question 6, using the default sparse configuration is beneficial due to the size of the matrix. Training the model was also more performant and didn’t produce an error message like dense mode."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-3/015_9610a0de1d_how-to-disableavoid-warnings-in-jupyter-notebooks.md", "metadata": {"id": "9610a0de1d", "question": "How to Disable/avoid Warnings in Jupyter Notebooks", "sort_order": 15}, "content": "The warnings in Jupyter notebooks can be disabled or avoided with the following commands:\n\n```python\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-3/016_f969361ef4_homework-could-you-please-help-me-with-hw3-q3-calc.md", "metadata": {"id": "f969361ef4", "question": "Homework: Could you please help me with HW3 Q3: \"Calculate the mutual information score with the (binarized) price for the categorical variable that we have. Use the training set only.\" What is the second variable that we need to use to calculate the mutual information score?", "sort_order": 16}, "content": "You need to calculate the mutual information score between the binarized price (above_average) variable and `ocean_proximity`, the only original categorical variable in the dataset."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-3/017_e35e6adc78_what-is-the-difference-between-onehotencoder-and-d.md", "metadata": {"id": "e35e6adc78", "question": "What is the difference between OneHotEncoder and DictVectorizer?", "sort_order": 17}, "content": "Both work in similar ways to convert categorical features to numerical variables for use in training a model. The difference lies in the input:\n\n- **OneHotEncoder** uses an array as input.\n- **DictVectorizer** uses a dictionary.\n\nBoth will produce the same result. However, with OneHotEncoder, features are sorted alphabetically. With DictVectorizer, you stack features as desired."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-3/018_146d28247c_what-is-the-difference-between-pandas-get_dummies.md", "metadata": {"id": "146d28247c", "question": "What is the difference between pandas get_dummies and sklearn OnehotEncoder?", "sort_order": 18}, "content": "They are basically the same. There are some key differences with regards to their input/output types, handling of missing values, etc., but they are both techniques to one-hot-encode categorical variables with identical results. \n\n- **pandas get_dummies**: A convenient choice when working with Pandas DataFrames.\n- **sklearn OneHotEncoder**: More suitable for building a scikit-learn-based machine learning pipeline to handle categorical data as part of that pipeline."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-3/019_c85cd9f595_correlation-before-or-after-splitting-the-data.md", "metadata": {"id": "c85cd9f595", "question": "Correlation before or after splitting the data", "sort_order": 19}, "content": "Correlation should be calculated after splitting the data, specifically on the train dataset.\n\nTo find the two most correlated features:\n\n1. Generate the correlation matrix of the train dataset.\n2. Identify the pair of features with the highest absolute correlation coefficient."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-3/020_c4be895874_transforming-non-numerical-columns-into-numerical.md", "metadata": {"id": "c4be895874", "question": "Transforming Non-Numerical Columns into Numerical Columns", "sort_order": 20}, "content": "Use [sklearn.preprocessing](https://scikit-learn.org/stable/modules/preprocessing.html) encoders and scalers, e.g. `OneHotEncoder`, `OrdinalEncoder`, and `StandardScaler`."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-3/021_862909f457_what-is-the-better-option-featurehasher-or-dictvec.md", "metadata": {"id": "862909f457", "question": "What is the better option FeatureHasher or DictVectorizer?", "sort_order": 21}, "content": "These methods both receive a dictionary as input. While the `DictVectorizer` will store a large vocabulary and take up more memory, `FeatureHasher` creates vectors with a predefined length. They are both used for handling categorical features.\n\n- If you have high cardinality in categorical features, it's better to use `FeatureHasher`.\n- If you want to preserve feature names in transformed data and have a small number of unique values, use `DictVectorizer`.\n\nYour choice will depend on your data. For more information, you can visit [scikit-learn.org](https://scikit-learn.org/stable/auto_examples/text/plot_hashing_vs_dict_vectorizer.html)"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-3/022_03e27c386a_isnt-it-easier-to-use-dictvertorizer-or-get-dummie.md", "metadata": {"id": "03e27c386a", "question": "Isn't it easier to use DictVertorizer or get dummies before splitting the data into train/val/test? Is there a reason we wouldn't do this? Or is it the same either way?", "sort_order": 22}, "content": "The reason it's recommended to do it after splitting is to avoid data leakage. You don't want any data from the test set influencing the training stage, similarly from the validation stage in the initial training. See e.g. scikit-learn documentation on \"Common pitfalls and recommended practices\": [https://scikit-learn.org/stable/common_pitfalls.html](https://scikit-learn.org/stable/common_pitfalls.html)"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-3/023_6be83091d9_encoding-techniques.md", "metadata": {"id": "6be83091d9", "question": "Encoding Techniques", "sort_order": 23}, "content": "This article explains different encoding techniques used.\n\n[All About Categorical Variable Encoding](https://towardsdatascience.com/all-about-categorical-variable-encoding-305f3361fd02)"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-3/024_5686aea16e_error-in-use-of-accuracy_score-from-sklearn-in-jup.md", "metadata": {"id": "5686aea16e", "question": "Error in use of accuracy_score from sklearn in Jupyter (sometimes)", "sort_order": 24}, "content": "I got this error multiple times; here is the code:\n\n```python\naccuracy_score(y_val, y_pred >= 0.5)\n```\n\n```\nTypeError: 'numpy.float64' object is not callable\n```\n\nI solved it using:\n\n```python\nfrom sklearn import metrics\n\nmetrics.accuracy_score(y_train, y_pred >= 0.5)\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-3/025_28f7297895_what-is-the-difference-between-decision_function-a.md", "metadata": {"id": "28f7297895", "images": [{"description": "image #1", "id": "image_1", "path": "images/machine-learning-zoomcamp/image_d2a8633d.png"}], "question": "What is the difference between .decision_function() and .predict_proba()?", "sort_order": 25}, "content": "In Scikit-Learn’s LogisticRegression, a model that is trained will have raw values and the predicted probabilities.\n\n- **.decision_function()** returns raw values that are a linear combination of the features and weights, similar to the output of Linear Regression.\n\n- **.predict_proba()** goes one step further by inputting these raw values into the sigmoid function to convert them into probabilities (between 0 and 1).\n\n<{IMAGE:image_1}>"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-3/026_1fe12ec81c_why-do-i-get-a-keyerror-when-dropping-features-aft.md", "metadata": {"id": "1fe12ec81c", "question": "Why do I get a KeyError when dropping features after one-hot encoding?", "sort_order": 26}, "content": "The error occurs because some features you try to drop have been one-hot encoded into multiple columns. After encoding, the original column may no longer exist, leading to the KeyError. To resolve this, identify and drop all related one-hot encoded columns (e.g., those starting with the original feature name) instead of the original feature itself.\n\nFor example, after one-hot encoding, the column `marital` could have been split into columns like `marital_single`, `marital_married`, etc. This means that the original column `marital` no longer exists, leading to the KeyError."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-4-homework/001_a64b95ea2c_multiple-thresholds-for-q4.md", "metadata": {"id": "a64b95ea2c", "question": "Multiple thresholds for Q4", "sort_order": 1}, "content": "I am getting multiple thresholds with the same F1 score. Does this indicate I am doing something wrong, or is there a method for choosing? Should I just pick the lowest?\n\n- Choose the threshold closest to any of the options.\n\nYou can also use `scikit-learn` (or other standard libraries/packages) to verify results obtained using your own code. For example, use `classification_report` to obtain precision, recall, and F1-score.\n\nRefer to the documentation: [scikit-learn classification_report](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html)"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-4-homework/002_186ed3fc35_homework-im-not-getting-the-exact-result.md", "metadata": {"id": "186ed3fc35", "question": "Homework: I’m not getting the exact result", "sort_order": 2}, "content": "That’s fine, use the closest option."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-4-homework/003_7c90af9692_what-dataset-should-i-use-to-compute-the-metrics-i.md", "metadata": {"id": "7c90af9692", "question": "What dataset should I use to compute the metrics in Question 3", "sort_order": 3}, "content": "You must use the `dt_val` dataset to compute the metrics asked in Question 3 and onwards, as you did in Question 2."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-4-homework/004_why-do-i-have-different-values.md", "metadata": {"id": "8351543816", "question": "Homework: Why do I have different values of accuracy than the options in the homework?", "sort_order": 4}, "content": "One main reason behind this issue is the method of splitting the data. For example, if we want to split the data into train/validation/test with the ratios 60%/20%/20%, different methods may yield different results even if the final ratios are the same.\n\n1. Method 1:\n   \n   ```python\n   df_train, df_temp = train_test_split(df, test_size=0.4, random_state=42)\n   df_val, df_test = train_test_split(df_temp, test_size=0.5, random_state=42)\n   ```\n\n2. Method 2:\n   \n   ```python\n   df_full_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\n   df_train, df_val = train_test_split(df_full_train, test_size=0.25, random_state=42)\n   ```\n\nWhile both methods achieve the same ratio, the data split differently, resulting in variations in accuracy. It is recommended to use the second method, as it is more consistent with the lessons and homeworks."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-4-homework/005_f7f0800d02_how-to-evaluate-feature-importance-for-numerical-v.md", "metadata": {"id": "f7f0800d02", "question": "How to evaluate feature importance for numerical variables with AUC?", "sort_order": 5}, "content": "You can use the `roc_auc_score` function from the `sklearn.metrics` module. Pass the vector of the target variable (e.g., `above_average`) as the first argument and the vector of feature values as the second one. This function will return the AUC score for the feature that was passed as the second argument.\n\n```python\nfrom sklearn.metrics import roc_auc_score\n\n# Example usage:\nauc_score = roc_auc_score(target_variable, feature_values)\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-4-homework/006_1b89887a15_homework-q1-is-not-clear-to-me-what-do-i-do-here.md", "metadata": {"id": "1b89887a15", "question": "Homework Q1 is not clear to me. What do I do here?", "sort_order": 6}, "content": "Q1 is not making sense to me. The score should be between 0 to 1. I tried computing `roc_curve (df_train['age'], y)` and the graph does not have the model line. Can anyone clarify?'\n\nThe idea of the question is to evaluate the importance of features with respect to the prediction of the binary target variable (yes/no).\n\nIn my case, I did the following:\n\n1. Identified the numerical features in the dataset.\n2. For each feature in the list of numerical features, I calculated the AUC:\n   \n   ```python\n   roc_auc_score(y_target, feature_vector)\n   ```\n   \n   Here, `y_target` is the target variable, and `feature_vector` contains the values for each numerical column in the train dataset.\n\n3. Created a data frame with two columns: the name of the numerical feature and the ROC AUC score.\n4. Sorted the data frame by the ROC AUC score to determine the numerical feature with the highest ROC AUC."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-4/001_56ebfe8111_homework-how-do-i-import-data-from-bank-fullcsv.md", "metadata": {"id": "56ebfe8111", "question": "Homework: How do I import data from 'bank-full.csv'?", "sort_order": 1}, "content": "Import the data using the following command:\n\n```python\nimport pandas as pd\n\ndf = pd.read_csv(\"bank-full.csv\", sep=';')\n```\n\nNote that the data is separated by a semicolon, not a comma."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-4/001_afd915ae5b_module-4-evaluation-overview.md", "metadata": {"id": "afd915ae5b", "question": "What exactly is taught in the evaluation module?", "sort_order": 1}, "content": "In Module 4, you’ll learn the core evaluation concepts used in classification tasks. Topics include:\n- Metrics and diagnostics: precision, recall, ROC curves, and precision-recall curves\n- Evaluation mindsets: how to think critically about metrics and validation in ML projects\n- Common pitfalls: data leakage, improper validation, misinterpretation of metrics and curves\n- Practical interpretation: selecting metrics based on context (class balance, costs of errors) and conveying results clearly\n- Real-world applicability: how these concepts guide model comparison, threshold selection, and reporting\nThis module is designed to be conceptual and abstract, yet practical for real-world ML work."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-4/002_636cd61ba8_why-do-we-sometimes-use-random_state-and-not-at-ot.md", "metadata": {"id": "636cd61ba8", "question": "Why do we sometimes use random_state and not at other times?", "sort_order": 2}, "content": "Refer to the sklearn documentation, `random_state` is used to ensure the \"randomness\" that is used to shuffle the dataset is reproducible. It typically requires both `random_state` and `shuffle` parameters to be set accordingly."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-4/003_234fd096e4_how-to-get-all-classification-metrics.md", "metadata": {"id": "234fd096e4", "question": "How to get all classification metrics?", "sort_order": 3}, "content": "To get classification metrics like precision, recall, F1 score, and accuracy simultaneously, use `classification_report` from `sklearn`.\n\nFor more information, check [here](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html)."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-4/004_fd83a2e6ad_valueerror-this-solver-needs-samples-of-at-least-2.md", "metadata": {"id": "fd83a2e6ad", "question": "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 0", "sort_order": 4}, "content": "This error indicates that your dataset's `churn` column only contains the class `0`, but at least two classes are required.\n\n\n1. Check your data processing steps where binary conversion might be applied. Specifically, ensure that the line:\n   \n   ```python\n   df.churn = (df.churn == 'yes').astype(int)\n   ```\n   \n   is operating correctly. Verify that there are indeed records where `churn` should evaluate to `1` (i.e., cases where `churn` equals `'yes'`).\n\n2. If all values are `0`, make sure your original dataset and preprocessing steps are correctly implemented to represent cases with both classes (`0` and `1`).\n\n3. Review data preprocessing steps and confirm the filtering, transformation, or data importing steps do not inadvertently drop or misclassify the non-zero class records.\n\nThis should resolve the error by ensuring your data contains at least one record for each class."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-4/005_ebcd9b6783_method-to-get-beautiful-classification-report.md", "metadata": {"id": "ebcd9b6783", "question": "Method to get beautiful classification report", "sort_order": 5}, "content": "Use Yellowbrick. Yellowbrick is a library that combines scikit-learn with matplotlib to produce visualizations for your models. It produces colorful classification reports."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-4/006_573c63b800_use-auc-to-evaluate-feature-importance-of-numerica.md", "metadata": {"id": "573c63b800", "question": "Use AUC to evaluate feature importance of numerical variables", "sort_order": 6}, "content": "Check the solutions from the 2021 iteration of the course. You should use [roc_auc_score.](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html)"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-4/007_c213e31ea0_how-to-use-auc-for-numerical-features.md", "metadata": {"id": "c213e31ea0", "question": "How to use AUC for numerical features?", "sort_order": 7}, "content": "When calculating the ROC AUC score using [`sklearn.metrics.roc_auc_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html), the function expects two parameters: `y_true` and `y_score`. For each numerical value in the dataframe, it will be passed as the `y_score` to the function, and the target variable will be passed as `y_true` each time."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-4/008_24d2042733_what-does-kfold-do.md", "metadata": {"id": "24d2042733", "question": "What does KFold do?", "sort_order": 8}, "content": "KFold is a cross-validation technique that splits your dataset into k equal parts (folds). It trains the model k times, each time using a different fold as the validation set while training on the remaining folds. This process helps provide a more reliable estimate of a model's performance by ensuring every data point gets to be in both the training and validation sets. The average score across all folds offers a robust evaluation, minimizing the risk of overfitting to a specific train-test split.\n\n### What does this line do?\n\n```python\nKFold(n_splits=n_splits, shuffle=True, random_state=1)\n```\n\n- **Positioning in Code:** Whether you instantiate KFold inside the loop over different regularization values like `[0.01, 0.1, 1, 10]` or outside, it typically does not affect your answer. This is because `KFold` is essentially a generator object containing the information `n_splits`, `shuffle`, and `random_state`.\n\n- **Impact of Random State:** Changing the `random_state` can yield different results because it affects how the data is shuffled. However, the creation of the `KFold` object, either inside or outside a loop, doesn't make a difference as long as the configuration (`n_splits`, `shuffle`, `random_state`) remains constant.\n\n- **Best Practice:** It is recommended to create the `KFold` object before the loop to avoid unnecessary repetition:\n\n  ```python\n  kFold = KFold(n_splits=n_splits, shuffle=True, random_state=1)\n  for C in [0.01, 0.1, 1, 10]:\n      for train_idx, val_idx in kFold.split(df_full_train):\n          # train and evaluate model\n  ```\n\nFor more details, you can refer to the official [scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html)."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-4/009_valueerror-multi_class-must-b.md", "metadata": {"id": "2488656439", "question": "ValueError: multi_class must be in ('ovo', 'ovr')", "sort_order": 9}, "content": "I'm getting \"ValueError: multi_class must be in ('ovo', 'ovr')\" when using `roc_auc_score` to evaluate feature importance of numerical variables in question 1.\n\nThis error occurs because the parameters were passed to `roc_auc_score` incorrectly. Here is the correct usage:\n\n```python\nroc_auc_score(y_train, df_train[col])\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-4/010_6a8dccc107_what-is-the-use-of-inverting-or-negating-the-varia.md", "metadata": {"id": "6a8dccc107", "question": "What is the use of inverting or negating the variables less than the threshold?", "sort_order": 10}, "content": "Inverting or negating variables with ROC AUC scores less than the threshold is a valuable technique to improve feature importance and model performance when dealing with negatively correlated features. It helps ensure that the direction of the correlation aligns with the expectations of most machine learning algorithms."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-4/011_139ae54c4f_difference-between-predictx-and-predict_probax-1.md", "metadata": {"id": "139ae54c4f", "question": "Difference between `predict(X)` and `predict_proba(X)[:, 1]`", "sort_order": 11}, "content": "Using `predict(X)` provides binary classification predictions, which are either 0 or 1. This could result in inaccurate evaluation values.\n\nThe alternative is to use `predict_proba(X)[:, 1]`, which gives the probability that the value belongs to a specific class.\n\n`predict_proba` displays probabilities for each class."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-4/012_921879ea38_why-are-fpr-and-tpr-equal-to-00-when-threshold-10.md", "metadata": {"id": "921879ea38", "question": "Why are FPR and TPR equal to 0.0, when threshold = 1.0?", "sort_order": 12}, "content": "For churn/not churn predictions, when the threshold is 1.0:\n\n- **FPR** (False Positive Rate) is 0.0\n- **TPR** (True Positive Rate) is 0.0\n\nWhen the threshold is set to 1.0, the condition for belonging to the positive class (churn class) is `g(x) >= 1.0`. However, `g(x)` is a sigmoid function in a binary classification problem, which produces values between 0 and 1. The function never reaches the outermost values of 0 or 1.\n\nTherefore, no sample will satisfy the condition for the positive class (churn), resulting in no positive (churn) predictions. Consequently, this leads to both the false positive and true positive rates being 0.0 when the threshold is 1.0."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-4/013_80176f658d_how-can-i-annotate-a-graph.md", "metadata": {"id": "80176f658d", "question": "How can I annotate a graph?", "sort_order": 13}, "content": "Matplotlib has a cool method to [annotate](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.annotate.html) where you can provide an X,Y point and annotate with an arrow and text. For example, this will show an arrow pointing to the x,y point optimal threshold.\n\n```python\nplt.annotate(f'Optimal Threshold: {optimal_threshold:.2f}\\nOptimal F1 Score: {optimal_f1_score:.2f}',\n\nxy=(optimal_threshold, optimal_f1_score),\n\nxytext=(0.3, 0.5),\n\ntextcoords='axes fraction',\n\narrowprops=dict(facecolor='black', shrink=0.05))\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-4/014_3c04f56b5d_i-didnt-fully-understand-the-roc-curve-can-i-move.md", "metadata": {"id": "3c04f56b5d", "question": "I didn’t fully understand the ROC curve. Can I move on?", "sort_order": 14}, "content": "It's a complex and abstract topic and it requires some time to understand. You can move on without fully understanding the concept.\n\nNonetheless, it might be useful for you to rewatch the video, or even watch videos/lectures/notes by other people on this topic, as the ROC AUC is one of the most important metrics used in Binary Classification models."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-4/015_fed6cad07b_how-to-find-the-intercept-between-precision-and-re.md", "metadata": {"id": "fed6cad07b", "question": "How to find the intercept between precision and recall curves by using numpy?", "sort_order": 15}, "content": "You can find the intercept between these two curves using numpy's `diff` and `sign` functions:\n\n1. Ensure your `df_scores` DataFrame is ready with three columns: `threshold`, `precision`, and `recall`.\n2. Determine the indices where the precision and recall curves intersect (i.e., where the sign of the difference between precision and recall changes):\n\n   ```python\n   import numpy as np\n\n   idx = np.argwhere(\n       np.diff(\n           np.sign(np.array(df_scores['precision']) - np.array(df_scores['recall']))\n       )\n   ).flatten()\n   ```\n\n3. Print the result to easily read it:\n\n   ```python\n   print(f\"The precision and recall curves intersect at a threshold equal to {df_scores.loc[idx]['threshold']}.\")\n   ```"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-4/016_02cd286eb8_compute-recall-precision-and-f1-score-using-scikit.md", "metadata": {"id": "02cd286eb8", "question": "Compute Recall, Precision, and F1 Score using scikit-learn library", "sort_order": 16}, "content": "You can use the Scikit Learn library to calculate precision, recall, and F1 score without having to define true positive, true negative, false positive, and false negative manually.\n\n```python\nfrom sklearn.metrics import precision_score, recall_score, f1_score\n\nprecision = precision_score(y_true, y_pred, average='binary')\nrecall = recall_score(y_true, y_pred, average='binary')\nf1 = f1_score(y_true, y_pred, average='binary')\n```\n\nReplace `y_true` and `y_pred` with your actual data. The `average` parameter is set to `'binary'` by default for binary classification."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-4/017_4c3e29589e_why-do-we-use-cross-validation.md", "metadata": {"id": "4c3e29589e", "question": "Why do we use cross validation?", "sort_order": 17}, "content": "Cross-validation evaluates the performance of a model and chooses the best hyperparameters. It does this by splitting the dataset into multiple parts (folds), typically 5 or 10. It then trains and evaluates your model multiple times, each time using a different fold as the validation set and the remaining folds as the training set.\n\n\"C\" is a hyperparameter typically associated with regularization in models like Support Vector Machines (SVM) and logistic regression.\n\n- **Smaller \"C\" values**: They introduce more regularization, which means the model will try to find a simpler decision boundary, potentially underfitting the data. This is because it penalizes the misclassification of training examples more severely.\n\n- **Larger \"C\" values**: They reduce the regularization effect, allowing the model to fit the training data more closely, potentially overfitting. This is because it penalizes misclassification less severely, allowing the model to prioritize getting training examples correct."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-4/018_861531763b_evaluate-the-model-using-scikit-learn-metrics.md", "metadata": {"id": "861531763b", "question": "Evaluate the Model using scikit learn metrics", "sort_order": 18}, "content": "Model evaluation metrics can be easily computed using the off-the-shelf calculations available in the scikit-learn library. This method is more precise compared to calculating from scratch using numpy and pandas libraries.\n\n```python\nfrom sklearn.metrics import (\n    accuracy_score,\n    precision_score,\n    recall_score,\n    f1_score,\n    roc_auc_score\n)\n\naccuracy = accuracy_score(y_val, y_pred)\nprecision = precision_score(y_val, y_pred)\nrecall = recall_score(y_val, y_pred)\nf1 = f1_score(y_val, y_pred)\nroc_auc = roc_auc_score(y_val, y_pred)\n\nprint(f'Accuracy: {accuracy}')\nprint(f'Precision: {precision}')\nprint(f'Recall: {recall}')\nprint(f'F1-Score: {f1}')\nprint(f'ROC AUC: {roc_auc}')\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-4/019_70eceb1a01_are-there-other-ways-to-compute-precision-recall-a.md", "metadata": {"id": "70eceb1a01", "question": "Are there other ways to compute Precision, Recall and F1 score?", "sort_order": 19}, "content": "Scikit-learn offers another way: `precision_recall_fscore_support`.\n\nExample:\n\n```python\nfrom sklearn.metrics import precision_recall_fscore_support\n\nprecision, recall, fscore, support = precision_recall_fscore_support(y_val, y_val_pred, zero_division=0)\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-4/020_18e9b4b2a2_when-do-i-use-roc-vs-precision-recall-curves.md", "metadata": {"id": "18e9b4b2a2", "question": "When do I use ROC vs Precision-Recall curves?", "sort_order": 20}, "content": "ROC curves are appropriate when the observations are balanced between each class, whereas precision-recall curves are appropriate for imbalanced datasets.\n\nThe reason for this recommendation is that ROC curves present an optimistic picture of the model on datasets with a class imbalance. This is because of the use of true negatives in the False Positive Rate in the ROC Curve and the careful avoidance of this rate in the Precision-Recall curve.\n\nIf the proportion of positive to negative instances changes in a test set, the ROC curves will not change. Metrics such as accuracy, precision, lift, and F scores use values from both columns of the confusion matrix. As a class distribution changes these measures will change as well, even if the fundamental classifier performance does not. ROC graphs are based upon TP rate and FP rate, in which each dimension is a strict columnar ratio, so cannot give an accurate picture of performance when there is class imbalance."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-4/021_bdc4c489fd_dependence-of-the-f-score-on-class-imbalance.md", "metadata": {"id": "bdc4c489fd", "question": "Dependence of the F-score on class imbalance", "sort_order": 21}, "content": "Precision-recall curves, and thus the F-score, explicitly depend on the ratio of positive to negative test cases. This means that comparing the F-score across different problems with differing class ratios can be problematic. \n\nOne way to address this issue is to use a standard class ratio when making such comparisons."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-4/022_ce5f41e600_quick-way-to-plot-precision-recall-curve.md", "metadata": {"id": "ce5f41e600", "question": "Quick way to plot Precision-Recall Curve", "sort_order": 22}, "content": "We can import `precision_recall_curve` from scikit-learn and plot the graph as follows:\n\n```python\nfrom sklearn.metrics import precision_recall_curve\n\nprecision, recall, thresholds = precision_recall_curve(y_val, y_predict)\n\nplt.plot(thresholds, precision[:-1], label='Precision')\nplt.plot(thresholds, recall[:-1], label='Recall')\nplt.legend()\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-4/023_f29b4790fc_what-is-stratified-k-fold.md", "metadata": {"id": "f29b4790fc", "question": "What is Stratified k-fold?", "sort_order": 23}, "content": "For multiclass classification, it is important to keep class balance when you split the dataset. Stratified k-fold returns folds that contain approximately the same percentage of samples of each class.\n\nPlease check the implementation in the scikit-learn library: [https://scikit-learn.org/stable/modules/cross_validation.html#stratified-k-fold](https://scikit-learn.org/stable/modules/cross_validation.html#stratified-k-fold)"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-4/024_f6aa440092_why-is-accuracy-not-always-the-best-metric-for-eva.md", "metadata": {"id": "f6aa440092", "question": "Why is accuracy not always the best metric for evaluating a classification model?", "sort_order": 24}, "content": "Accuracy is the proportion of correct predictions made by the model, but it can be misleading, especially with imbalanced datasets. For example, if 95% of your data belongs to one class, a model that always predicts this majority class will have high accuracy, even though it completely fails to identify the minority class. In such cases, metrics like precision, recall, F1-score, or AUROC might be more appropriate, as they provide a clearer view of model performance on both classes."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-4/025_13102dfe14_how-to-easily-remember-precision-and-recall.md", "metadata": {"id": "13102dfe14", "question": "How to easily remember precision and recall?", "sort_order": 25}, "content": "Precision is `TruePositive/PredictedPositive` and recall means `TruePositive / ActualPositive`.\n\n- **Precision:** Precise predictions (how accurate are our YES predictions?)\n- **Recall:** Remembering (how many real YES cases did we find?)"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-4/026_79b2760cc5_how-do-i-interpret-precision-and-recall.md", "metadata": {"id": "79b2760cc5", "question": "How do I interpret precision and recall?", "sort_order": 26}, "content": "**Precision:**\n\nMemory tip: Think of Precision as \"How Precise Are Our Positive Predictions?\" It relates to the accuracy of the positive results, emphasizing how many of the predicted positive instances are actually correct.\n\n**Interpretation:**\n\n- **High Precision:**\n  - Most of the predicted positives are correct.\n  - This makes the model more reliable.\n\n- **Low Precision:**\n  - Indicates a higher rate of false positives.\n  - This decreases trust in the positive predictions.\n\n**When to prioritize precision:** In scenarios like email spam detection, where marking a legitimate email as spam (false positive) can lead to missed communications, high precision is preferred to ensure that most flagged emails are indeed spam.\n\n**Recall:**\n\nMemory tip: Think of Recall as \"How Sensitive Are We to the Positives?\" It emphasizes capturing all actual positive cases. A high recall means that the model is good at identifying most of the positives.\n\n**Interpretation:**\n\n- **High Recall:**\n  - The model captures most of the true positives.\n  - This is crucial in situations where missing a positive case is costly.\n\n- **Low Recall:**\n  - Many actual positives are overlooked.\n  - This highlights potential issues in detection.\n\n**When to prioritize recall:** In medical diagnostics for a severe or highly contagious disease, missing a true positive (an actual case of the disease) can have serious public health implications.\n\n**Balancing Precision and Recall:**\n\n- Improving one metric may lead to a decrease in the other.\n- The choice between precision and recall depends on specific goals and acceptable trade-offs in a given application."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-4/027_385807829b_how-to-address-undefinedmetricwarning-f-score-is-i.md", "metadata": {"id": "385807829b", "question": "How to address UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples?", "sort_order": 27}, "content": "This warning occurs when your model doesn't predict any samples for certain labels, causing a zero-division error when calculating the F-score. The warning is triggered when there are no true positives or predicted positives for certain labels, leading to undefined precision or recall.\n\nTo address this, you can use the `zero_division` parameter in scikit-learn's `f1_score` function. This parameter defines what should happen in cases of zero division:\n\n- **Set `zero_division=1`**: This will set the precision, recall, and F-score to 1 when no positive samples are predicted.\n- **Set `zero_division=0`**: This is the default behavior, setting the metric to 0 when there are no predicted samples for a given label.\n- **Set `zero_division='warn'`**: This is the default behavior, acts like 0 but also raises a warning.\n\nExample usage:\n\n```python\nfrom sklearn.metrics import f1_score, precision_score, recall_score\n\n# For precision score\nprecision = precision_score(y_true, y_pred, average='weighted', zero_division='warn')\n\n# For recall score\nrecall = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n\n# For f1-score\nf1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-5-homework/001_a5ee2dc890_docker-i-cannot-pull-the-image-with-docker-pull-co.md", "metadata": {"id": "a5ee2dc890", "question": "Docker: I cannot pull the image with docker pull command", "sort_order": 1}, "content": "**Problem:** When trying to pull the image using the `docker pull svizor/zoomcamp-model` command, an error occurs:\n\n```bash\nUsing default tag: latest\nError response from daemon: manifest for svizor/zoomcamp-model:latest not found: manifest unknown: manifest unknown\n```\n\n**Solution:** Docker defaults to the `latest` tag. To resolve this, use the correct tag from the image description. Use the following command:\n\n```bash\ndocker pull svizor/zoomcamp-model:3.10.12-slim\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-5-homework/002_32c95df124_error-failed-to-compute-cache-key-model2bin-not-fo.md", "metadata": {"id": "32c95df124", "question": "Error: failed to compute cache key: \"/model2.bin\" not found: not found", "sort_order": 2}, "content": "Initially, I did not assume there was a `model2`. I copied the original `model1.bin` and `dv.bin`. Then when I tried to load using:\n\n```dockerfile\nCOPY [\"model2.bin\", \"dv.bin\", \"./\"]\n```\n\nI got the error above in MINGW64 (git bash) on Windows.\n\nThe temporary solution I found was to use:\n\n```dockerfile\nCOPY [\"*\", \"./\"]\n```\n\nThis seems to combine all the files from the original Docker image and the files in your working directory."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-5-homework/003_3ab0a03ad2_homework-q6-which-model-and-dict-vectorizer-to-use.md", "metadata": {"id": "3ab0a03ad2", "question": "Homework Q6: Which model and dict vectorizer to use?", "sort_order": 3}, "content": "The provided image `FROM svizor/zoomcamp-model:3.10.12-slim` has a model and DictVectorizer that should be used for question 6. \"model2.bin\", \"dv.bin\"."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-5-homework/004_fa4068ee43_i-have-m1-and-dont-use-docker-desktop.md", "metadata": {"id": "fa4068ee43", "question": "I have M1 and don't use Docker Desktop.", "sort_order": 4}, "content": "- If you replaced Docker Desktop with 'lima', you can create an instance of Lima using the [following template](https://gist.github.com/akrylysov/7c1ea3bac409da2758e525f2f82e6373). Follow the instructions listed on the page to create an instance using the supplied template.\n- Switch your current Docker context to the context associated with this new (running) image.\n- Use `svizor/zoomcamp-model:3.11.5-slim` as a base image and run your built image without issues.\n\nSimple Solution:\n\n- Specify the platform:\n\n  ```bash\n  docker run --platform linux/amd64 -it --rm -p 9696:9696 <your-docker-image-name>\n  ```"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-5/001_a638d65685_errors-related-to-the-default-environment-wsl-ubun.md", "metadata": {"id": "a638d65685", "question": "Errors related to the default environment: WSL, Ubuntu, proper Python version, installing pipenv etc.", "sort_order": 1}, "content": "While weeks 1-4 can relatively easily be followed and the associated homework completed with just about any default environment or local setup, week 5 introduces several layers of abstraction and dependencies.\n\nIt is advised to prepare your “homework environment” with a cloud provider of your choice. A thorough step-by-step guide for doing so for an AWS EC2 instance is provided in an introductory video taken from the MLOPS course here:\n\n[https://www.youtube.com/watch?v=IXSiYkP23zo](https://www.youtube.com/watch?v=IXSiYkP23zo)\n\nNote that (only) small instances can be run for free, and larger ones will be billed hourly based on usage (but can and should be stopped when not in use).\n\nAlternative ways are sketched here: [https://github.com/alexeygrigorev/mlbookcamp-code/blob/master/course-zoomcamp/01-intro/06-environment.md](https://github.com/alexeygrigorev/mlbookcamp-code/blob/master/course-zoomcamp/01-intro/06-environment.md)"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-5/002_85fa1b961a_how-to-download-csv-data-via-jupyter-nb-and-the-ka.md", "metadata": {"id": "85fa1b961a", "question": "How to download CSV data via Jupyter NB and the Kaggle API, for one seamless experience", "sort_order": 2}, "content": "To download CSV data via Jupyter Notebook using the Kaggle API, follow these steps:\n\n1. **Set up a Kaggle account**:\n   - Go to your Kaggle account settings, navigate to the API section, and click `Create New Token`. This will download a `kaggle.json` file containing your `username` and `key`.\n\n2. **Place the `kaggle.json` file**:\n   - Ensure the `kaggle.json` file is in the same directory as your Jupyter Notebook.\n\n3. **Set permissions for the `kaggle.json` file**:\n   ```bash\n   !chmod 600 <ENTER YOUR FILEPATH>/kaggle.json\n   ```\n\n4. **Configure the environment**:\n   - Import the `os` module and set the Kaggle config directory:\n   ```python\n   import os\n   os.environ['KAGGLE_CONFIG_DIR'] = '<STRING OF YOUR FILE PATH>'\n   ```\n\n5. **Download the dataset**:\n   - Use the Kaggle API to download your desired dataset:\n   ```bash\n   !kaggle datasets download -d kapturovalexander/bank-credit-scoring\n   ```\n\n6. **Unzip and access the CSV file**:\n   ```bash\n   !unzip -o bank-credit-scoring.zip\n   ```\n\nFollow these steps to seamlessly integrate Kaggle data retrieval into your Jupyter workflow."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-5/003_faa864b7f6_basic-ubuntu-commands.md", "metadata": {"id": "faa864b7f6", "question": "Basic Ubuntu Commands:", "sort_order": 3}, "content": "- ```bash\n  cd ..\n  ```\n  Go back to the previous directory.\n\n- ```bash\n  ls\n  ```\n  List the contents of the current directory.\n\n- ```bash\n  cd 'path'/\n  ```\n  Navigate to the specified path.\n\n- ```bash\n  pwd\n  ```\n  Print the current working directory.\n\n- ```bash\n  cat 'file name'\n  ```\n  Display the contents of a file."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-5/004_dce5e4842c_installing-and-updating-to-the-python-version-310.md", "metadata": {"id": "dce5e4842c", "question": "Installing and updating to the python version 3.10 and higher", "sort_order": 4}, "content": "To check your current Python version, open your terminal and run:\n\n```bash\npython3 --version\n```\n\nFor Windows:\n\n1. Visit the official Python website to download the desired version: [Python Downloads](https://www.python.org/downloads/).\n2. Run the installer and ensure you check the box that says \"Add Python to PATH\" during installation.\n3. Complete the installation by following the prompts.\n\nFor Python 3:\n\nOpen your command prompt or terminal and run the following command:\n\n```bash\npip install --upgrade python\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-5/005_accd08bead_how-to-install-wsl-on-windows-10-and-11.md", "metadata": {"id": "accd08bead", "question": "How to install WSL on Windows 10 and 11?", "sort_order": 5}, "content": "**Windows 10:**\n\n1. Open PowerShell as Admin.\n2. Run the following command:\n   ```bash\n   wsl --install\n   ```\n3. Restart your computer.\n4. Set up your Linux distribution (e.g., Ubuntu).\n\n**Windows 11:**\n\n1. Open Windows Terminal as Admin.\n2. Run:\n   ```bash\n   wsl --install\n   ```\n3. Restart if prompted.\n4. Set up your Linux distribution.\n\n**Additional Notes:**\n\n- To install a specific distribution, use:\n  ```bash\n  wsl --install -d <DistributionName>\n  ```\n- For updates, run:\n  ```bash\n  wsl --update\n  ```\n\nIt is important to ensure that the \"Virtual Machine Platform\" feature is activated in your Windows \"Features.\" You can check this by searching for \"features\" in the search bar to see if the checkbox is selected. Additionally, ensure that your system (in the BIOS) supports virtualization.\n\nIn the Microsoft Store, search for ‘Ubuntu’ or ‘Debian’ (or any Linux distribution you want) and install it. After downloading, open the app and choose a username and a password. Note that while typing your password, it may not display any characters (this is normal).\n\nOnce inside your Linux system, you can try commands such as `pwd`. To navigate back to your Windows system:\n\n1. Use `cd ../..` twice.\n2. Go to the \"mnt\" directory:\n   ```bash\n   cd mnt\n   ```\n3. List your files to view your disks and move to the desired folder.\n\n   ```\n   mfouesnard@DESKTOP-39IH8UP:/mnt/c/Users/Melanie/ML_Zoomcamp/ML_ZoomCamp$ ls\n   Homework_week2.ipynb  Homework_week3_2023.ipynb  README.md  car_price.csv  data.csv  housing.csv\n   Homework_week3_2022.ipynb  Homework_week4_2023.ipynb  Untitled.ipynb  churn.csv  homework_week1.ipynb\n   ```\n\nPython should be already installed, but you can check with:\n```bash\nsudo apt install python3\n```\n\nTo make your current folder the default when opening Ubuntu terminal, use:\n```bash\necho \"cd ../../mnt/your/folder/path\" >> ~/.bashrc\n```\n\nTo disable bell sounds, edit the inputrc file:\n1. Open the file:\n   ```bash\n   sudo vim /etc/inputrc\n   ```\n2. Uncomment `set bell-style none`:\n   - Press `i` (for insert), navigate to the line, delete `#`, press `Escape`, and then `:wq` to save and quit.\n3. Open a new terminal to check the changes.\n\nTo install pip, run:\n```bash\nsudo apt install python3-pip\n```\n\n#### Possible Error\n\nYou might encounter the following error when installing pipenv:\n```bash\n/sbin/ldconfig.real: Can't link /usr/lib/wsl/lib/libnvoptix_loader.so.1 to libnvoptix.so.1\n/sbin/ldconfig.real: /usr/lib/wsl/lib/libcuda.so.1 is not a symbolic link\n```\nTo resolve, create a symbolic link:\n```bash\nsudo ln -s /usr/lib/wsl/lib/libcuda.so.1 /usr/lib64/libcuda.so\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-5/006_9df18c830e_error-building-docker-images-on-mac-with-m1-silico.md", "metadata": {"id": "9df18c830e", "question": "Error building Docker images on Mac with M1 silicon", "sort_order": 6}, "content": "Do you get errors building the Docker image on the Mac M1 chipset?\n\nThe error received was:\n\n```\nCould not open '/lib64/ld-linux-x86-64.so.2': No such file or directory\n```\n\nTo fix this error:\n\n1. Open the `mlbookcamp-code/course-zoomcamp/01-intro/environment/Dockerfile`.\n\n2. Replace line 1 with:\n   \n   ```dockerfile\n   FROM --platform=linux/amd64 ubuntu:latest\n   ```\n\n3. Now build the image as specified.\n\nNote: Building the image may take over 2 hours, but it should complete successfully."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-5/007_7064ef8f01_method-to-find-the-version-of-any-installed-python.md", "metadata": {"id": "7064ef8f01", "question": "Method to find the version of any installed Python libraries in Jupyter Notebook", "sort_order": 7}, "content": "To find the version of a Python library in a Jupyter Notebook, you can use the following method:\n\n```python\nimport waitress\n\nprint(waitress.__version__)\n```\n\nSimply replace `waitress` with the name of the library you want to check."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-5/008_371f4a6519_docker-cannot-connect-to-the-docker-daemon-is-the.md", "metadata": {"id": "371f4a6519", "question": "Docker: Cannot connect to the docker daemon. Is the Docker daemon running?", "sort_order": 8}, "content": "Ensure Docker Daemon Is Running\n\n**On Windows:**\n\n- Open Docker Desktop (admin rights may be required).\n- Check if it’s running, and restart Docker Desktop if necessary.\n\n**On Linux:**\n\n1. Run the following command to start the Docker daemon:\n   \n   ```bash\n   sudo systemctl start docker\n   ```\n2. Verify it’s running with:\n\n   ```bash\n   sudo systemctl status docker\n   ```\n\nVerify Docker Group Membership (Linux Only)\n\n- Check if your user is in the Docker group:\n\n  ```bash\n  groups $USER\n  ```\n\n- If \"docker\" isn’t listed, add yourself with:\n\n  ```bash\n  sudo usermod -aG docker $USER\n  ```\n\n- Log out and back in to apply changes.\n\nRestart the Docker Service (Linux)\n\n```bash\nsudo systemctl restart docker\n```\n\nCheck Docker Socket Permissions (Linux)\n\n- Run the following command to confirm Docker socket permissions:\n\n  ```bash\n  sudo chmod 666 /var/run/docker.sock\n  ```\n\nTry Running Docker with sudo (Linux)\n\n- Run the following to check if permissions are causing the issue:\n\n  ```bash\n  sudo docker ps\n  ```\n\nTest Docker Setup\n\n- Run a test Docker command to verify connection:\n\n  ```bash\n  docker run hello-world\n  ```\n\nSolution for WSL Error\n\nIf you’re encountering the error on WSL, re-install Docker by removing the Docker installation from WSL and installing Docker Desktop on your host machine (Windows).\n\n**On Linux,** start the docker daemon with either of these commands:\n\n- Start the Docker daemon:\n\n  ```bash\n  sudo dockerd\n  ```\n\n  or\n\n  ```bash\n  sudo service docker start\n  ```"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-5/009_a43ed572fa_docker-the-command-binsh-c-pipenv-install-deploy-s.md", "metadata": {"id": "a43ed572fa", "question": "Docker: The command '/bin/sh -c pipenv install --deploy --system && rm -rf /root/.cache' returned a non-zero code: 1", "sort_order": 9}, "content": "After using the command `docker build -t churn-prediction .` to build the Docker image, this error occurs, and the image is not created.\n\nTo fix this issue, adjust the Python version in your Dockerfile to match the version installed on your system:\n\n1. Determine your Python version by running:\n   \n   ```bash\n   python --version\n   ```\n   \n   Example output:\n   \n   ```bash\n   Python 3.9.7\n   ```\n\n2. Update the first line of your Dockerfile with the correct Python version:\n\n   ```dockerfile\n   FROM python:3.9.7-slim\n   ```\n\nMake sure to replace `3.9.7` with your actual Python version."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-5/010_b2781bc5f8_running-pipenv-install-sklearn102-gives-errors-wha.md", "metadata": {"id": "b2781bc5f8", "question": "Running \"pipenv install sklearn==1.0.2\" gives errors. What should I do?", "sort_order": 10}, "content": "When installing sklearn version 1.0.2, you may encounter errors. This issue is due to the package name. Instead of \"sklearn,\" you should use its full name. Here's how you can resolve this:\n\n1. Use the following command to install the correct version:\n   \n   ```bash\n   pipenv install scikit-learn==1.0.2\n   ```\n\n2. If your homework requires version 1.3.1, use the following command:\n   \n   ```bash\n   pipenv install scikit-learn==1.3.1\n   ```\n\nUsing the correct full package name should resolve the installation issues."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-5/011_af2a35cdf2_error-failed-to-lock-files-with-pipfilelock.md", "metadata": {"id": "af2a35cdf2", "question": "Error: Failed to lock files with Pipfile.lock", "sort_order": 11}, "content": "When adding libraries to the virtual environment in lesson 5.5, the trainer used the command:\n\n```bash\npipenv install numpy scikit-learn==0.24.2 flask\n```\n\nHowever, some people using Python 3.11 or later may encounter an error, failing to lock files correctly with `Pipfile.lock`. You may need to install `scikit-learn==1.4.2` as the error differs from the trainer's example. This should resolve the issue.\n\nIf you are still having problems, try the following steps:\n\n- Delete the `Pipfile.lock` using:\n  ```bash\n  rm Pipfile.lock\n  ```\n- Rebuild the lock with:\n  ```bash\n  pipenv lock\n  ```\n- If it still doesn't work, delete the pipenv environment, `Pipfile`, and `Pipfile.lock`, and create a new one:\n  ```bash\n  pipenv --rm\n  rm Pipfile*\n  ```"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-5/012_ec6919a46b_how-do-i-resolve-the-no-module-named-flask-error.md", "metadata": {"id": "ec6919a46b", "question": "How do I resolve the \"No module named flask\" error?", "sort_order": 12}, "content": "I initially installed Flask with pipenv, but I received a \"No module named 'flask'\" error. I then reinstalled Flask using pip, and after that, I was able to import Flask successfully."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-5/013_2dc4089953_why-do-we-need-the-rm-flag.md", "metadata": {"id": "2dc4089953", "question": "Why do we need the --rm flag?", "sort_order": 13}, "content": "When running Docker containers, using the `--rm` flag ensures that the containers are removed upon exit. This helps in managing disk space by preventing the accumulation of stopped containers, which can consume unnecessary space.\n\nHere are the main points regarding the use of the `--rm` flag:\n\n- **Space Management**: Running containers with the `--rm` flag prevents the accumulation of stopped containers, thus conserving disk space.\n- **Development and Testing**: During these phases, containers often don't need to persist, making the `--rm` flag useful for automatic removal.\n- **Images vs Containers**: It's crucial to differentiate between them. Images are not modified upon execution; containers are the instances created from these images. The `--rm` flag affects containers, not the images themselves.\n- **Rebuilding**: When a file like a Pipfile changes, the image is rebuilt, often under the same or a new tag, and the `--rm` flag helps maintain a clean environment.\n\nUse `docker images` to list images and `docker ps -a` to list all containers, helping you manage your Docker resources efficiently."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-5/014_failed-to-read-dockerfile.md", "metadata": {"id": "9802179265", "question": "Failed to read Dockerfile", "sort_order": 14}, "content": "When you create the Dockerfile, ensure the name is `Dockerfile` without any extensions. A common mistake is naming it with an extension, such as `Dockerfile.dockerfile`, which results in an error during the image build. To avoid this, create the file simply as `Dockerfile`.\n\n```bash\n# Incorrect way:\nDockerfile.dockerfile\n\n# Correct way:\nDockerfile\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-5/015_951b79cf6b_install-docker-on-macos.md", "metadata": {"id": "951b79cf6b", "question": "Install docker on MacOS", "sort_order": 15}, "content": "Refer to the page [https://docs.docker.com/desktop/install/mac-install/](https://docs.docker.com/desktop/install/mac-install/). Remember to check if you have an Apple chip or Intel chip."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-5/016_acdd201a06_dumpingretrieving-only-the-size-of-for-a-specific.md", "metadata": {"id": "acdd201a06", "question": "Dumping/Retrieving only the size of for a specific Docker image", "sort_order": 16}, "content": "To list all information for all local Docker images, you can use the following commands:\n\n```bash\ndocker images\ndocker image ls\n```\n\nTo retrieve information for a specific image, use:\n\n```bash\ndocker image ls <image_name>\n```\n\nOr alternatively:\n\n```bash\ndocker images <image_name>\n```\n\nTo dump only the size of a specified image, use the `--format` option. This will display only the image size:\n\n```bash\ndocker image ls --format \"{{.Size}}\" <image_name>\n```\n\nOr alternatively:\n\n```bash\ndocker images --format \"{{.Size}}\" <image_name>\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-5/017_768fa21d6b_where-does-pipenv-create-environments-and-how-does.md", "metadata": {"id": "768fa21d6b", "question": "Where does pipenv create environments and how does it name them?", "sort_order": 17}, "content": "Pipenv creates environments in different locations depending on the operating system:\n\n- **OSX/Linux:** `~/.local/share/virtualenvs/folder-name_cryptic-hash`\n- **Windows:** `C:\\Users\\<USERNAME>\\.virtualenvs\\folder-name_cryptic-hash`\n\nFor example:\n\n- `C:\\Users\\Ella\\.virtualenvs\\code-qsdUdabf` (for module-05 lesson)\n\nThe environment name is based on the name of the last folder in the directory where the `pipenv install` command was executed. For example, if you run any pipenv command in the directory `~/home/user/Churn-Flask-app`, it will create an environment named `Churn-Flask-app-some_random_characters`, and its path will look like:\n\n- `/home/user/.local/share/virtualenvs/churn-flask-app-i_mzGMjX`\n\nAll libraries for this environment will be installed inside this folder. To activate the environment, navigate back to the project folder and type `pipenv shell`. Essentially, the location of the project folder acts as an identifier for an environment, replacing any specific name."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-5/018_2acc8ec64a_docker-how-do-i-debug-a-docker-container.md", "metadata": {"id": "2acc8ec64a", "question": "Docker: How do I debug a docker container?", "sort_order": 18}, "content": "To debug a Docker container, follow these steps:\n\n1. **Launch the container image in interactive mode** while overriding the entrypoint, so that it starts with a bash command:\n   \n   ```bash\n   docker run -it --entrypoint bash <image>\n   ```\n\n2. If the container is already running, **execute a command in the specific container**:\n\n   - First, find the container ID by listing the running containers:\n     \n     ```bash\n     docker ps\n     ```\n   \n   - Then, execute bash in the container:\n     \n     ```bash\n     docker exec -it <container-id> bash\n     ```"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-5/019_10316840ce_docker-the-input-device-is-not-a-tty-when-running.md", "metadata": {"id": "10316840ce", "question": "Docker: The input device is not a TTY when running docker in interactive mode (Running Docker on Windows in GitBash)", "sort_order": 19}, "content": "```bash\ndocker exec -it 1e5a1b663052 bash\n```\n\nError:\n```\nthe input device is not a TTY. If you are using mintty, try prefixing the command with 'winpty'\n```\n\nFix:\n\n```bash\nwinpty docker exec -it 1e5a1b663052 bash\n```\n\nA TTY is a terminal interface that supports escape sequences, moving the cursor around, etc. Winpty is a Windows software package providing an interface similar to a Unix pty-master for communicating with Windows console programs.\n\nFor more information on terminal, shell, and console applications:\n\n[https://conemu.github.io/en/TerminalVsShell.html](https://conemu.github.io/en/TerminalVsShell.html)"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-5/020_26a0558e81_failed-to-write-the-dependencies-to-pipfile-and-pi.md", "metadata": {"id": "26a0558e81", "question": "Failed to write the dependencies to pipfile and piplock file", "sort_order": 20}, "content": "Create a virtual environment using the command line and use the `pip freeze` command to write the requirements to a text file."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-5/021_3e4e288e93_pipenv-is-not-recognized-as-an-internal-or-externa.md", "metadata": {"id": "3e4e288e93", "images": [{"description": "image #1", "id": "image_1", "path": "images/machine-learning-zoomcamp/image_6db28918.png"}, {"description": "image #2", "id": "image_2", "path": "images/machine-learning-zoomcamp/image_3b4fce52.png"}, {"description": "image #3", "id": "image_3", "path": "images/machine-learning-zoomcamp/image_b2712e9d.png"}, {"description": "image #4", "id": "image_4", "path": "images/machine-learning-zoomcamp/image_a4cd7017.png"}], "question": "'pipenv' is not recognized as an internal or external command, operable program or batch file.", "sort_order": 21}, "content": "This error occurs because `pipenv` is installed but not accessible from the PATH.\n\nYou might encounter this error when running:\n\n```bash\npipenv --version\n```\n\nor\n\n```bash\npipenv shell\n```\n\n### Solution for Windows:\n\n1. Open this option:\n   \n   <{IMAGE:image_1}>\n\n2. Click here:\n   \n   <{IMAGE:image_2}>\n\n3. Click the Edit button:\n   \n   <{IMAGE:image_3}>\n\n4. Ensure the following locations are included in the PATH. If not, add them:\n   \n   - `C:\\Users\\AppData\\...\\Python\\PythonXX\\`\n   - `C:\\Users\\AppData\\...\\Python\\PythonXX\\Scripts\\`\n   \n   <{IMAGE:image_4}>\n\n**Note:** This solution is for setups without Anaconda. If you use Windows, Anaconda might be a better and less error-prone choice."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-5/022_4b501f0a25_attributeerror-module-collections-has-no-attribute.md", "metadata": {"id": "4b501f0a25", "question": "AttributeError: module ‘collections’ has no attribute ‘MutableMapping’", "sort_order": 22}, "content": "Following the instruction from video week-5.6, using pipenv to install Python libraries throws the error shown below:\n\n```\nnaneen@xps:ml_zoomcamp_ht$ pipenv install numpy\nTraceback (most recent call last):\n  File \"/usr/bin/pipenv\", line 33, in <module>\n    sys.exit(load_entry_point('pipenv==11.9.0', 'console_scripts', 'pipenv')())\n  File \"/usr/lib/python3/dist-packages/pipenv/vendor/click/core.py\", line 722, in __call__\n    return self.main(*args, **kwargs)\n  File \"/usr/lib/python3/dist-packages/pipenv/vendor/click/core.py\", line 697, in main\n    rv = self.invoke(ctx)\n  File \"/usr/lib/python3/dist-packages/pipenv/vendor/click/core.py\", line 1066, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File \"/usr/lib/python3/dist-packages/pipenv/vendor/click/core.py\", line 895, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"/usr/lib/python3/dist-packages/pipenv/vendor/click/core.py\", line 535, in invoke\n    return callback(*args, **kwargs)\n  File \"/usr/lib/python3/dist-packages/pipenv/cli/py.py\", line 347, in install\n    from . import core\n  File \"/usr/lib/python3/dist-packages/pipenv/core.py\", line 21, in <module>\n    import requests\n  File \"/usr/lib/python3/dist-packages/pipenv/vendor/requests/__init__.py\", line 65, in <module>\n    from . import utils\n  File \"/usr/lib/python3/dist-packages/pipenv/vendor/requests/utils.py\", line 27, in <module>\n    from .cookies import RequestsCookieJar, cookiejar_from_dict\n  File \"/usr/lib/python3/dist-packages/pipenv/vendor/requests/cookies.py\", line 172, in <module>\n    class RequestsCookieJar(cookielib.CookieJar, collections.MutableMapping):\nAttributeError: module 'collections' has no attribute 'MutableMapping'\nnaneen@xps:ml_zoomcamp_ht$\n```\n\nSolution:\n\n- Ensure you are working with Python version 3.10+"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-5/023_f6d64d22b9_valueerror-path-not-found-or-generated-windowspath.md", "metadata": {"id": "f6d64d22b9", "question": "ValueError: Path not found or generated: WindowsPath('C:/Users/username/.virtualenvs/envname/Scripts')", "sort_order": 23}, "content": "After entering `pipenv shell`, ensure you use `exit` before `pipenv --rm`. Failing to do so may cause installation errors, making it unclear whether you are \"in the shell\" on Windows, as there are no clear markers for it.\n\nIf this messes up your PATH, use these terminal commands to fix it:\n\n- **For Windows**:\n  \n  ```bash\n  set VIRTUAL_ENV \"\"\n  ```\n\n- **For Unix**:\n  \n  ```bash\n  export VIRTUAL_ENV=\"\"\n  ```\n\nAdditionally, manually re-creating the removed folder at `C:\\Users\\username\\.virtualenvs\\removed-envname` can help. The `removed-envname` can be identified in the error message."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-5/024_f2f7180201_connectionerror-connection-aborted-remotedisconnec.md", "metadata": {"id": "f2f7180201", "question": "ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))", "sort_order": 24}, "content": "Set the host to '0.0.0.0' on the Flask app and Dockerfile, then run the URL using localhost."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-5/025_ced071dd97_docker-build-error-copy-pipfile-pipfilelock.md", "metadata": {"id": "ced071dd97", "images": null, "question": "docker: build ERROR COPY [Pipfile, Pipfile.lock]", "sort_order": 25}, "content": "```\n% docker build -t zoomcamp_test .\n\n[+] Building 0.1s (10/10) FINISHED\n => [internal] load build definition from Dockerfile\n => => transferring dockerfile: 332B\n => [internal] load .dockerignore\n => => transferring context: 2B\n => [internal] load build context\n => => transferring context: 2B\n => [internal] load metadata for docker.io/svizor/zoomcamp-model:3.9.12-slim\n => [1/6] FROM docker.io/svizor/zoomcamp-model:3.9.12-slim\n => [internal] load build context\n => => transferring context: 2B\n => CACHED [2/6] RUN pip install pipenv\n => CACHED [3/6] WORKDIR /app\n => ERROR [4/6] COPY [Pipfile, Pipfile.lock, ./]\n => CACHED [5/6] RUN pipenv install --system --deploy\n => ERROR [6/6] COPY [q5_predict.py, model1.bin, dv.bin, ./]\n```\n\n\nThis error occurred because I used single quotes around the filenames. Stick to double quotes."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-5/026_efae76d03c_docker-fix-error-during-installation-of-pipfile-in.md", "metadata": {"id": "efae76d03c", "question": "Docker: Fix error during installation of Pipfile inside Docker container", "sort_order": 26}, "content": "```\n(hw5) (base) home@ls-MacBook-Pro hw5 % docker build -t zoomcamp_test .\n[+] Building 19.7s (9/10)\n => [internal] load build definition from Dockerfile\n => => transferring dockerfile: 332B\n => [internal] load .dockerignore\n => => transferring context: 2B\n => [internal] load metadata for docker.io/svizor/zoomcamp-model:3.9.12-slim\n => CACHED [1/6] FROM docker.io/svizor/zoomcamp-model:3.9.12-slim\n => [internal] load build context\n => => transferring context: 19.77kB\n => [2/6] RUN pip install pipenv\n => [3/6] WORKDIR /app\n => [4/6] COPY [Pipfile, Pipfile.lock, ./]\n => ERROR [5/6] RUN pipenv install --system --deploy\n------\n > [5/6] RUN pipenv install --system --deploy:\n#8 0.659 Your Pipfile.lock (65dad0) is out of date. Expected: (f3760a).\n#8 0.660 Usage: pipenv install [OPTIONS] [PACKAGES]...\n#8 0.660 ERROR:: Aborting deploy\n```\n\nI tried the first solution on Stackoverflow which recommended running `pipenv lock` to update the Pipfile.lock.\n\nHowever, this didn’t resolve it. The following switch to the pipenv installation worked:\n\n```bash\nRUN pipenv install --system --deploy --ignore-pipfile\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-5/027_ed90e0f589_bind-for-00009696-failed-port-is-already-allocated.md", "metadata": {"id": "ed90e0f589", "question": "Bind for 0.0.0.0:9696 failed: port is already allocated", "sort_order": 27}, "content": "I was getting the following error when I rebuilt the Docker image, although the port was not allocated, and it was working fine.\n\nError message:\n\n```\nError response from daemon: driver failed programming external connectivity on endpoint beautiful_tharp (875be95c7027cebb853a62fc4463d46e23df99e0175be73641269c3d180f7796): Bind for 0.0.0.0:9696 failed: port is already allocated.\n```\n\n\n\nThe issue can be resolved by running the following command:\n\n```bash\ndocker kill $(docker ps -q)\n```\n\nFor more information, refer to the [GitHub issue on Docker for Windows](https://github.com/docker/for-win/issues/2722)."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-5/028_84301e35dd_connectionerror-connection-aborted-for-bind-127001.md", "metadata": {"id": "84301e35dd", "question": "ConnectionError 'Connection aborted.' for --bind 127.0.0.1:5000", "sort_order": 28}, "content": "I was getting an error on the client side with this:\n\n**Client Side Error:**\n\n```plaintext\nFile \"C:\\python\\lib\\site-packages\\urllib3\\connectionpool.py\", line 703, in urlopen ...\n\nraise ConnectionError(err, request=request)\n\nrequests.exceptions.ConnectionError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n```\n\n**Server Side:**\n\nAn error was shown for Gunicorn, although the `waitress` command was running smoothly from the server side.\n\n**Solution:**\n\n- Use the IP address `0.0.0.0:8000` or `0.0.0.0:9696`. They are the ones which work most of the time."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-5/029_f884543e2c_installing-md5sum-on-macos.md", "metadata": {"id": "f884543e2c", "question": "Installing md5sum on Macos", "sort_order": 29}, "content": "To install `md5sum` on macOS, use the following command:\n\n```bash\nbrew install md5sha1sum\n```\n\nThen run the command to check the hash for a file to see if it matches the provided hash:\n\n```bash\nmd5sum model1.bin dv.bin\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-5/030_de0726d5ea_how-to-run-a-script-while-a-web-server-is-working.md", "metadata": {"id": "de0726d5ea", "question": "How to run a script while a web-server is working?", "sort_order": 30}, "content": "I started a web-server in a terminal (command window, PowerShell, etc.). How can I run another Python script that makes a request to this server?\n\n\n- Open another terminal (command window, PowerShell, etc.).\n- Run your Python script from this new terminal."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-5/031_e8e58afd7b_trying-to-unpickle-estimator-from-version-111-when.md", "metadata": {"id": "e8e58afd7b", "question": "Trying to unpickle estimator from version 1.1.1 when using version 0.24.2", "sort_order": 31}, "content": "When executing the commands:\n\n```bash\npipenv shell\npipenv run gunicorn --bind 0.0.0.0:9696 predict:app\n```\n\nthe following warning may occur:\n\n```python\nUserWarning: Trying to unpickle estimator DictVectorizer from version 1.1.1 when using version 0.24.2. This might lead to breaking code or invalid results. Use at your own risk.\n```\n\n- Ensure you create the virtual environment with the same version of Scikit-Learn that was used to train the model, in this case, version 1.1.1.\n- Resolve version conflicts by verifying that the model and `DictVectorizer` files are compatible with the Scikit-Learn version used for the project."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-5/032_8a8f00802a_python_version-and-python_full_version-error-after.md", "metadata": {"id": "8a8f00802a", "question": "Python_version and Python_full_version error after running pipenv install:", "sort_order": 32}, "content": "If you install packages via `pipenv install`, and encounter an error like this:\n\n```python\npipenv.vendor.plette.models.base.ValidationError: {'python_version': '3.9', 'python_full_version': '3.9.13'}\n\npython_full_version: 'python_version' must not be present with 'python_full_version'\n\npython_version: 'python_full_version' must not be present with 'python_version'\n```\n\nFollow these steps to resolve the issue:\n\n1. Open the `Pipfile` in a text editor, such as `nano`:\n   \n   ```bash\n   nano Pipfile\n   ```\n\n2. Remove either the `python_version` or `python_full_version` line.\n\n3. Save the changes by pressing `CTRL+X`, then type `Y` and press `Enter`.\n\n4. Run the following command to create the `Pipfile.lock`:\n\n   ```bash\n   pipenv lock\n   ```\n\nYou can now continue with your work."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-5/033_554fac782d_your-pipfilelock-221d14-is-out-of-date-during-dock.md", "metadata": {"id": "554fac782d", "question": "Your Pipfile.lock (221d14) is out of date (during Docker build)", "sort_order": 33}, "content": "If during running the docker build command, you get an error like this:\n\n```\nYour Pipfile.lock (221d14) is out of date. Expected: (939fe0).\n\nUsage: pipenv install [OPTIONS] [PACKAGES]...\n\nERROR:: Aborting deploy\n```\n\nYou can try the following solutions:\n\n1. **Delete and Rebuild Pipfile.lock:**\n   - Delete the `Pipfile.lock` using the command:\n   \n     ```bash\n     rm Pipfile.lock\n     ```\n   \n   - Rebuild the lock file:\n   \n     ```bash\n     pipenv lock\n     ```\n   \n   - Retry the `docker build` command.\n\n2. **Remove and Recreate Pipenv Environment:**\n   - Remove the pipenv environment:\n   \n     ```bash\n     pipenv --rm\n     ```\n   \n   - Remove the Pipfile and Pipfile.lock:\n   \n     ```bash\n     rm Pipfile*\n     ```\n   \n   - Create a new environment before retrying the Docker build."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-5/034_927d11d38f_conda-environment-you-are-using-windows-you-then-u.md", "metadata": {"id": "927d11d38f", "question": "Conda environment: You are using Windows. You then use Waitress instead of Gunicorn. After a few runs, suddenly the MLflow server fails to run.", "sort_order": 34}, "content": "1. Uninstall Waitress and MLflow:\n\n   ```bash\n   pip uninstall waitress mlflow\n   ```\n\n2. Reinstall MLflow:\n\n   ```bash\n   pip install mlflow\n   ```\n\nBy this time, you should have successfully built your Docker image, so you don't need to reinstall Waitress.\n\nAll good. Happy learning."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-5/035_03331f413e_completed-creating-the-environment-locally-but-cou.md", "metadata": {"id": "03331f413e", "question": "Completed creating the environment locally but could not find the environment on AWS.", "sort_order": 35}, "content": "Ensure that you are in the correct AWS region. Check if you are in `eu-west-1` (Ireland) when reviewing your Elastic Beanstalk environments. It's possible you might be in a different region in your AWS console."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-5/036_f580e98fdc_installing-waitress-on-windows-via-gitbash-waitres.md", "metadata": {"id": "f580e98fdc", "question": "Installing waitress on Windows via GitBash: “waitress-serve” command not found", "sort_order": 36}, "content": "Running `pip install waitress` as a command on GitBash may not download the executable file `waitress-serve.exe`. You need this file to use the commands with waitress in Git Bash. To resolve this issue:\n\n1. Open a Jupyter notebook and run the command `pip install waitress`. This will download the executable file. You may see the following warning:\n   \n   ```bash\n   WARNING: The script waitress-serve.exe is installed in 'c:\\Users\\....\\anaconda3\\Scripts' which is not on PATH. Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n   ```\n\n2. Add the path where `waitress-serve.exe` is installed to GitBash's PATH:\n\n   - Enter the following command in GitBash: `nano ~/.bashrc`\n   \n   - Add the path to `waitress-serve.exe` to PATH using the command:\n     \n     ```bash\n     export PATH=\"/path/to/waitress:$PATH\"\n     ```\n\n3. Close GitBash and open it again. You should now be able to run `waitress-serve` successfully."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-5/037_8b15cafd96_warning-the-environment-variable-lang-is-not-set.md", "metadata": {"id": "8b15cafd96", "question": "Warning: the environment variable LANG is not set!", "sort_order": 37}, "content": "This is an error encountered while using Pipenv to install Scikit-Learn version 1.3.1 in the [ml-zoomcamp conda environment](https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/01-intro/06-environment.md). The error indicates that explicit language specifications are not set in the bash profile.\n\nThe error is not fatal and can usually be ignored. However, if you'd like to address it, consider the following quick-fix:\n\nVisit this discussion for more details: [StackOverflow - Getting error while trying to run this command 'pipenv install requests' in ma](https://stackoverflow.com/questions/49436922/getting-error-while-trying-to-run-this-command-pipenv-install-requests-in-ma).\n\nIn most cases, you can proceed without fixing this warning."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-5/038_0e2b70a390_terminal-used-in-week-5-videos.md", "metadata": {"id": "0e2b70a390", "question": "Terminal Used in Week 5 videos:", "sort_order": 38}, "content": "[https://apps.microsoft.com/detail/windows-terminal/9N0DX20HK701?hl=es-419&gl=CO](https://apps.microsoft.com/detail/windows-terminal/9N0DX20HK701?hl=es-419&gl=CO)"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-5/039_748ffe7992_waitress-waitress-serve-shows-malformed-applicatio.md", "metadata": {"id": "748ffe7992", "question": "Waitress: waitress-serve shows Malformed application", "sort_order": 39}, "content": "When running the command:\n\n```bash\npipenv run waitress-serve --listen=localhost:9696 q4-predict:app\n```\n\nYou may encounter the following error message:\n\n```\nThere was an exception (ValueError) importing your module.\n\nIt had these arguments:\n\n1. Malformed application 'q4-predict:app'\n```\n\n\nWaitress doesn’t accept a dash in the Python file name.\n\nTo resolve this, rename the file by replacing the dash with an underscore, for example, use `q4_predict.py`."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-5/040_cde1c48afc_testing-http-post-requests-from-command-line-using.md", "metadata": {"id": "cde1c48afc", "question": "Testing HTTP POST requests from command line using curl", "sort_order": 40}, "content": "I wanted to have a fast and simple way to check if the HTTP POST requests are working just by running a request from the command line. This can be done using `curl`. (Used with WSL2 on Windows; should also work on Linux and MacOS)\n\n```bash\ncurl --json '<json data>' <url>\n```\n\nPiping the structure to the command:\n\n```bash\ncat <json file path> | curl --json @- <url>\necho '<json data>' | curl --json @- <url>\n```\n\nExample using piping:\n\n```bash\necho '{\"job\": \"retired\", \"duration\": 445, \"poutcome\": \"success\"}' \\\n| curl --json @- http://localhost:9696/predict\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-5/041_5ba1e7ba64_error-notsupportederror-you-can-use-eb-local-only.md", "metadata": {"id": "5ba1e7ba64", "question": "Error: NotSupportedError - You can use \"eb local\" only with preconfigured, generic, and multicontainer Docker platforms.", "sort_order": 41}, "content": "When executing:\n\n```bash\neb local run  --port 9696\n```\n\nYou may encounter the following error:\n\n```\nERROR: NotSupportedError - You can use \"eb local\" only with preconfigured, generic and multicontainer Docker platforms.\n```\n\n\nThere are two options to fix this issue:\n\n1. **Re-initialize the Environment:**\n   - Run the initialization command:\n     ```bash\n     eb init -i\n     ```\n   - Choose the appropriate options from the list provided (the first default option for the Docker platform should suffice).\n\n2. **Manually Edit Configuration:**\n   - Open and edit the ‘.elasticbeanstalk/config.yml’ file.\n   - Change `default_platform` from `Docker` to:\n     ```yaml\n     default_platform: Docker running on 64bit Amazon Linux 2023\n     ```\n   - Note that this option might not be available in the future.\n\nAlternative Solution:\n\n- Re-run the init command and change the `-p` flag value:\n  ```bash\n  eb init -p \"Docker running on 64bit Amazon Linux\" <appname>\n  ```\n- Then re-run:\n  ```bash\n  eb local run --port 9696\n  ```\n\nOriginal solution from [Stack Overflow](https://stackoverflow.com/a/75804355/24066976)"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-5/042_4d94846cbe_requests-error-no-connection-adapters-were-found-f.md", "metadata": {"id": "4d94846cbe", "question": "Requests Error: No connection adapters were found for 'localhost:9696/predict'.", "sort_order": 42}, "content": "You need to include the protocol scheme: `[http://localhost:9696/predict](http://localhost:9696/predict)`.\n\nWithout the `http://` part, requests has no idea how to connect to the remote server.\n\nNote that the protocol scheme must be all lowercase; if your URL starts with `HTTP://` for example, it won’t find the `http://` connection adapter either."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-5/043_cee7450404_getting-the-same-result.md", "metadata": {"id": "cee7450404", "question": "Getting the same result", "sort_order": 43}, "content": "While running the Docker image, if you get the same result, check which model you are using.\n\nRemember, you are using a model by downloading the model and Python version. Ensure to change the model in your file when running your prediction test."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-5/044_6d91bd8f56_docker-trying-to-run-a-docker-image-i-built-but-it.md", "metadata": {"id": "6d91bd8f56", "question": "Docker: Trying to run a docker image I built but it says it’s unable to start the container process", "sort_order": 44}, "content": "Ensure that you used `pipenv` to install the necessary modules including `gunicorn`. Follow these steps:\n\n1. Use `pipenv shell` to enter the virtual environment.\n2. Build and run your Docker image.\n\nMake sure all dependencies are correctly specified in your Pipfile."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-5/045_f5b6e3f04b_how-do-i-copy-files-from-my-local-machine-to-a-doc.md", "metadata": {"id": "f5b6e3f04b", "question": "How do I copy files from my local machine to a docker container?", "sort_order": 45}, "content": "You can copy files from your local machine into a Docker container using the `docker cp` command. Here's how to do it:\n\nTo copy a file or directory from your local machine into a running Docker container, use the following syntax:\n\n```bash\ndocker cp /path/to/local/file_or_directory container_id:/path/in/container\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-5/046_993a359b90_how-do-i-copy-files-from-a-different-folder-into-a.md", "metadata": {"id": "993a359b90", "question": "How do I copy files from a different folder into a Docker container’s working directory?", "sort_order": 46}, "content": "You can copy files from your local machine into a Docker container using the `docker cp` command.\n\nIn the Dockerfile, you can specify the folder containing the files you want to copy. The basic syntax is as follows:\n\n```dockerfile\nCOPY [\"src/predict.py\", \"models/xgb_model.bin\", \"./\"]\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-5/047_9e5f381432_aws-elastic-beanstalk-i-cant-create-the-environmen.md", "metadata": {"id": "9e5f381432", "question": "AWS Elastic Beanstalk: I can’t create the environment with the command proposed during the video", "sort_order": 47}, "content": "I struggled with the command:\n\n```bash\n eb init -p docker tumor-diagnosis-serving -r eu-west-1\n```\n\nWhich resulted in an error when running:\n\n```bash\neb local run --port 9696\n```\n\n```\nERROR: NotSupportedError - You can use \"eb local\" only with preconfigured, generic and multicontainer Docker platforms.\n```\n\nI replaced it with:\n\n```bash\neb init -p \"Docker running on 64bit Amazon Linux 2\" tumor-diagnosis-serving -r eu-west-1\n```\n\nThis allowed the recognition of the Dockerfile and the build/run of the docker container."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-5/048_01514d66d5_docker-dockerfile-missing-when-creating-the-aws-el.md", "metadata": {"id": "01514d66d5", "question": "Docker: Dockerfile missing when creating the AWS ElasticBean environment", "sort_order": 48}, "content": "I encountered this error when creating an AWS ElasticBean environment using the command:\n\n```bash\neb create tumor-diagnosis-env\n```\n\n**Error Message:**\n\n```bash\nERROR   Instance deployment: Both 'Dockerfile' and 'Dockerrun.aws.json' are missing in your source bundle. Include at least one of them. The deployment failed.\n```\n\nThe error occurred because I had not committed the files used to build the container, particularly the `Dockerfile`. After performing the following Git operations, the command worked properly:\n\n1. Add the modified files to staging:\n   \n   ```bash\n   git add .\n   ```\n\n2. Commit the changes:\n   \n   ```bash\n   git commit -m \"Add Dockerfile and necessary files\"\n   ```"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-5/049_d66d501514_eb-create-error-commanderror-git-could-not-find-th.md", "metadata": {"id": "d66d501514", "question": "eb create: ERROR: CommandError - git could not find the HEAD", "sort_order": 49}, "content": "When creating and launching an AWS Elastic Beanstalk environment with `eb create`, you might encounter the following error:\n\n\n```bash\nERROR: CommandError - git could not find the HEAD; most likely because there are no commits present\n```\n\n**Explanation and Steps to Resolve:**\n\nThis error indicates that your project directory has not been initialized as a Git repository or is in a \"detached HEAD\" state. Elastic Beanstalk's CLI relies on Git for managing application versions. Here's how to resolve it:\n\n1. **Check Git Initialization:**\n   - Run:\n     \n     ```bash\n     git status\n     ```\n\n   - If Git is not initialized, you will see an error or a message indicating no repository exists.\n\n2. **Initialize Git:**\n   \n   ```bash\n   git init\n   ```\n\n3. **Create an Initial Commit (if none exists):**\n   \n   ```bash\n   git add .\n   git commit -m \"Initial commit\"\n   ```\n\n4. **Manage \"Detached HEAD\" State:**\n   - Create a new branch (if needed):\n     \n     ```bash\n     git checkout -b main\n     ```\n\n   - Or switch to an existing branch:\n     \n     ```bash\n     git checkout main\n     ```\n\n5. **Reinitialize Elastic Beanstalk (if necessary):**\n   \n   ```bash\n   eb init\n   ```\n\n6. **Retry Deployment:**\n   \n   ```bash\n   eb create <env_name> --enable-spot\n   ```"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-5/050_57ec9fbb5d_why-doesnt-the-eb-create-command-use-the-latest-ve.md", "metadata": {"id": "57ec9fbb5d", "question": "Why doesn’t the eb create command use the latest version of my Dockerfile?", "sort_order": 50}, "content": "When you make local changes to the Dockerfile or any other files and do not commit these changes, AWS Elastic Beanstalk (EB) won’t deploy them. By default, the EB CLI deploys the latest commit in the current branch. \n\nIf you want to deploy to your environment without committing, you can use the `–-stage` option to deploy changes that have been added to the staging area.\n\nIf the Docker image creation fails during the `eb create` process, you can still create the image and deploy it by running `eb deploy`.\n\n**To deploy changes without committing:**\n\n1. Add new and changed files to the staging area:\n   \n   ```bash\n   ~/eb$ git add .\n   ```\n\n2. Deploy the staged changes with `eb deploy`:\n   \n   ```bash\n   ~/eb$ eb deploy --staged\n   ```"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-5/051_6ae0381ba8_elastic-beanstalk-eb-create-error-creating-auto-sc.md", "metadata": {"id": "6ae0381ba8", "question": "Elastic Beanstalk ‘eb create’: ERROR   Creating Auto Scaling launch configuration failed Reason: Resource handler returned message: \"The Launch Configuration creation operation is not available in your account. Use launch templates to create configuration templates for your Auto Scaling groups.\"", "sort_order": 51}, "content": "To resolve this issue, you can create your environment using the `--enable-spot` flag, which automatically uses Launch Templates.\n\nExample:\n\n```bash\neb create med-app-env --enable-spot\n```\n\nAnother option is to run `eb create` and follow the wizard options:\n\n1. Enter Environment Name:\n   - Default: `churn-serving-dev`\n   - Example: `churn-serving-dev`\n\n2. Enter DNS CNAME prefix:\n   - Default: `churn-serving-dev`\n   - Example: `churn-serving-dev`\n\n3. Select a load balancer type:\n   \n   ```\n   1) classic\n   2) application\n   3) network\n   (default is 2): 1\n   ```\n\n4. Would you like to enable Spot Fleet requests for this environment? \n   - Prompt: `(y/N):`\n   - Example: `y`\n\n5. Enter a list of one or more valid EC2 instance types separated by commas (at least two instance types are recommended).\n   - Defaults provided on Enter: Press `Enter`"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-5/052_a6755bdf4d_aws-discontinues-support-for-launch-configurations.md", "metadata": {"id": "a6755bdf4d", "question": "AWS: Discontinues Support for Launch Configurations", "sort_order": 52}, "content": "Starting on October 1, 2024, the Amazon EC2 Auto Scaling service will no longer support the creation of launch configurations for new accounts. This change is due to launch configurations being phased out and replaced by launch templates by the Amazon EC2 Auto Scaling service.\n\nFor more details refer to: [AWS Documentation](https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/environments-cfg-autoscaling-launch-templates.html)\n\nThis replacement of launch configurations by launch templates is what caused the error described previously (“...use launch templates to create configuration templates for your Auto Scaling groups)."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-5/053_cc8c2d4b32_default-vpc-error-when-deploying-to-aws-elastic-be.md", "metadata": {"id": "cc8c2d4b32", "images": [{"description": "image #1", "id": "image_1", "path": "images/machine-learning-zoomcamp/image_a34a34fc.png"}], "question": "Default VPC Error when deploying to AWS Elastic Beanstalk:", "sort_order": 53}, "content": "When encountering a VPC configuration error during the deployment to AWS Elastic Beanstalk, follow these steps:\n\n1. Execute the command:\n   ```bash\n   eb create churn-prediction-env\n   ```\n   \n2. If the environment creation initially appears successful but later shows an error related to VPC configuration, it likely means there is no default VPC for the selected region.\n\n3. Go to the AWS Console and select your region from the top bar (e.g., `us-east-2`).\n\n4. Search for \"VPC\" and from the left menu, navigate to \"Your VPCs\".\n\n5. If no VPCs are present, the option to create a default VPC will be available. Click on it.\n\n6. Once the default VPC is created, rerun the command.\n\n<{IMAGE:image_1}>"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-5/054_11f27e4673_whats-the-advantage-of-using-gunicorn-with-flask-i.md", "metadata": {"id": "11f27e4673", "question": "What's the advantage of using Gunicorn with Flask in Docker?", "sort_order": 54}, "content": "Gunicorn is a Python WSGI HTTP server that is more suitable for production than the default Flask development server:\n\n- **Performance**: Better at handling multiple simultaneous requests.\n- **Stability**: More robust and can manage worker processes.\n\n**Usage**:\n\nModify the CMD in your Dockerfile:\n\n```dockerfile\nCMD [\"gunicorn\", \"--bind\", \"0.0.0.0:9696\", \"app:app\"]\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-5/055_c8d2668615_fix-warning-python-312-was-not-found-on-your-syste.md", "metadata": {"id": "c8d2668615", "question": "Fix warning: Python 3.12 was not found on your system… Neither 'pyenv' nor 'asdf' could be found to install Python.", "sort_order": 55}, "content": "This warning occurs because the Pipfile is expecting Python 3.12, but the local container is likely running an older version, such as Python 3.8.12-slim, as shown in the video [5.6 - Environment Management : Docker](https://www.youtube.com/watch?v=wAtyYZ6zvAs&list=PL3MmuxUbc_hIhxl5Ji8t4O6lPAOpHaCLR&index=57).\n\nTo resolve this issue, update the `Dockerfile` to use the appropriate version:\n\n```dockerfile\nFROM python:3.12.7-slim\n```\n\nEnsure that both Python versions (the local version shown in the Pipfile and the container version) match to guarantee compatibility."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-5/056_a173b94288_how-to-use-a-specific-python-version-eg-311-from-c.md", "metadata": {"id": "a173b94288", "question": "How to use a specific python version (e.g. 3.11) from conda with pipenv?", "sort_order": 56}, "content": "First, avoid being in a virtual environment when using pipenv. You can point pipenv directly to the Python 3.11 interpreter from your Conda installation:\n\n1. **Activate the Conda environment:**  \n   ```bash\n   conda activate env_name\n   ```\n\n2. **Get the Python path:**  \n   ```bash\n   which python\n   ```\n\n3. **Deactivate the Conda environment:**  \n   ```bash\n   conda deactivate\n   ```\n\n4. **Use pipenv with the Python path found in step 2:**  \n   ```bash\n   pipenv --python /path/to/python\n   ```"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-5/057_a75f85c9da_pipenv-is-taking-forever-to-lock-file-i-have-delet.md", "metadata": {"id": "a75f85c9da", "question": "Pipenv is taking forever to lock file. I have deleted the lockfile, and restarted my pc. Please, what is a possible solution?", "sort_order": 57}, "content": "You could try running your homework on GitHub Codespaces instead of your local computer. In my experience, the compute resources on GitHub Codespaces are quite sufficient for Homework 5. No issues at all in terms of speed."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-5/058_9db5014252_how-to-savedownload-jupyter-notebook-to-python-scr.md", "metadata": {"id": "9db5014252", "question": "How to save/download jupyter notebook to python script", "sort_order": 58}, "content": "You can convert a Jupyter notebook to a Python script using the following methods:\n\n- **Using the terminal**\n  \n  Run the command below in the terminal:\n\n  ```bash\n  jupyter nbconvert --to python notebook.ipynb\n  ```\n  \n  This converts the notebook into a Python script with the same name but with a `.py` extension.\n  \n- **Using the Jupyter Notebook interface**\n  \n  1. Navigate to `File` in the menu.\n  2. Select `Save and Export Notebook As`.\n  3. Choose `Executable Scripts`.\n\n  This will download the file to your downloads folder."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-5/059_e842ae080e_does-it-matter-if-we-let-the-python-file-create-th.md", "metadata": {"id": "e842ae080e", "question": "Does it matter if we let the Python file create the server or if we run gunicorn directly?", "sort_order": 59}, "content": "They both do the same; it's just less typing from the script."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-5/060_1b08427cd4_no-module-named-ping.md", "metadata": {"id": "1b08427cd4", "question": "No module named ‘ping’?", "sort_order": 60}, "content": "When you encounter the error stating that there is no module named 'ping', it means the 'ping' module is not found in your Python environment.\n\nTo fix this, you can import the 'ping' function directly from the specific file where it is defined using:\n\n```python\nfrom [file name] import ping\n```\n\nReplace `[file name]` with the actual file name where the 'ping' function is located."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-5/061_80a2e18431_docker-error-response-from-daemon-failed-to-create.md", "metadata": {"id": "80a2e18431", "question": "docker: Error response from daemon: failed to create task for container: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: exec: \"gunicorn\": executable file not found in $PATH: unknown.", "sort_order": 61}, "content": "This error indicates that the executable `gunicorn` is not found in the container's `$PATH`. To resolve this, you need to add `gunicorn` and `flask` to your `Pipfile`.\n\nUpdate your `Pipfile` as follows:\n\n```plaintext\n[[source]]\nurl = \"https://pypi.org/simple\"\nverify_ssl = true\nname = \"pypi\"\n\n[packages]\nscikit-learn = \"==1.5.2\"\ngunicorn = \"*\"\nflask = \"*\"\n\n[dev-packages]\n\n[requires]\npython_version = \"3.11\"\n```\n\nAfter making these changes, follow these steps:\n\n1. Run `pipenv lock` to update the `Pipfile.lock`.\n2. Build the Docker image with:\n   ```bash\n   docker build -t [name] .\n   ```\n3. Run the Docker container with:\n   ```bash\n   docker run [name]\n   ```"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-5/062_fca4671a69_pretrained-models-deployment.md", "metadata": {"id": "fca4671a69", "question": "Can I use pre-trained models and focus on deployment?", "sort_order": 62}, "content": "Yes, you can use pre-trained models and focus on deployment, but you must include some training element (such as fine-tuning, retraining, or a comparable approach). Without a training element, you’ll lose points in training-related criteria, though you can still pass overall if other parts are strong."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-6-homework/001_66c3aaa71f_homework-q3-what-does-it-mean-that-rmse-stops-impr.md", "metadata": {"id": "66c3aaa71f", "question": "Homework Q3: What does it mean that RMSE stops improving?", "sort_order": 1}, "content": "Question 3 of homework 6 if I see that RMSE goes up at a certain\nnumber of `n_estimators` but then goes back down lower than it was before, should\nthe answer be the number of `n_estimators` after which RMSE initially went up, or\nthe number after which it was its overall lowest value?\n\nWhen RMSE stops improving, it means when it stops decreasing or remains nearly the same."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-6/001_091cf6f73e_xgboost-typeerror-expecting-a-sequence-of-strings.md", "metadata": {"id": "091cf6f73e", "question": "XGBoost: TypeError: Expecting a sequence of strings for feature names, got: <class 'numpy.ndarray'>", "sort_order": 1}, "content": "This error occurs because recent versions of xgb.DMatrix expect the `feature_names` parameter to be a list of strings rather than a NumPy array. Older tutorial videos might use `feature_names=dv.get_feature_names_out()` directly, which now results in this error.\n\nConvert `dv.get_feature_names_out()` to a list using `.tolist()`. Here's an updated example:\n\n```python\n# Convert feature names to a list\nfeature_names = dv.get_feature_names_out().tolist()\n\n# Create DMatrix objects with the corrected feature names\ndfulltrain = xgb.DMatrix(\n    X_full_train, \n    label=y_full_train, \n    feature_names=feature_names\n)\n\ndtest = xgb.DMatrix(\n    X_test, \n    feature_names=feature_names\n)\n```\n\n**Explanation:** The `dv.get_feature_names_out()` method returns a NumPy array, but `xgb.DMatrix` now expects `feature_names` to be a list of strings. Using `.tolist()` converts the array into a compatible format, allowing the code to run without errors."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-6/002_dc86b83814_how-to-ensure-none-values-are-not-interpreted-as-n.md", "metadata": {"id": "dc86b83814", "question": "How to ensure \"none\" values are not interpreted as NaN when reading a CSV file in Pandas", "sort_order": 2}, "content": "To ensure that the string values like \"None\" are treated as valid strings rather than being converted to NaN when reading a CSV file, you can read the CSV file with `keep_default_na` set to `False` and specify the values you want to consider as NaN with the `na_values` parameter.\n\nHere’s an example of how to do this:\n\n```python\nimport pandas as pd\n\ndf = pd.read_csv(\"dataset_path.csv\", keep_default_na=False, na_values=['', 'NaN', 'null'])\n```\n\nUsing `keep_default_na=False` prevents Pandas from applying its default set of NaN values, allowing \"None\" to be read as a regular string."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-6/003_c6f2a2afe5_how-to-fix-when-capture-output-is-not-working-in-g.md", "metadata": {"id": "c6f2a2afe5", "images": [{"description": "image #1", "id": "image_1", "path": "images/machine-learning-zoomcamp/image_03f8bfd8.jpg"}, {"description": "image #2", "id": "image_2", "path": "images/machine-learning-zoomcamp/image_a95d78a7.jpg"}], "question": "How to fix when %%capture output is not working in Google Collab Notebook", "sort_order": 3}, "content": "I was using Google Collab Notebook for the 2024 cohort HW 06. For Question 6, the following was not working in the Collab Notebook:\n\n<{IMAGE:image_1}>\n\n<{IMAGE:image_2}>\n\nThis led me to find a solution as follows:\n\n1. **Import the required libraries:**\n\n   ```python\n   import io\n   import sys\n   ```\n\n2. **Capture output using `io.StringIO`:**\n\n   ```python\n   output_capture = io.StringIO()\n   sys.stdout = output_capture  # Redirect stdout to the StringIO buffer\n   \n   # Train the model with eta=0.3\n   model_eta_03 = xgb.train(xgb_params, dtrain, num_boost_round=num_rounds, verbose_eval=2, evals=watchlist)\n   \n   # Reset stdout\n   sys.stdout = sys.__stdout__\n   \n   # Retrieve and print the captured output\n   captured_output = output_capture.getvalue()\n   ```\n\n3. **Modify the parser function for one line:**\n\n   Replace this line in Alexey’s parser function:\n\n   ```python\n   for line in output.stdout.strip().split('\\n'):\n   ```\n\n   With this line:\n\n   ```python\n   for line in output.strip().split('\\n'):\n   ```\n\n4. **Call the parser function:**\n\n   Use `df_score_03 = parse_xgb_output(captured_output)` to get the desired dataframe."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-6/004_7ec4b76943_how-to-get-the-training-and-validation-metrics-fro.md", "metadata": {"id": "7ec4b76943", "images": [{"description": "image #1", "id": "image_1", "path": "images/machine-learning-zoomcamp/image_ca7081bf.png"}], "question": "How to get the training and validation metrics from XGBoost?", "sort_order": 4}, "content": "During the XGBoost lesson, we created a parser to extract the training and validation AUC from the standard output. However, we can accomplish that in a more straightforward way.\n\nWe can use the `evals_result` parameter, which takes an empty dictionary and updates it for each tree. Additionally, you can store the data in a dataframe and plot it in an easier manner.\n\n<{IMAGE:image_1}>"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-6/005_049c4cee4e_how-to-solve-regression-problems-with-random-fores.md", "metadata": {"id": "049c4cee4e", "question": "How to solve regression problems with random forest in scikit-learn?", "sort_order": 5}, "content": "You should create a `sklearn.ensemble.RandomForestRegressor` object. It’s similar to `sklearn.ensemble.RandomForestClassifier` for classification problems. For more information, check the [official documentation](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html)."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-6/006_b78f247dc5_valueerror-feature_names-must-be-string-and-may-no.md", "metadata": {"id": "b78f247dc5", "question": "ValueError: feature_names must be string, and may not contain [, ] or <:", "sort_order": 6}, "content": "When creating DMatrix for train and validation, you might encounter the error:\n\n```\nValueError: feature_names must be string, and may not contain [, ] or <\n```\n\nThe cause of this error is special characters in feature names, such as `=` and `<`. To fix this error, you can remove or replace these characters:\n\n```python\nfeatures = [i.replace(\"=<\", \"_\").replace(\"=\",\"_\") for i in features]\n```\n\n\nIf the equal sign `=` is not a problem for you, the following adjustment could also work:\n\n```python\nfeatures = []\n\nfor f in dv.feature_names_:\n    string = f.replace(\"=<\", \"-le\")\n    features.append(string)\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-6/007_23f66acd0a_q6-valueerror-or-typeerror-while-setting-xgbdmatri.md", "metadata": {"id": "23f66acd0a", "question": "Q6: ValueError or TypeError while setting xgb.DMatrix(feature_names=)", "sort_order": 7}, "content": "If you’re encountering a **TypeError** like this:\n\n```\nTypeError: Expecting a sequence of strings for feature names, got: <class 'numpy.ndarray'>\n```\n\nThis might be because you have executed:\n\n```python\nfeatures = dv.get_feature_names_out()\n```\n\nThis returns a `numpy.ndarray` instead of a list. Converting it to a list with `list(features)` won't solve the issue.\n\n\nIf you face a **ValueError** such as:\n\n```\nValueError: feature_names must be string, and may not contain [, ] or <\n```\n\nThis could be because you have tried one of these:\n\n- `features = list(dv.get_feature_names_out())`\n- `features = dv.feature_names_`\n\nThe problem originates from the output of `DictVectorizer`, which might look like:\n\n```\n['households',\n 'housing_median_age',\n 'latitude',\n 'longitude',\n 'median_income',\n 'ocean_proximity=<1H OCEAN',\n 'ocean_proximity=INLAND',\n 'population',\n 'total_bedrooms',\n 'total_rooms']\n```\n\nThe symbols `[, ]` or `<` are not compatible with XGBoost.\n\nSolutions:\n\n1. Do not specify `feature_names=` when creating `xgb.DMatrix`.\n2. Alternatively, you can clean your feature names using regex:\n\n    ```python\n    import re\n    features = dv.feature_names_\n    pattern = r'[\\[\\]<>]'\n    features = [re.sub(pattern, '  ', f) for f in features]\n    ```"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-6/008_bc2d83ddc7_how-to-install-xgboost.md", "metadata": {"id": "bc2d83ddc7", "question": "How to Install Xgboost", "sort_order": 8}, "content": "To install Xgboost, use the following command directly in your Jupyter Notebook:\n\n```bash\npip install xgboost\n```\n\n**Note:** Pip 21.3+ is required.\n\nYou can update your pip using the command below:\n\n```bash\npip install --upgrade pip\n```\n\nFor more information about Xgboost and installation details, check the [official documentation](https://xgboost.readthedocs.io/en/stable/install.html)."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-6/009_c33a1ac422_what-is-eta-in-xgboost.md", "metadata": {"id": "c33a1ac422", "question": "What is eta in XGBoost", "sort_order": 9}, "content": "Sometimes someone might wonder what eta means in the tunable hyperparameters of XGBoost and how it helps the model.\n\nETA is the learning rate of the model. XGBoost uses gradient descent to calculate and update the model. In gradient descent, we are looking for the minimum weights that help the model to learn the data very well. This minimum weight for the features is updated each time the model passes through the features and learns them during training. Tuning the learning rate helps you tell the model what speed it would use in deriving the minimum for the weights."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-6/010_11783b6524_what-is-the-difference-between-bagging-and-boostin.md", "metadata": {"id": "11783b6524", "question": "What is the difference between bagging and boosting?", "sort_order": 10}, "content": "For ensemble algorithms during week 6, one bagging algorithm and one boosting algorithm were presented: Random Forest and XGBoost, respectively.\n\nRandom Forest trains several models in parallel. The output can be, for example, the average value of all the outputs of each model. This is called bagging.\n\nXGBoost trains several models sequentially: the previous model error is used to train the following model. Weights are used to ponderate the models so that the best models have higher weights and are therefore favored for the final output. This method is called boosting.\n\nNote that boosting is not necessarily better than bagging.\n\nBagging stands for “Bootstrap Aggregation”:\n\n- It involves taking multiple samples with replacement to derive multiple training datasets from the original training dataset (bootstrapping).\n- A classifier (e.g., decision trees or stumps for Random Forests) is trained on each such training dataset.\n- The predictions are combined (aggregation) to obtain the final prediction.\n  - For classification, predictions are combined via voting; for regression, via averaging.\n- Bagging can be done in parallel since the various classifiers are independent.\n- Bagging decreases variance (but not bias) and is robust against overfitting.\n\nBoosting, on the other hand, is sequential:\n\n- Each model learns from the mistakes of its predecessor.\n- Observations are given different weights - observations/samples misclassified by the previous classifier are given a higher weight.\n- This process is continued until a stopping condition is reached (e.g., a maximum number of models is reached, or error is acceptably small).\n- Boosting reduces bias and is generally more accurate than bagging, but can be prone to overfitting."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-6/011_609792b49e_capture-stdout-for-each-iteration-of-a-loop-separa.md", "metadata": {"id": "609792b49e", "question": "Capture stdout for each iteration of a loop separately", "sort_order": 11}, "content": "I wanted to directly capture the output from the XGBoost training for multiple eta values to a dictionary without needing to run the same cell multiple times, edit the eta value manually, or copy the code for different eta values.\n\nUsing the magic cell command `%%capture output`, I could only capture the complete output for all iterations of the loop. I was able to solve this using the following approach. This is just a code sample to illustrate the idea:\n\n```python\nfrom IPython.utils.capture import capture_output\nimport sys\n\ndifferent_outputs = {}\n\nfor i in range(3):\n    with capture_output(sys.stdout) as output:\n        print(i)\n        print(\"testing capture\")\n    different_outputs[i] = output.stdout\n\n# Output:\n# different_outputs\n# {0: '0\\ntesting capture\\n',\n#  1: '1\\ntesting capture\\n',\n#  2: '2\\ntesting capture\\n'}\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-6/012_3a1f2deaa8_valueerror-continuous-format-is-not-supported.md", "metadata": {"id": "3a1f2deaa8", "question": "ValueError: continuous format is not supported", "sort_order": 12}, "content": "Calling `roc_auc_score()` to get AUC is throwing the above error.\n\nSolution to this issue is to ensure that you pass `y_actuals` as the first argument and `y_pred` as the second argument.\n\n```python\nroc_auc_score(y_train, y_pred)\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-6/013_04109cf821_what-is-one-method-to-visualize-decision-trees.md", "metadata": {"id": "04109cf821", "question": "What is one method to visualize decision trees?", "sort_order": 13}, "content": "To visualize decision trees, you can use Graphviz along with Scikit-learn's `export_graphviz` method and `plot_tree` function.\n\nHere are two approaches:\n\n1. **Using `export_graphviz`:**\n   ```python\n   from sklearn import tree\n   import graphviz\n\n   dot_data = tree.export_graphviz(regr, out_file=None,\n                                   feature_names=boston.feature_names,\n                                   filled=True)\n   graphviz.Source(dot_data, format=\"png\")\n   ```\n\n2. **Using `plot_tree`:**\n   ```python\n   from sklearn import tree\n   \n   tree.plot_tree(dt, feature_names=dv.feature_names_)\n   ```\n\nBoth methods help in generating a visual representation of the decision tree."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-6/014_2ab69926dc_valueerror-unknown-label-type-continuous.md", "metadata": {"id": "2ab69926dc", "question": "ValueError: Unknown label type: 'continuous'", "sort_order": 14}, "content": "This problem occurs when using `DecisionTreeClassifier` instead of `DecisionTreeRegressor`.\n\nTo resolve this issue:\n\n- Check whether you want to use a decision tree for classification or regression.\n- Use `DecisionTreeRegressor` for regression tasks.\n\n```python\nfrom sklearn.tree import DecisionTreeRegressor\n\n# Example: for regression\nmodel = DecisionTreeRegressor()\nmodel.fit(X_train, y_train)\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-6/015_744f73a640_different-values-of-auc-each-time-code-is-re-run.md", "metadata": {"id": "744f73a640", "question": "Different values of AUC, each time code is re-run", "sort_order": 15}, "content": "When running `dt = DecisionTreeClassifier()` in Jupyter on the same laptop, different AUC values are observed each time it is re-run or after restarting the kernel. Examples include values like 0.674, 0.652, 0.642, 0.669, etc. This variability is discussed in a video between 7:40-7:45 of section 6.3.\n\n**Solution:**\n\n- Set a random seed to ensure reproducibility by using:\n\n  ```python\n  dt = DecisionTreeClassifier(random_state=22)\n  ```"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-6/016_e80cb08772_dictvectorizer-feature-names.md", "metadata": {"id": "e80cb08772", "question": "DictVectorizer: feature names", "sort_order": 16}, "content": "The DictVectorizer has a function to get the feature names using [`get_feature_names_out()`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html#sklearn.feature_extraction.DictVectorizer.get_feature_names_out:~:text=always%202%2Dd.-,get_feature_names_out,-(input_features%3D)). This is helpful if you need to analyze feature importance but are using the dict vectorizer for one-hot encoding. \n\nKeep in mind that it returns a NumPy array, so you may need to convert this to a list depending on your usage. For example:\n\n- `dv.get_feature_names_out()` will return an ndarray of string objects.\n- `list(dv.get_feature_names_out())` will convert it to a standard list of strings.\n\nAlso, ensure that you fit the predictor and response arrays before accessing the feature names."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-6/017_74c19141ab_does-it-matter-if-we-let-the-python-file-create-th.md", "metadata": {"id": "74c19141ab", "question": "Does it matter if we let the Python file create the server or if we run gunicorn directly?", "sort_order": 17}, "content": "They both do the same. Using the script involves less typing."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-6/018_fee2e5fbda_visualize-feature-importance-by-using-horizontal-b.md", "metadata": {"id": "fee2e5fbda", "question": "Visualize Feature Importance by using horizontal bar chart", "sort_order": 18}, "content": "To make it easier to determine which features are important, we can use a horizontal bar chart to illustrate feature importance sorted by value.\n\n1. **Extract the feature importances from the model**\n\n    ```python\n    feature_importances = list(zip(features_names, rdr_model.feature_importances_))\n    importance_df = pd.DataFrame(feature_importances, columns=['feature_names', 'feature_importances'])\n    ```\n\n2. **Sort the DataFrame in descending order using the feature_importances value**\n\n    ```python\n    importance_df = importance_df.sort_values(by='feature_importances', ascending=False)\n    ```\n\n3. **Create a horizontal bar chart**\n\n    ```python\n    plt.figure(figsize=(8, 6))\n    sns.barplot(x='feature_importances', y='feature_names', data=importance_df, palette='Blues_r')\n    plt.xlabel('Feature Importance')\n    plt.ylabel('Feature Names')\n    plt.title('Feature Importance Chart')\n    ```"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-6/019_7fa434b922_rmse-using-metricsroot_meas_square.md", "metadata": {"id": "7fa434b922", "question": "RMSE using metrics.root_meas_square()", "sort_order": 19}, "content": "Instead of using `np.sqrt()` as a second step, you can extract it using:\n\n```python\nmean_squared_error(y_val, y_predict_val, squared=False)\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-6/020_9b9e23bca0_features-importance-graph.md", "metadata": {"id": "9b9e23bca0", "question": "Features Importance graph", "sort_order": 20}, "content": "I like this visual implementation of features importance in scikit-learn library:\n\n[https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html](https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html)\n\nIt adds standard errors to features importance, allowing you to trace the stability of features—important for a model’s explainability—over different parameters of the model."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-6/021_a51d605063_xgboostcorexgboosterror-this-app-has-encountered-a.md", "metadata": {"id": "a51d605063", "question": "xgboost.core.XGBoostError: This app has encountered an error. The original error message is redacted to prevent data leaks.", "sort_order": 21}, "content": "Expanded error:\n\n```\nxgboost.core.XGBoostError: sklearn needs to be installed in order to use this module.\n```\n\nSolution:\n\n- Ensure that `sklearn` is listed in your requirements and installed. This should solve the problem."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-6/022_c0dcde5293_information-gain.md", "metadata": {"id": "c0dcde5293", "question": "Information Gain", "sort_order": 22}, "content": "Information gain in Y due to X, or the mutual information of Y and X:\n\nWhere \\( I(Y; X) = H(Y) - H(Y | X) \\).\n\n- If X is completely uninformative about Y, then \\( I(Y; X) = 0 \\).\n- If X is completely informative about Y, then \\( I(Y; X) = H(Y) \\)."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-6/023_025c1ba06b_data-leakage.md", "metadata": {"id": "025c1ba06b", "question": "Data Leakage", "sort_order": 23}, "content": "Filling in missing values using an entire dataset before splitting for training/testing/validation causes data leakage."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-6/024_8a5d89cbae_saving-and-loading-xgboost.md", "metadata": {"id": "8a5d89cbae", "question": "Saving and loading Xgboost", "sort_order": 24}, "content": "If you have problems with pickling the models, you \ncan use an alternative approach.\n\nSave model by calling `save_model`, load with `load_model`:\n\n```python\nmodel.save_model('model.bin')\n\n...\n\nbst = xgb.Booster()\nbst.load_model('model.bin')\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-6/025_c0444b72ca_why-does-decisiontreeclassifier-and-decisiontreere.md", "metadata": {"id": "c0444b72ca", "question": "Why does DecisionTreeClassifier and DecisionTreeRegressor not throw an error when there are nan (missing) values in the feature matrix?", "sort_order": 25}, "content": "In lesson 6.3 around 6:00, there is an error due to missing values. Subsequently, `.fillna(0)` is used on `df_train` to deal with this. However, since version 1.3, support for missing values has been added for `DecisionTreeClassifier` and `DecisionTreeRegressor`.\n\nMore details can be found [here](https://scikit-learn.org/1.5/whats_new/v1.3.html) under `sklearn.tree`."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-6/026_traversing-feature-names-and-feature-importance-va.md", "metadata": {"id": "0655982228", "question": "Traversing feature names and feature importance values", "sort_order": 26}, "content": "To pair feature names with their importance values, use `dv.get_feature_names_out()` to retrieve the feature names and `rf.feature_importances_` for the importances. Then, combine them with `zip(feature_names, importances)` to view or sort by importance.\n\n```python\n# Assuming rf is your RandomForest model and dv is your DictVectorizer\n\nfeature_names = dv.get_feature_names_out()\nfeature_importances = rf.feature_importances_\n\n# Pair feature names with their importance values\nfeature_importance_dict = dict(zip(feature_names, feature_importances))\n\n# Sort by importance (optional)\nsorted_feature_importance = sorted(feature_importance_dict.items(), key=lambda x: x[1], reverse=True)\n\n# Display results\nfor feature, importance in sorted_feature_importance:\n    print(f\"{feature}: {importance}\")\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-6/027_257c14ccec_which-xgboost-parameters-are-the-most-important-to.md", "metadata": {"id": "257c14ccec", "question": "Which XGBoost parameters are the most important to start with?", "sort_order": 27}, "content": "XGBoost’s performance stems from its flexibility, thanks to a range of parameters.\n\nFor initial tuning, focus on:\n\n- **learning_rate**: Controls the impact of each tree. Lower values (e.g., 0.01–0.1) typically improve performance but require more trees (`n_estimators`).\n- **n_estimators**: Sets the number of boosting rounds; adjust this in conjunction with `learning_rate`.\n- **max_depth**: Prevents overfitting by limiting the tree’s depth.\n- **subsample**: Dictates the fraction of samples used for training each tree, adding randomness to improve generalization.\n\nBegin with these parameters before exploring others, like `gamma` and `min_child_weight`, for additional control over model complexity and performance."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-6/028_c2f713127c_how-to-get-feature-importance-for-xgboost-model.md", "metadata": {"id": "c2f713127c", "question": "How to get feature importance for XGboost model", "sort_order": 28}, "content": "Using `model.feature_importances_` can give you an error:\n\n```\nAttributeError: 'Booster' object has no attribute 'feature_importances_'\n```\n\nIf you train the model like this: `model = xgb.train`, you should use `get_score()` instead."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-8/001_89f4d77539_how-to-install-tensorflow-in-ubuntu-wsl2.md", "metadata": {"id": "89f4d77539", "question": "How to install Tensorflow in Ubuntu WSL2", "sort_order": 1}, "content": "Running a CNN on your CPU can take a long time, and once you’ve run out of free time on some cloud providers, it’s time to pay up. Both can be tackled by installing TensorFlow with CUDA support on your local machine if you have the right hardware.\n\nI was able to get it working by using the following resources:\n\n- [CUDA on WSL :: CUDA Toolkit Documentation (nvidia.com)](https://docs.nvidia.com/cuda/wsl-user-guide/index.html)\n- [Install TensorFlow with pip](https://www.tensorflow.org/install/pip#windows-wsl2)\n- [Start Locally | PyTorch](https://pytorch.org/get-started/locally/)\n\nI included the link to PyTorch so that you can get that one installed and working too while everything is fresh on your mind. Just select your options, and for Computer Platform, I chose CUDA 11.7 and it worked for me.\n\nIf you need GPU support, check this article: [https://knowmledge.com/2023/12/07/ml-zoomcamp-2023-project/](https://knowmledge.com/2023/12/07/ml-zoomcamp-2023-project/)"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-8/002_7ab955e247_how-to-use-kaggle-for-deep-learning.md", "metadata": {"id": "7ab955e247", "images": [{"description": "image #1", "id": "image_1", "path": "images/machine-learning-zoomcamp/image_af398803.png"}], "question": "How to use Kaggle for Deep Learning?", "sort_order": 2}, "content": "- Create or import your notebook into Kaggle.\n- Click on the three dots at the top right-hand side.\n- Click on \"Accelerator.\"\n- Choose \"T4 GPU.\"\n\n<{IMAGE:image_1}>"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-8/003_76134388e5_how-to-use-google-colab-for-deep-learning.md", "metadata": {"id": "76134388e5", "images": [{"description": "image #1", "id": "image_1", "path": "images/machine-learning-zoomcamp/image_982c6309.png"}, {"description": "image #2", "id": "image_2", "path": "images/machine-learning-zoomcamp/image_ce53403a.png"}], "question": "How to use Google Colab for Deep Learning?", "sort_order": 3}, "content": "1. Create or import your notebook into Google Colab.\n2. Click on the drop-down at the top right-hand side.\n3. Click on “Change runtime type.”\n4. Choose T4 GPU.\n\n<{IMAGE:image_1}>\n\n<{IMAGE:image_2}>"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-8/004_be8e47a85c_how-do-i-push-from-saturn-cloud-to-github.md", "metadata": {"id": "be8e47a85c", "question": "How do I push from Saturn Cloud to Github?", "sort_order": 4}, "content": "Connecting your GPU on Saturn Cloud to a Github repository is not compulsory, as you can download the notebook and copy it to the Github folder manually. However, if you prefer an automated approach, follow these instructions:\n\n1. **Create SSH Keys**:\n   - Refer to the following GitHub documentation to generate an SSH private and public key:\n     - [Generating a new SSH key and adding it to the SSH agent](https://docs.github.com/en/authentication/connecting-to-github-with-ssh/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent)\n     - [Adding a new SSH key to your GitHub account](https://docs.github.com/en/authentication/connecting-to-github-with-ssh/adding-a-new-ssh-key-to-your-github-account?tool=webui)\n\n2. **Authenticating via Terminal**:\n   - Access the second video in the Saturn Cloud module to learn how to add SSH keys to secrets and authenticate via a terminal.\n\n3. **Using Saturn Cloud's Default Public Keys**\n   - Click on your username and select \"Manage\".\n   - In the \"Git SSH keys\" section, copy the default public key provided by Saturn Cloud.\n   - Paste this key into the SSH keys section of your GitHub repository.\n   - Open a terminal on Saturn Cloud and run the following command:\n\n```bash\nssh -T git@github.com\n```\n\nYou should receive a successful authentication notice.\n\nFollow these steps to efficiently push from Saturn Cloud to GitHub."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-8/005_dec67e98e1_where-is-the-python-tensorflow-template-on-saturn.md", "metadata": {"id": "dec67e98e1", "question": "Where is the Python TensorFlow template on Saturn Cloud?", "sort_order": 5}, "content": "The template referred to in the video 8.1b [Setting up the Environment on Saturn Cloud](https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/08-deep-learning/01b-saturn-cloud.md) has been moved.\n\nYou can now find it under \"Python Deep Learning Tutorials\" on the Saturn Cloud homepage.\n\nFor updated text instructions (as of Nov 2024) for setting up Saturn Cloud with TensorFlow and GPU, refer to the link provided above."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-8/006_9dc70bef80_getting-error-module-scipy-not-found-during-model.md", "metadata": {"id": "9dc70bef80", "question": "Getting error module scipy not found during model training in Saturn Cloud tensorflow image", "sort_order": 6}, "content": "The error occurs because the `scipy` module is not installed in the Saturn Cloud TensorFlow image.\n\nTo resolve this issue:\n\n1. When creating the Jupyter server resource, navigate to the \"Extra Packages\" section.\n2. In the pip textbox, write `scipy`.\n3. A command will appear below the textbox: `pip install scipy`.\n4. This ensures that when the resource starts, the `scipy` package will be automatically installed.\n\nThis method can be used to install additional Python packages as needed."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-8/007_c76ca2f769_how-to-upload-kaggle-data-to-saturn-cloud.md", "metadata": {"id": "c76ca2f769", "question": "How to upload Kaggle data to Saturn Cloud?", "sort_order": 7}, "content": "Uploading data directly from Kaggle to Saturn Cloud can save time, especially for large datasets. You can download it to your local machine and then upload it to Saturn Cloud, but there is a more efficient method.\n\n\n1. **Install Kaggle Package**\n   \n   Run the following command in your notebook:\n   \n   ```bash\n   !pip install -q kaggle\n   ```\n\n2. **Generate Kaggle API Token**\n   \n   - Go to the Kaggle website and log into your account.\n   - Click on your profile image and select \"Account.\"\n   - Scroll down to the \"API\" section.\n   - Click on \"Create New API Token.\" A JSON file named `kaggle.json` will download to your local computer.\n\n3. **Upload the Kaggle API Token to Saturn Cloud**\n   \n   - In your notebook, click on the folder icon in the upper left corner to navigate to the root folder.\n   - Click on the `.kaggle` folder.\n   - Upload the `kaggle.json` file into the `.kaggle` folder.\n\n4. **Set File Permissions**\n   \n   Run this command to secure your Kaggle API token:\n   \n   ```bash\n   !chmod 600 /home/jovyan/.kaggle/kaggle.json\n   ```\n\n5. **Download the Dataset**\n   \n   Use the following command to download your desired dataset (e.g., the \"dino-or-dragon\" dataset):\n   \n   ```bash\n   !kaggle datasets download -d agrigorev/dino-or-dragon\n   ```\n\n6. **Unzip the Dataset**\n   \n   - Create a folder to unzip your files:\n     \n     ```bash\n     !mkdir data\n     ```\n   \n   - Unzip your files inside that folder:\n     \n     ```bash\n     !unzip dino-or-dragon.zip -d data\n     ```\n\nBy following these steps, you set up Saturn Cloud with access to all Kaggle datasets efficiently."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-8/008_f3254c8c94_how-to-install-cuda-cudnn-on-ubuntu-2204.md", "metadata": {"id": "f3254c8c94", "question": "How to install CUDA & cuDNN on Ubuntu 22.04", "sort_order": 8}, "content": "In order to run TensorFlow with GPU on your local machine, you’ll need to set up CUDA and cuDNN.\n\nThe process can be overwhelming. Here’s a simplified [guide](https://gist.github.com/denguir/b21aa66ae7fb1089655dd9de8351a202)."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-8/009_7915e5968b_valueerror-unable-to-load-weights-saved-in-hdf5-fo.md", "metadata": {"id": "7915e5968b", "question": "ValueError: Unable to load weights saved in HDF5 format into a subclassed Model", "sort_order": 9}, "content": "When loading a saved model, you encounter the error:\n\n```\nValueError: Unable to load weights saved in HDF5 format into a subclassed Model which has not created its variables yet. Call the Model first, then load the weights.\n```\n\n\nBefore loading the model, you need to evaluate the model on input data:\n\n```python\nmodel.evaluate(train_ds)\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-8/010_a19360f1ab_getting-error-when-connect-git-on-saturn-cloud-per.md", "metadata": {"id": "a19360f1ab", "question": "Getting error when connect git on Saturn Cloud: permission denied", "sort_order": 10}, "content": "When following module 8.1b video to set up Git in Saturn Cloud, running the command:\n\n```bash\nssh -T git@github.com\n```\n\nresults in the error:\n\n```bash\nPermission denied (publickey).\n```\n\n\nAn alternative method involves setting up Git in your Saturn Cloud environment by generating an SSH key in Saturn Cloud and adding it to your Git account. After completing this setup, you can access and manage your Git repositories through Saturn’s Jupyter server.\n\nFor detailed steps, refer to this tutorial: [https://saturncloud.io/docs/using-saturn-cloud/gitrepo/](https://saturncloud.io/docs/using-saturn-cloud/gitrepo/)"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-8/011_4538b68902_cloning-into-clothing-dataset-host-key-verificatio.md", "metadata": {"id": "4538b68902", "question": "Cloning into 'clothing-dataset'... Host key verification failed.", "sort_order": 11}, "content": "Getting an error using \n\n```bash\ngit clone git@github.com:alexeygrigorev/clothing-dataset-small.git\n```\n\nThe error:\n\n```\nCloning into 'clothing-dataset'...\n\nHost key verification failed.\n\nfatal: Could not read from remote repository.\n\nPlease make sure you have the correct access rights\nand the repository exists.\n```\n\n\nWhen cloning the repo, you can also choose HTTPS, then it should work. This error occurs when your SSH key is not configured.\n\nUse the following command:\n\n```bash\ngit clone https://github.com/alexeygrigorev/clothing-dataset-small.git\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-8/012_cd934427d8_the-same-accuracy-on-epochs.md", "metadata": {"id": "cd934427d8", "question": "The same accuracy on epochs", "sort_order": 12}, "content": "The accuracy and the loss are both the same or nearly the same while training.\n\n- In the homework, set `class_mode='binary'` while reading the data.\n- The problem may also occur if you choose the wrong optimizer, batch size, or learning rate."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-8/013_57e17a3e5f_model-breaking-after-augmentation-high-loss-bad-ac.md", "metadata": {"id": "57e17a3e5f", "question": "Model breaking after augmentation – high loss + bad accuracy", "sort_order": 13}, "content": "When resuming training after augmentation, the loss skyrockets (1000+ during the first epoch) and accuracy settles around 0.5 – i.e. the model becomes as good as a random coin flip.\n\n**Check** that the augmented `ImageDataGenerator` still includes the `rescale` option as specified in the preceding step."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-8/014_2ab8be1a97_missing-channel-value-error-while-reloading-model.md", "metadata": {"id": "2ab8be1a97", "question": "Missing channel value error while reloading model:", "sort_order": 14}, "content": "While attempting to reload a model with TensorFlow, you might encounter the following error:\n\n```\nValueError: The channel dimension of the inputs should be defined. The input_shape received is (None, None, None, None), where axis -1 (0-based) is the channel dimension, which found to be `None`.\n```\n\nThis error is usually caused when the number of channels is not explicitly defined in the Input layer of the model. \n\nEnsure that you explicitly specify the number of channels in the Input layer of the model architecture.\n\nExample:\n\n```python\nimport tensorflow as tf\nfrom tensorflow import keras\n\n# Model architecture:\ninputs = keras.Input(shape=(input_size, input_size, 3))\n\nbase = base_model(inputs, training=False)\nvectors = keras.layers.GlobalAveragePooling2D()(base)\ninner = keras.layers.Dense(size_inner, activation='relu')(vectors)\ndrop = keras.layers.Dropout(droprate)(inner)\noutputs = keras.layers.Dense(10)(drop)\n\nmodel = keras.Model(inputs, outputs)\n```\n\nThis configuration ensures that the channel dimension is explicitly defined, preventing the reload error."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-8/015_0e58ef1736_how-to-unzip-a-folder-with-an-image-dataset-and-su.md", "metadata": {"id": "0e58ef1736", "question": "How to unzip a folder with an image dataset and suppress output?", "sort_order": 15}, "content": "If you unzip a dataset within a Jupyter Notebook using the `! unzip` command, you may encounter extensive output messages for each file. To suppress this output, follow these solutions:\n\nUsing Magic Commands\n\n```python\n%%capture\n\n! unzip zipped_folder_name.zip -d destination_folder_name\n```\n\nUsing Python's zipfile Library\n\n```python\nimport zipfile\n\nlocal_zip = 'data.zip'\nzip_ref = zipfile.ZipFile(local_zip, 'r')\nzip_ref.extractall('data')\nzip_ref.close()\n```\n\nUse Solution 1 to suppress output directly in the notebook. Solution 2 provides an alternative approach using Python code."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-8/016_6c51e7d7bb_how-does-keras-flow_from_directory-know-the-names.md", "metadata": {"id": "6c51e7d7bb", "question": "How does keras flow_from_directory know the names of classes in images?", "sort_order": 16}, "content": "Keras `flow_from_directory` understands the names of classes from the names of folders.\n\n\n- When using `train_gen.flow_from_directory()`, the class names are derived from the folder names within the specified directory.\n- For example, if you create a folder named \"xyz\", it will be considered a class.\n- This behavior aligns with the function name `flow_from_directory`.\n\nA detailed explanation can be found in this [tutorial](https://vijayabhaskar96.medium.com/tutorial-image-classification-with-keras-flow-from-directory-and-generators-95f75ebe5720)."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-8/017_0377df757c_error-with-scipy-missing-module-in-saturncloud.md", "metadata": {"id": "0377df757c", "question": "Error with scipy missing module in SaturnCloud", "sort_order": 17}, "content": "I created a new environment in SaturnCloud and chose the image corresponding to Saturn with Tensorflow, but when I tried to fit the model it showed an error about the missing module: `scipy`.\n\n\n\n1. Install the module in a new cell:\n   \n   ```bash\n   !pip install scipy\n   ```\n\n2. Restart the kernel and fit the model again."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-8/018_98f2d2b078_how-are-numeric-class-labels-determined-in-flow_fr.md", "metadata": {"id": "98f2d2b078", "question": "How are numeric class labels determined in flow_from_directory using binary class mode and what is meant by the single probability predicted by a binary Keras model:", "sort_order": 18}, "content": "The command to read folders in the dataset in the TensorFlow source code is:\n\n```python\nfor subdir in sorted(os.listdir(directory)):\n```\n\nReference: [Keras Image Preprocessing](https://github.com/keras-team/keras/blob/master/keras/preprocessing/image.py), line 563.\n\nThis means folders will be read in alphabetical order. For example, with a folder named `dino` and another named `dragon`, `dino` will be read first and will have class label 0, whereas `dragon` will be read next and will have class label 1.\n\nWhen a Keras model predicts binary labels, it returns one value, which is the probability of class 1. This occurs with the sigmoid activation function in the last dense layer with 2 neurons. The probability of class 0 can be calculated as:\n\n```\nprob(class(0)) = 1 - prob(class(1))\n```\n\nIn the case of using `from_logits` to get results, you will receive two values for each of the labels.\n\nA prediction of 0.8 indicates that the probability the image has class label 1 (in this case, `dragon`) is 0.8. Conversely, the probability that the image has class label 0 is 0.2."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-8/019_55a29a88a1_what-if-your-accuracy-and-std-training-loss-dont-m.md", "metadata": {"id": "55a29a88a1", "question": "What if your accuracy and std training loss don’t match HW?", "sort_order": 19}, "content": "Running the wasp/bee model on a Mac laptop might result in higher reported accuracy and lower standard deviation than expected from the HW answers. This discrepancy could be related to the version of the SGD optimizer being used. A message may appear about new and legacy versions.\n\n- Try running the same code on Google Colab or another platform. The results may align closer with HW answers on Colab.\n- Change the runtime to use a T4 GPU for faster execution compared to CPU."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-8/020_c0ceddd1a3_using-multi-threading-for-data-generation-in-model.md", "metadata": {"id": "c0ceddd1a3", "question": "Using multi-threading for data generation in “model.fit()”", "sort_order": 20}, "content": "When running `model.fit(...)`, an additional parameter `workers` can be specified for speeding up data loading and generation. The default value is `1`. Try different values between `1` and the CPU count on your system to check which performs best.\n\nFor more information, refer to the [TensorFlow documentation](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit)."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-8/021_ee14ae778d_reproducibility-with-tensorflow-using-a-seed-point.md", "metadata": {"id": "ee14ae778d", "question": "Reproducibility with TensorFlow using a seed point", "sort_order": 21}, "content": "Reproducibility for training runs can be achieved by following these instructions: [TensorFlow Documentation](https://www.tensorflow.org/versions/r2.8/api_docs/python/tf/config/experimental/enable_op_determinism)\n\n```python\nseed = 1234\n\ntf.keras.utils.set_random_seed(seed)\n\ntf.config.experimental.enable_op_determinism()\n```\n\nThis ensures consistent results when the script is executed multiple times."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-8/022_05c264fddf_can-we-use-pytorch-for-this-lessonhomework.md", "metadata": {"id": "05c264fddf", "question": "Can we use PyTorch for this lesson/homework?", "sort_order": 22}, "content": "PyTorch is also a deep learning framework that allows you to perform equivalent tasks as Keras. Here is a tutorial to create a CNN from scratch using PyTorch:\n\n[Writing CNNs from Scratch in PyTorch](https://blog.paperspace.com/writing-cnns-from-scratch-in-pytorch/)\n\nThe functions have similar goals, although the syntax may vary slightly. For the lessons and the homework, we use Keras, but you are free to make a pull request with the equivalent implementation using PyTorch for the lessons and homework!"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-8/023_ace84c3970_keras-model-training-fails-with-failed-to-find-dat.md", "metadata": {"id": "ace84c3970", "question": "Keras: Model training fails with “Failed to find data adapter”", "sort_order": 23}, "content": "While training a Keras model, you may encounter the error:\n\n```\nFailed to find data adapter that can handle input: <class 'keras.src.preprocessing.image.ImageDataGenerator'>, <class 'NoneType'>\n```\n\nThis typically occurs if you accidentally pass the image generator instead of the dataset to the model. Here is an example of incorrect usage:\n\n```python\ntrain_gen = ImageDataGenerator(rescale=1./255)\n\ntrain_ds = train_gen.flow_from_directory(…)\n\nhistory_after_augmentation = model.fit(\n    train_gen,  # this should be train_ds!!!\n    epochs=10,\n    validation_data=test_gen  # this should be test_ds!!!\n)\n```\n\n\nThe fix is straightforward. Use the training and validation datasets (`train_ds` and `val_ds`) returned from `flow_from_directory`:\n\n- Ensure you pass `train_ds` instead of `train_gen` when calling `model.fit()`.\n- Similarly, use `val_ds` for `validation_data` instead of `test_gen`."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-8/024_e3c5c81223_running-nvidia-smi-in-a-loop-without-using-watch.md", "metadata": {"id": "e3c5c81223", "question": "Running ‘nvidia-smi’ in a loop without using ‘watch’", "sort_order": 24}, "content": "The command `nvidia-smi` has a built-in function that allows it to run in a loop, updating every N seconds, without using the `watch` command.\n\n```bash\nnvidia-smi -l <N seconds>\n```\n\nFor example, the following command will run `nvidia-smi` every 2 seconds until interrupted by pressing `CTRL+C`:\n\n```bash\nnvidia-smi -l 2\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-8/025_38c419b57c_checking-gpu-and-cpu-utilization-using-nvitop.md", "metadata": {"id": "38c419b57c", "images": [{"description": "image #1", "id": "image_1", "path": "images/machine-learning-zoomcamp/image_e3c103cd.png"}], "question": "Checking GPU and CPU utilization using ‘nvitop’", "sort_order": 25}, "content": "The Python package `nvitop` is an interactive GPU process viewer similar to `htop` for CPU.\n\n[https://pypi.org/project/nvitop/](https://pypi.org/project/nvitop/)\n\n<{IMAGE:image_1}>\n\nImage source: [https://pypi.org/project/nvitop/](https://pypi.org/project/nvitop/)"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-8/026_52fbe7351e_where-does-the-number-of-conv2d-layers-params-come.md", "metadata": {"id": "52fbe7351e", "question": "Where does the number of Conv2d layer’s params come from? Where does the number of 'features' we get after the Flatten layer come from?", "sort_order": 26}, "content": "Let's say we define our Conv2d layer like this:\n\n```python\n tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(150, 150, 3))\n```\n\nThis means our input image is RGB (3 channels, 150 by 150 pixels), the kernel is 3x3, and the number of filters (layer’s width) is 32.\n\nIf we check `model.summary()` we will get this:\n\n```\n_________________________________________________________________\nLayer (type)                Output Shape              Param #\n=================================================================\nconv2d (Conv2D)             (None, 148, 148, 32)      896\n```\n\nSo where do 896 params come from? It’s computed like this:\n\n```python\n(3*3*3 + 1) * 32\n```\n\nThis results in 896:\n\n- 3x3 kernel\n- 3 channels RGB\n- +1 for bias\n- 32 filters\n\n\nNumber of 'Features' after the Flatten Layer\n\nFor our homework, `model.summary()` for the last MaxPooling2d and Flatten layers looked like this:\n\n```\n_________________________________________________________________\nLayer (type)                Output Shape              Param #\n=================================================================\nmax_pooling2d_3       (None, 7, 7, 128)         0\nflatten (Flatten)           (None, 6272)              0\n```\n\nSo where do 6272 vectors come from? It’s computed like this:\n\n```python\n7*7*128\n```\n\nThis results in 6272:\n\n- 7x7 \"image shape\" after several convolutions and poolings\n- 128 filters"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-8/027_ddc7d09c3d_sequential-vs-functional-model-modes-in-keras-tf2.md", "metadata": {"id": "ddc7d09c3d", "question": "Sequential vs. Functional Model Modes in Keras (TF2)", "sort_order": 27}, "content": "It’s useful to understand that all types of models in the course are a plain stack of layers where each layer has exactly one input tensor and one output tensor. See the [Sequential model TF page](https://www.tensorflow.org/guide/keras/sequential_model) and the [Sequential class](https://keras.io/api/models/sequential/).\n\nYou can start with an “empty” model and add more layers in sequential order. This is called the “Sequential Model API,” which is easier to use.\n\nIn Alexey’s videos, it’s implemented as chained calls of different entities (“inputs,” “base,” “vectors,” “outputs”) in a more advanced mode, the “Functional Model API.” A more complicated approach makes sense for Transfer Learning, where you want to separate the “Base” model from the rest, but in homework, you're required to recreate the full model from scratch. It might be easier to work with a sequence of similar layers.\n\nFor more information, see this TF2 [tutorial](https://machinelearningmastery.com/tensorflow-tutorial-deep-learning-with-tf-keras/).\n\nA useful Sequential model example is available in Kaggle’s “Bee or Wasp” dataset folder: [notebook](https://www.kaggle.com/code/tammygusmao/bee-or-wasp-from-scratch-to-transfer-learning)."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-8/028_92eac8d507_out-of-memory-errors-when-running-tensorflow.md", "metadata": {"id": "92eac8d507", "question": "Out of memory errors when running tensorflow", "sort_order": 28}, "content": "I found this code snippet fixed my OOM errors, as I have an Nvidia GPU. Can't speak to OOM errors on CPU, though.\n\n[Official Documentation](https://www.tensorflow.org/api_docs/python/tf/config/experimental/set_memory_growth)\n\n```python\nphysical_devices = tf.config.list_physical_devices('GPU')\n\ntry:\n    tf.config.experimental.set_memory_growth(physical_devices[0], True)\nexcept:\n    # Invalid device or cannot modify virtual devices once initialized.\n    pass\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-8/029_bc8bd71f9c_how-did-i-resolve-the-out-of-memory-oom-issue-when.md", "metadata": {"id": "bc8bd71f9c", "question": "How did I resolve the out of memory (OOM) issue when training my model on a GPU?", "sort_order": 29}, "content": "To address the out of memory (OOM) issue, I followed these steps:\n\n1. **Check GPU Memory Usage:**\n   \n   I ran the following command to see how much memory was being used and which processes were occupying it:\n\n   ```bash\n   !nvidia-smi\n   ```\n\n   This command provided details about memory usage and active processes on the GPU.\n\n2. **Identify Active Processes:**\n\n   From the output of `nvidia-smi`, I noticed that a Python process (e.g., `...a3/envs/tensorflow2_p310/bin/python`) was consuming a significant amount of GPU memory.\n\n3. **Terminate the Python Process:**\n\n   I used the process ID (PID) to kill the Python process that was consuming the excessive memory. For example, to kill a process with PID 11208, I executed:\n\n   ```bash\n   !kill 11208\n   ```\n\n4. **Kernel Restart:**\n\n   After terminating the process, I noticed that the kernel automatically restarted, freeing up the GPU memory.\n\n5. **Recheck GPU Memory:**\n\n   I ran `nvidia-smi` again to confirm that the memory usage had decreased, and there were no longer any blocking processes.\n\nBy following these steps, I was able to free up GPU memory and continue training my model successfully."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-8/030_39112d0ede_model-training-very-slow-in-google-colab-with-t4-g.md", "metadata": {"id": "39112d0ede", "question": "Model training very slow in google colab with T4 GPU", "sort_order": 30}, "content": "When training models in Google Colab, you can improve performance by specifying the number of workers/threads in the `fit` function.\n\nIncreasing the number of threads can also be beneficial for GPUs. This adjustment proved useful for the T4 GPU in Google Colab, as the default value for workers is 1, which can result in very slow processing.\n\nTo improve performance:\n\n- Change the `workers` variable to a higher value, such as 2560, to accelerate model training.\n\nFor further information, consult this [Stack Overflow thread](https://stackoverflow.com/questions/68208398/how-to-find-the-number-of-cores-in-google-colabs-gpu)."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-8/031_2584892a72_using-image_dataset_from_directory-instead-of-imag.md", "metadata": {"id": "2584892a72", "question": "Using image_dataset_from_directory instead of ImageDataGenerator for loading images", "sort_order": 31}, "content": "From the Keras documentation:\n\nDeprecated: [`tf.keras.preprocessing.image.ImageDataGenerator`](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator) is not recommended for new code.\n\nPrefer loading images with [`tf.keras.utils.image_dataset_from_directory`](https://www.tensorflow.org/api_docs/python/tf/keras/utils/image_dataset_from_directory) and transforming the output [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) with preprocessing layers.\n\nFor more information, see the tutorials for [loading images](https://www.tensorflow.org/tutorials/load_data/images) and [augmenting images](https://www.tensorflow.org/tutorials/images/data_augmentation), as well as the [preprocessing layer guide](https://www.tensorflow.org/guide/keras/preprocessing_layers)."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-8/032_1d2b397d4f_saturn-cloud-openssl-version-mismatch-built-agains.md", "metadata": {"id": "1d2b397d4f", "question": "Saturn Cloud: OpenSSL version mismatch. Built against 30000020, you have 30300020", "sort_order": 32}, "content": "This error occurs because the OpenSSH client is built against a specific version of OpenSSL (e.g., 3.0.0), but the system tries to use a different version (e.g., 3.0.3). This mismatch prevents the SSH client from working properly.\n\n\nSet the correct OpenSSL library path by running the following command in the terminal:\n\n```bash\nexport LD_LIBRARY_PATH=/usr/lib/x86_64-linux-gnu\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-8/033_56e523f708_how-can-data-augmentation-improve-model-performanc.md", "metadata": {"id": "56e523f708", "question": "How can data augmentation improve model performance?", "sort_order": 33}, "content": "Data augmentation artificially expands the training dataset by applying transformations like flipping, cropping, and adjusting brightness or contrast. This improves model robustness by exposing it to varied data and helps reduce overfitting."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-9/001_07cc362868_where-is-the-model-for-week-9.md", "metadata": {"id": "07cc362868", "question": "Where is the model for week 9?", "sort_order": 1}, "content": "The week 9 uses a link to GitHub to fetch the models.\n\nThe original link was moved to here:\n\n[GitHub](https://github.com/DataTalksClub/machine-learning-zoomcamp/releases)"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-9/001_5f69e4c975_why-onnx-runtime-deployment.md", "metadata": {"id": "5f69e4c975", "question": "Why ONNX Runtime for DL deployment?", "sort_order": 1}, "content": "ONNX Runtime provides a cross-framework inference engine for models exported to the ONNX format. If you train in TensorFlow or PyTorch, you can export your model to ONNX and run it with ONNX Runtime, enabling a common deployment path across different environments. This can simplify Lambda/Kubernetes serving by reducing the need to maintain separate framework-specific runtimes (such as TensorFlow Serving or TorchServe) and by providing optimizations available in ONNX Runtime."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-9/002_b634303091_executing-the-command-echo-remote_uri-returns-noth.md", "metadata": {"id": "b634303091", "question": "Executing the command echo ${REMOTE_URI} returns nothing.", "sort_order": 2}, "content": "In Unit 9.6, the command `echo ${REMOTE_URI}` is used to print the URI address, but if it returns nothing, the variable may not be set correctly. Here's a solution to address this:\n\n- Set a local variable and assign the URI address in the terminal:\n\n   ```bash\n   REMOTE_URI=2278222782.dkr.ecr.ap-south-1.amazonaws.com/clothing-tflite-images\n   ```\n\n- Use this variable to log in to the registry. Note that this variable will be lost once the session is terminated.\n\nHere's a step-by-step example on Ubuntu terminal, which faced the same issue:\n\n1. Execute the following command to set the environment variable:\n\n   ```bash\n   export REMOTE_URI=1111111111.dkr.ecr.us-west-1.amazonaws.com/clothing-tflite-images:clothing-model-xception-v4-001\n   ```\n\n2. Display the value of the variable:\n\n   ```bash\n   echo $REMOTE_URI\n   ```\n\n   This should print:\n\n   ```\n   111111111.dkr.ecr.us-west-1.amazonaws.com/clothing-tflite-images:clothing-model-xception-v4-001\n   ```\n\n**Note:**\n- The command `echo $REMOTE_URI` does not require curly brackets, unlike in video 9.6.\n- Replace `REMOTE_URI` with your actual URI."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-9/003_ac62d9ccf0_getting-a-syntax-error-while-trying-to-get-the-pas.md", "metadata": {"id": "ac62d9ccf0", "question": "Getting a syntax error while trying to get the password from aws-cli", "sort_order": 3}, "content": "The command `aws ecr get-login --no-include-email` returns an invalid choice error:\n\n```\nInvalid choice error\n```\n\nThe solution is to use the following command instead:\n\n```bash\naws ecr get-login-password\n```\n\nTo simplify the login process, replace `<ACCOUNT_NUMBER>` and `<REGION>` with your values:\n\n```bash\nexport PASSWORD=`aws ecr get-login-password`\n\ndocker login -u AWS -p $PASSWORD <ACCOUNT_NUMBER>.dkr.ecr.<REGION>.amazonaws.com/clothing-tflite-images\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-9/004_b1ecde8177_pass-many-parameters-in-the-model-at-once.md", "metadata": {"id": "b1ecde8177", "question": "Pass many parameters in the model at once", "sort_order": 4}, "content": "We can use the `keras.models.Sequential()` function to pass many parameters of the CNN at once."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-9/005_f14bb9c2ba_getting-error-internal-load-metadata-for-publicecr.md", "metadata": {"id": "f14bb9c2ba", "question": "Getting ERROR [internal] load metadata for public.ecr.aws/lambda/python:3.8", "sort_order": 5}, "content": "This error is produced sometimes when building your Docker image from the Amazon Python base image.\n\n### Solution Description:\n\nThe following could solve the problem:\n\n- **Update Docker Desktop**: Ensure you have the latest version installed.\n- **Restart Docker and Terminal**: Try restarting Docker Desktop and your terminal application, then build the image again.\n- **Disable BuildKit**: If the above steps do not work, run the following command to disable Docker BuildKit and build your image:\n  \n  ```bash\n  DOCKER_BUILDKIT=0 docker build .\n  ```"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-9/006_c336d11260_problem-ls-is-not-recognized-as-an-internal-or-ext.md", "metadata": {"id": "c336d11260", "question": "Problem: 'ls' is not recognized as an internal or external command, operable program or batch file.", "sort_order": 6}, "content": "When trying to run the command `!ls -lh` in a Windows Jupyter Notebook, you may receive an error:\n\n```\n'ls' is not recognized as an internal or external command, operable program or batch file.\n```\n\nSolution:\n\nInstead of `!ls -lh`, you can use this command:\n\n```bash\n!dir\n```\n\nThis will provide similar output."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-9/007_ae3626c120_importerror-generic_type-type-interpreterwrapper-i.md", "metadata": {"id": "ae3626c120", "question": "ImportError: generic_type: type \"InterpreterWrapper\" is already registered!", "sort_order": 7}, "content": "When importing `tflite_runtime.interpreter` using:\n\n```python\nimport tflite_runtime.interpreter as tflite\n```\n\nYou might encounter the error:\n\n```\nImportError: generic_type: type \"InterpreterWrapper\" is already registered!\n```\n\nThis error occurs if you import both `tensorflow` and `tflite_runtime.interpreter` in the same environment. To resolve it:\n\n1. Restart the kernel.\n2. Import only `tflite_runtime.interpreter`:\n   \n   ```python\n   import tflite_runtime.interpreter as tflite\n   ```"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-9/008_1c62848f3d_warning-you-are-using-pip-version-2204-however-ver.md", "metadata": {"id": "1c62848f3d", "question": "WARNING: You are using pip version 22.0.4; however, version 22.3.1 is available", "sort_order": 8}, "content": "When running `docker build -t dino-dragon-model`, you might encounter the warning about an outdated pip version.\n\nThis warning often comes up due to a mismatch in the versions of the wheel file shown in Alex's video. The video might show a version compatible with Python 8, but you need a wheel for the version you are working on, such as Python 9.\n\nAdditionally, ensure you download the wheel file using its raw format link, as copying the link might cause errors. Use the following link:\n\n[GitHub](https://github.com/alexeygrigorev/tflite-aws-lambda/raw/main/tflite/tflite_runtime-2.7.0-cp39-cp39-linux_x86_64.whl)\n\nEnsure to address the pip version warning when possible by updating pip using:\n\n```bash\npip install --upgrade pip\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-9/009_52d7a5d6b2_how-to-do-aws-configure-after-installing-awscli.md", "metadata": {"id": "52d7a5d6b2", "question": "How to do AWS configure after installing awscli", "sort_order": 9}, "content": "In video 9.6, after installing `awscli`, we should configure it with `aws configure`. It asks for Access Key ID, Secret Access Key, Default Region Name, and Default Output Format. What should we put for Default Output Format? Is leaving it as None okay?\n\nYes, you can leave everything as the provided defaults (except for the Access Key and the Secret Access Key)."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-9/010_320a5c1b9a_object-of-type-float32-is-not-json-serializable.md", "metadata": {"id": "320a5c1b9a", "question": "Object of type float32 is not JSON serializable", "sort_order": 10}, "content": "While passing local testing of the lambda function without issues, trying to test the same input with a running Docker instance results in an error message like:\n\n```bash\n{\n  'errorMessage': 'Unable to marshal response: Object of type float32 is not JSON serializable',\n  'errorType': 'Runtime.MarshalError',\n  'requestId': 'f155492c-9af2-4d04-b5a4-639548b7c7ac',\n  'stackTrace': []\n}\n```\n\nThis occurs when a model returns estimation values as NumPy `float32` values. These need to be converted to base-Python floats to become serializable.\n\nIn the following example, the dino vs dragon model returns a label and predicted probability for each class. Below is an excerpt of the `predict()` function in `lambda_function.py`:\n\n```python\npreds = [interpreter.get_tensor(output_index)[0][0], \\\n         1-interpreter.get_tensor(output_index)[0][0]]\n```\n\nTo fix the serialization issue, convert the values to floats:\n\n```python\npreds = [float(interpreter.get_tensor(output_index)[0][0]), \\\n         float(1-interpreter.get_tensor(output_index)[0][0])]\n```\n\nYou can resolve the rest by following the instructions in Chapter 9 (and/or Chapter 5) lecture videos."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-9/011_29f149ff07_error-with-the-line-interpreterset_tensorinput_ind.md", "metadata": {"id": "29f149ff07", "question": "Error with the line interpreter.set_tensor(input_index, X)", "sort_order": 11}, "content": "I had this error when running the command:\n\n```python\ninterpreter.set_tensor(input_index, x)\n```\n\nYou might see this around 12 minutes into video 9.3.\n\nError message:\n\n```\nValueError: Cannot set tensor: Got value of type UINT8 but expected type FLOAT32 for input 0, name: serving_default_conv2d_input:0\n```\n\nThis occurs because `X` is an integer, but a float is expected.\n\nTo resolve this issue, convert `X` to `float32` before using `set_tensor`:\n \n```python\nX = np.float32(X)\n```\n\nWith this conversion, the code works properly. Note that this was tested on TensorFlow 2.15.0, and newer versions may require such changes."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-9/012_8637328bbb_how-to-easily-get-file-size-in-powershell-terminal.md", "metadata": {"id": "8637328bbb", "question": "How to easily get file size in PowerShell terminal?", "sort_order": 12}, "content": "To check your file size using the PowerShell terminal, you can use the following command lines:\n\n```powershell\n$File = Get-Item -Path path_to_file\n$fileSize = (Get-Item -Path $File).Length\n```\n\nNow you can check the size of your file, for example in MB:\n\n```powershell\nWrite-Host \"MB: \" ($fileSize/1MB)\n```\n\n[Source](https://www.sharepointdiary.com/2020/10/powershell-get-file-size.html#:~:text=To%20get%20the%20size%20of,the%20file%2C%20including%20its%20size)"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-9/013_5484fc22f7_how-do-lambda-container-images-work.md", "metadata": {"id": "5484fc22f7", "question": "How do Lambda container images work?", "sort_order": 13}, "content": "To understand how Lambda container images work and how Lambda functions are initialized, refer to the following documentation:\n\n- [AWS Lambda Images - Create](https://docs.aws.amazon.com/lambda/latest/dg/images-create.html)\n- [AWS Lambda Runtimes API](https://docs.aws.amazon.com/lambda/latest/dg/runtimes-api.html)"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-9/014_6bdfe65552_how-to-use-aws-serverless-framework-to-deploy-on-a.md", "metadata": {"id": "6bdfe65552", "question": "How to use AWS Serverless Framework to deploy on AWS Lambda and expose it as REST API through APIGatewayService?", "sort_order": 14}, "content": "The Docker image for AWS Lambda can be created and pushed to AWS ECR, and it can be exposed as a REST API through APIGatewayService using AWS Serverless Framework. Refer to the article below for a detailed walkthrough.\n\n[Deploy Containerized Serverless Flask to AWS Lambda](https://medium.com/hoonio/deploy-containerized-serverless-flask-to-aws-lambda-c0eb87c1404d)"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-9/015_98e0d0b961_error-building-docker-image-on-m1-mac.md", "metadata": {"id": "98e0d0b961", "question": "Error building docker image on M1 Mac", "sort_order": 15}, "content": "While trying to build the Docker image in Section 9.5 with the command:\n\n```bash\ndocker build -t clothing-model .\n```\n\nIt throws a pip install error for the tflite runtime `.whl` file:\n\n```\nERROR: failed to solve: process \"/bin/sh -c pip install https://github.com/alexeygrigorev/tflite-aws-lambda/blob/main/tflite/tflite_runtime-2.14.0-cp310-cp310-linux_x86_64.whl\" did not complete successfully: exit code: 1\n```\n\n1. Try using this direct link for the `.whl` file:\n   - [tflite_runtime-2.14.0](https://github.com/alexeygrigorev/tflite-aws-lambda/raw/main/tflite/tflite_runtime-2.14.0-cp310-cp310-linux_x86_64.whl)\n\n2. If the link above does not work:\n   - The problem occurs due to the ARM architecture of the M1. You may need to run the code on a PC or Ubuntu OS.\n\n3. You can also try the commands below:\n   \n   - To build the Docker image:\n     \n     ```bash\n     docker build --platform linux/amd64 -t clothing-model .\n     ```\n\n   - To run the built image:\n     \n     ```bash\n     docker run -it --rm -p 8080:8080 --platform linux/amd64 clothing-model:latest\n     ```"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-9/016_1a166fc048_error-invoking-api-gateway-deploy-api-locally.md", "metadata": {"id": "1a166fc048", "question": "Error invoking API Gateway deploy API locally", "sort_order": 16}, "content": "When attempting to test the API gateway in [9.7 - API Gateway: Exposing the Lambda Function](https://www.youtube.com/watch?v=wyZ9aqQOXvs&list=PL3MmuxUbc_hIhxl5Ji8t4O6lPAOpHaCLR), running the command:\n\n```bash\npython test.py\n```\n\nYou might encounter the following error message:\n\n```python\n{'message': 'Missing Authentication Token'}\n```\n\n\nYou need to ensure you have the correct deployed API URL for the specific path you are invoking. An example of a correct URL format is:\n\n```\nhttps://<random_string>.execute-api.us-east-2.amazonaws.com/test/predict\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-9/017_8b5cf23fcf_error-could-not-find-a-version-that-satisfies-the.md", "metadata": {"id": "8b5cf23fcf", "question": "Error: Could not find a version that satisfies the requirement tflite_runtime (from versions: none)", "sort_order": 17}, "content": "When trying to install `tflite_runtime` using the command below, you receive an error message:\n\n```bash\n!pip install --extra-index-url https://google-coral.github.io/py-repo/ tflite_runtime\n```\n\n\n`tflite_runtime` is only available for specific OS-Python version combinations. You can find the available combinations here: [https://google-coral.github.io/py-repo/tflite-runtime/](https://google-coral.github.io/py-repo/tflite-runtime/). Your environment combination might be missing.\n\nTo proceed, follow these steps:\n\n1. Check if any of the available versions work for you at [https://github.com/alexeygrigorev/tflite-aws-lambda/tree/main/tflite](https://github.com/alexeygrigorev/tflite-aws-lambda/tree/main/tflite).\n\n2. Install the needed version using pip. For example:\n\n   ```bash\n   pip install https://github.com/alexeygrigorev/tflite-aws-lambda/raw/main/tflite/tflite_runtime-2.7.0-cp38-cp38-linux_x86_64.whl\n   ```\n\n3. Reference how it's done in the lecture code [here](https://github.com/alexeygrigorev/mlbookcamp-code/blob/master/course-zoomcamp/09-serverless/code/Dockerfile#L4).\n\nAlternatively, you can:\n\n- Use a virtual machine (e.g., VM VirtualBox) with a Linux system.\n- Run the code on a virtual machine within a cloud service such as Vertex AI Workbench on GCP, which provides notebooks and terminals for tasks execution."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-9/018_2fc75f3eed_error-a-module-that-was-compiled-using-numpy-1x-ca.md", "metadata": {"id": "2fc75f3eed", "question": "Error: A module that was compiled using NumPy 1.x cannot be run in NumPy 2.2.0 as it may crash", "sort_order": 18}, "content": "After installing the tflite runtime using the wheel suggested in Homework 9, I encountered a runtime error while testing the lambda handler. The error was:\n\n```\nImportError:\n\nA module that was compiled using NumPy 1.x cannot be run in\nNumPy 2.2.0 as it may crash. To support both 1.x and 2.x\nversions of NumPy, modules must be compiled with NumPy 2.0.\nSome modules may need to be rebuilt instead, e.g., with 'pybind11>=2.12'.\n```\n\nThe issue with the version of NumPy was due to it being overwritten by the installation of tflite-runtime. To prevent this, you should install the wheel using the `--no-deps` option.\n\n```bash\nRUN pip install --no-deps https://github.com/alexeygrigorev/tflite-aws-lambda/raw/main/tflite/tflite_runtime-2.14.0-cp310-cp310-linux_x86_64.whl\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-9/019_fdcf991009_docker-run-error.md", "metadata": {"id": "fdcf991009", "question": "Docker: run error", "sort_order": 19}, "content": "```\ndocker: Error response from daemon: mkdir /var/lib/docker/overlay2/37be849565da96ac3fce34ee9eb2215bd6cd7899a63ebc0ace481fd735c4cb0e-init: read-only file system.\n```\n\nTo resolve this error, restart the Docker services."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-9/020_5d9aed1fd0_docker-save-docker-image-to-local-machine-and-view.md", "metadata": {"id": "5d9aed1fd0", "question": "Docker: Save Docker Image to local machine and view contents", "sort_order": 20}, "content": "The Docker image can be saved/exported to tar format on a local machine using the following command:\n\n```bash\ndocker image save <image-name> -o <name-of-tar-file.tar>\n```\n\nThe individual layers of the Docker image for the filesystem content can be viewed by extracting the `layer.tar` present in the `<name-of-tar-file.tar>` created from above."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-9/021_814a8657e2_running-out-of-space-for-aws-instance.md", "metadata": {"id": "814a8657e2", "question": "Running out of space for AWS instance.", "sort_order": 21}, "content": "Due to experimenting extensively, I've run out of space on my 30-GB AWS instance. Deleting Docker images alone does not free up the space as expected. After removing Docker images, you need to run the following command:\n\n```bash\ndocker system prune\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-9/022_71a682959a_using-tensorflow-215-for-aws-deployment.md", "metadata": {"id": "71a682959a", "question": "Using Tensorflow 2.15 for AWS deployment", "sort_order": 22}, "content": "Using Tensorflow 2.14 with Python 3.11 works fine.\n\nIf it doesn’t work, try using Tensorflow 2.4.4 with supported Python versions like 3.8. Installing Tensorflow 2.4.4 with unsupported versions may cause issues."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-9/023_f067a2c8e6_command-aws-ecr-get-login-no-include-email-returns.md", "metadata": {"id": "f067a2c8e6", "question": "Command aws ecr get-login --no-include-email returns “aws: error: argument operation: Invalid choice…”", "sort_order": 23}, "content": "The error occurs because the `aws ecr get-login` command is deprecated.\n\nInstead, use the following command to authenticate Docker to an ECR registry:\n\n```bash\naws ecr get-login-password --region <your-region> | docker login --username AWS --password-stdin <your-account-id>.dkr.ecr.<your-region>.amazonaws.com\n```\n\nReplace `<your-region>` with your AWS region and `<your-account-id>` with your account ID."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-9/024_ec7b94d408_what-iam-permission-policy-is-needed-to-complete-w.md", "metadata": {"id": "ec7b94d408", "question": "What IAM permission policy is needed to complete Week 9: Serverless?", "sort_order": 24}, "content": "1. **Sign in to the AWS Console**: Log in to the AWS Console.\n\n2. **Navigate to IAM**: Go to the IAM service by clicking on \"Services\" in the top left corner and selecting \"IAM\" under the \"Security, Identity, & Compliance\" section.\n\n3. **Create a new policy**:\n   - In the left navigation pane, select \"Policies\" and click on \"Create policy.\"\n\n4. **Select the service and actions**:\n   - Click on \"JSON\" and copy and paste the JSON policy for the specific ECR actions.\n\n```json\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Sid\": \"VisualEditor0\",\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"ecr:CreateRepository\",\n        \"ecr:GetAuthorizationToken\",\n        \"ecr:BatchCheckLayerAvailability\",\n        \"ecr:BatchGetImage\",\n        \"ecr:InitiateLayerUpload\",\n        \"ecr:UploadLayerPart\",\n        \"ecr:CompleteLayerUpload\",\n        \"ecr:PutImage\"\n      ],\n      \"Resource\": \"*\"\n    }\n  ]\n}\n```\n\n5. **Review and create the policy**:\n   - Click on \"Review policy.\"\n   - Provide a name and description for the policy.\n   - Click on \"Create policy.\"\n\n**Error Resolution**\n\nIf you encounter the following error:\n\n```bash\nERROR: failed to solve: public.ecr.aws/lambda/python:3.10: error getting credentials - err: exec: \"docker-credential-desktop.exe\": executable file not found in $PATH, out: ``\n```\n\n- **Solution**: Delete the file `~/.docker/config.json`"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-9/025_cb7c81f796_docker-temporary-failure-in-name-resolution.md", "metadata": {"id": "cb7c81f796", "question": "Docker: Temporary failure in name resolution", "sort_order": 25}, "content": "Add the following lines to your Docker daemon configuration file using `vim /etc/docker/daemon.json`:\n\n```json\n{\n    \"dns\": [\"8.8.8.8\", \"8.8.4.4\"]\n}\n```\n\nThen, restart Docker using the command:\n\n```bash\nsudo service docker restart\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-9/026_69c624cc4e_keras-model-h5-doesnt-load-error-weight_decay-is-n.md", "metadata": {"id": "69c624cc4e", "question": "Keras model *.h5 doesn’t load. Error: weight_decay is not a valid argument, kwargs should be empty for `optimizer_experimental.Optimizer`", "sort_order": 26}, "content": "Solution: Add `compile=False` to the `load_model` function.\n\n```python\nkeras.models.load_model('model_name.h5', compile=False)\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-9/027_8a23cfbf67_how-to-test-aws-lambda-docker-locally.md", "metadata": {"id": "8a23cfbf67", "question": "How to test AWS Lambda + Docker locally?", "sort_order": 27}, "content": "This deployment setup can be tested locally using [AWS RIE](https://github.com/aws/aws-lambda-runtime-interface-emulator/#test-an-image-with-rie-included-in-the-image) (Runtime Interface Emulator).\n\nIf your Docker image is built upon the base AWS Lambda image (e.g., `FROM public.ecr.aws/lambda/python:3.10`), use specific ports for `docker run` and a specific localhost link for testing:\n\n```bash\ndocker run -it --rm -p 9000:8080 name\n```\n\nThis command runs the image as a container and starts an endpoint locally at:\n\n`localhost:9000/2015-03-31/functions/function/invocations`\n\nPost an event to the following endpoint using a curl command:\n\n```bash\ncurl -XPOST \"http://localhost:9000/2015-03-31/functions/function/invocations\" -d '{}'\n```\n\nExamples of Curl Testing:\n\n- **Windows Testing:**\n  \n  ```bash\n  curl -XPOST \"http://localhost:9000/2015-03-31/functions/function/invocations\" -d \"{\\\"url\\\": \\\"https://habrastorage.org/webt/rt/d9/dh/rtd9dhsmhwrdezeldzoqgijdg8a.jpeg\\\"}\"\n  ```\n\n- **Unix Testing:**\n  \n  ```bash\n  curl -XPOST \"http://localhost:9000/2015-03-31/functions/function/invocations\" -d '{\"url\": \"https://habrastorage.org/webt/rt/d9/dh/rtd9dhsmhwrdezeldzoqgijdg8a.jpeg\"}'\n  ```\n\nIf during testing you encounter an error like this:\n\n```json\n{\n  \"errorMessage\": \"Unable to marshal response: Object of type float32 is not JSON serializable\",\n  \"errorType\": \"Runtime.MarshalError\",\n  \"requestId\": \"7ea5d17a-e0a2-48d5-b747-a16fc530ed10\",\n  \"stackTrace\": []\n}\n```\n\njust convert your response in `lambda_handler()` to a string using `str(result)`."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-9/028_eba3d936f6_unable-to-import-module-lambda_function-no-module.md", "metadata": {"id": "eba3d936f6", "question": "\"Unable to import module 'lambda_function': No module named 'tensorflow'\" when running `python test.py`", "sort_order": 28}, "content": "Ensure that all the code in `test.py` does not have any dependencies on the TensorFlow library. A common cause of this error is that `tflite` is still imported from TensorFlow. Change the import statement:\n\n```python\nimport tensorflow.lite as tflite\n```\n\nTo:\n\n```python\nimport tflite_runtime.interpreter as tflite\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-9/029_c4ad81910b_install-docker-udocker-in-google-colab.md", "metadata": {"id": "c4ad81910b", "question": "Install Docker (udocker) in Google Colab", "sort_order": 29}, "content": "To work with Docker in Google Colab, follow these steps:\n\n1. Open your Google Colab notebook.\n2. Run the following commands:\n\n   ```bash\n   %%shell\n   pip install udocker\n   udocker --allow-root install\n   ```\n\n3. Test the installation:\n\n   ```bash\n   !udocker --allow-root run hello-world\n   ```\n\nFor more details, refer to this [gist](https://gist.github.com/mwufi/6718b30761cd109f9aff04c5144eb885)."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-9/030_18cb2be438_lambda-api-gateway-errors.md", "metadata": {"id": "18cb2be438", "question": "Lambda API Gateway errors:", "sort_order": 30}, "content": "```plaintext\nAuthorization header requires 'Credential' parameter. Authorization header requires 'Signature' parameter. Authorization header requires 'SignedHeaders' parameter. Authorization header requires existence of either a 'X-Amz-Date' or a 'Date' header.\n\nMissing Authentication Token\n```\n\nTo test invoke a method using Boto3, you can use the following Python script:\n\n```python\nimport boto3\n\nclient = boto3.client('apigateway')\n\nresponse = client.test_invoke_method(\n    restApiId='your_rest_api_id',\n    resourceId='your_resource_id',\n    httpMethod='POST',\n    pathWithQueryString='/test/predict', # Adjust path as per API setup\n    body='{\"url\": \"https://habrastorage.org/webt/rt/d9/dh/rtd9dhsmhwrdezeldzoqgijdg8a.jpeg\"}')\n\nprint(response['body'])\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-9/031_ffab512d4f_unable-to-run-pip-install-tflite_runtime-from-gith.md", "metadata": {"id": "ffab512d4f", "question": "Unable to run pip install tflite_runtime from github wheel links?", "sort_order": 31}, "content": "To overcome this issue, you can download the `.whl` file to your local project folder and in the Dockerfile add the following lines:\n\n```dockerfile\nCOPY <file-name> .\n\nRUN pip install <file-name>\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-9/032_8d914fcbe3_python-312-vs-tf-lite-217.md", "metadata": {"id": "8d914fcbe3", "question": "Python 3.12 vs TF Lite 2.17", "sort_order": 32}, "content": "The latest versions of TF Lite don't support Python 3.12 yet. See [update.md](https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/09-serverless/updates.md) for the 2024 cohort in the main repository."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-9/033_ed312ff54d_how-can-i-monitor-and-maintain-models-deployed-on.md", "metadata": {"id": "ed312ff54d", "question": "How can I monitor and maintain models deployed on AWS Lambda?", "sort_order": 33}, "content": "To monitor Lambda deployments, use AWS CloudWatch to access detailed logs, metrics, and alarms. Metrics like invocation count, duration, error rate, and memory usage can help diagnose performance issues. Use AWS X-Ray for tracing requests and analyzing latency.\n\nFor model maintenance:\n\n- Set up an automated CI/CD pipeline to retrain models on updated data.\n- Redeploy using tools like Amazon SageMaker or custom workflows.\n- Regularly evaluate model performance with a monitoring service to detect drift in predictions or data quality issues."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-9/034_69fde52a67_how-to-use-aws-sam-cli-to-create-a-lambda-function.md", "metadata": {"id": "69fde52a67", "question": "How to Use AWS SAM CLI to Create a Lambda Function as a Container Image", "sort_order": 34}, "content": "**Set Up SAM CLI on Your Machine**\n\nFollow the installation guide for the AWS SAM CLI: [AWS SAM CLI Installation Guide](https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/install-sam-cli.html)\n\nAdditional reference: [Getting started with AWS SAM](https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/serverless-getting-started.html)\n\n**Create a New Project**\n\nOpen your command prompt and run the following command to generate boilerplate code:\n\n```bash\nsam init\n```\n\nFollow the SAM CLI Wizard:\n\n1. Select \"AWS Quick Start Templates\".\n2. Choose \"Machine Learning\" as the application type.\n3. Select the version of Python you will use for your runtime.\n4. When prompted for the starter template, choose \"TensorFlow Machine Learning Inference API\".\n\nAfter these steps, a new folder will be created with your selected name. This is your \"SAM project folder\". Inside, you'll find an \"app\" folder.\n\n**Add Required Files for Deployment**\n\nMove all the deployment files (such as the TensorFlow Lite model and your Lambda function) into the \"app\" folder.\n\n**Modify Files Inside the \"app\" Folder**\n\n**requirements.txt**\n\nReplace the TensorFlow dependency with tflite-runtime, and add any other dependencies. Example content:\n\n```\npillow==11.1.0\nrequests==2.32.3\nnumpy==1.26.4\ntflite-runtime==2.7.0\n```\n\n**Dockerfile**\n\nModify the Dockerfile to copy the necessary files for deployment. Example Dockerfile:\n\n```dockerfile\nFROM public.ecr.aws/lambda/python:3.9\n\nCOPY requirements.txt ./\n\nRUN python3.9 -m pip install -r requirements.txt -t .\n\nCOPY app.py ./\nCOPY class_indices.json ./\nCOPY classification_model.tflite ./\n\nENV MODEL_PATH ./classification_model.tflite\nENV CLASSES_PATH ./class_indices.json\n\nCMD [\"app.lambda_handler\"]\n```\n\n**Build the Lambda Function**\n\nFrom the SAM project directory, build the Lambda function:\n\n```bash\nsam build --build-dir .aws-build\n```\n\nAfter building, verify the Docker image by running:\n\n```bash\ndocker images\n```\n\n**Test the Lambda Function Locally**\n\nModify the `app/event/event.json` file to include the expected JSON input:\n\n```json\n{\n  \"url\": \"http://bit.ly/mlbookcamp-pants\"\n}\n```\n\nRun the following command from the SAM project folder:\n\n```bash\nsam local invoke -t .aws-build/template.yaml -e events/event.json\n```\n\nThis command will start a container, send the event, and display the response. The Docker image name used for the container will be shown.\n\n**Deploy the Image**\n\nTo deploy the image, follow classroom instructions or use:\n\n```bash\nsam deploy --guided\n```\n\nAWS SAM will handle creating an ECR repository."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/module-9/035_dc705a3c4e_tflite_runtime-unable-to-install.md", "metadata": {"id": "dc705a3c4e", "question": "Tflite_runtime unable to install", "sort_order": 35}, "content": "When trying to install `tflite_runtime` in a pipenv environment, the following error message appears:\n\n```bash\nERROR: Could not find a version that satisfies the requirement tflite_runtime (from versions: none)\nERROR: No matching distribution found for tflite_runtime\n```\n\n\nThis version of `tflite_runtime` does not run on Python 3.10. To resolve this issue, follow these steps:\n\n1. **Install Python 3.9**: Use Python 3.9 instead of Python 3.10.\n2. **Reinstall `tflite_runtime`**: With Python 3.9, the installation should proceed without issues.\n\n\n- Check all available versions here: [TFLite Runtime Versions](https://google-coral.github.io/py-repo/tflite-runtime/)\n- If no suitable version is found, consider the options provided at [GitHub Repository](https://github.com/alexeygrigorev/tflite-aws-lambda/tree/main/tflite). You can install it using:\n\n  ```bash\n  pip install \"https://github.com/alexeygrigorev/tflite-aws-lambda/raw/main/tflite/tflite_runtime-2.7.0-cp38-cp38-linux_x86_64.whl\"\n  ```\n\n- For local development, use the TFLite included in TensorFlow and Docker for testing Lambda."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/projects/001_04b9f695e8_could-i-partner-up-for-the-mid-and-final-projects.md", "metadata": {"id": "04b9f695e8", "question": "Could I partner up for the mid and final projects?", "sort_order": 1}, "content": "No, you need to do projects individually, but it’s okay to partner up to discuss weekly lectures or exchange ideas."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/projects/001_6e943f57bc_are-projects-solo-or-collaborativegroup-work.md", "metadata": {"id": "6e943f57bc", "question": "Are projects solo or collaborative/group work?", "sort_order": 1}, "content": "All midterms and capstones are meant to be solo projects."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/projects/002_d8f72f3007_what-modules-topics-problem-sets-should-a-midtermc.md", "metadata": {"id": "d8f72f3007", "question": "What modules, topics, problem-sets should a midterm/capstone project cover? Can I do xyz?", "sort_order": 2}, "content": "Ideally, midterms should cover up to module-06, while capstone projects should include all modules in the cohort’s syllabus. However, you can include any other topics you wish to feature. Ensure that you document anything not covered in class.\n\nAlso, consider watching office hours from previous cohorts. Visit the DTC YouTube channel, click on Playlists, and search for {course yyyy}. ML Zoomcamp was first launched in 2021.\n\nMore discussions can be found in relevant Slack channels (links omitted for privacy)."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/projects/003_2c1cb358cb_how-to-conduct-peer-reviews-for-projects.md", "metadata": {"id": "2c1cb358cb", "question": "How to conduct peer reviews for projects?", "sort_order": 3}, "content": "Previous cohorts' projects page has instructions (YouTube).\n\n[GitHub Instructions](https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/cohorts/2022/projects.md#midterm-project)\n\nAlexey and his team will compile a Google Sheet with links to submitted projects using our hashed emails, similar to how we check the leaderboard for homework. These will be our projects to review within the evaluation deadline."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/projects/004_6834d136c1_learning-in-public-links-for-the-projects.md", "metadata": {"id": "6834d136c1", "question": "Learning in public links for the projects", "sort_order": 4}, "content": "For the learning in public for this midterm project, it seems the total value is 14. Does this mean that we need to make 14 posts, or the regular seven posts for each module, each one with a value of 2? Or just one with a total value of 14?\n\n- You need to make 14 posts: one for each day and another 2 posts for evaluating other participants' projects."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/projects/005_307139bab1_my-dataset-is-too-large-and-i-cant-load-it-in-gith.md", "metadata": {"id": "307139bab1", "question": "My dataset is too large and I can't load it in GitHub. Does anyone know about a solution?", "sort_order": 5}, "content": "You can use [Git LFS](https://git-lfs.com/) for uploading large files to a GitHub repository."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/projects/006_8ea1e7e31f_what-if-i-submitted-only-two-projects-and-failed-t.md", "metadata": {"id": "8ea1e7e31f", "question": "What do I need to earn the certificate?", "sort_order": 6}, "content": "- You must pass 2 out of the 3 projects to earn the certificate: Midterm + Capstone 1, or Capstone 1 + Capstone 2.\n- Certificates show pass/fail only—no percentage or rank.\n- Eligibility for the certificate also follows the existing two-project policy: If you have submitted two projects and peer-reviewed at least 3 course-mates’ projects for each submission, you will receive the certificate. According to the course coordinator, only two projects are needed to get the course certificate.\n- Are projects solo or collaborative? All midterms and capstones are solo projects.\n- Grading basis: The pass decision is based on the scores across projects; no overall percentage or rank is awarded."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/projects/007_9f4011849d_acceptable-datasets-projects.md", "metadata": {"id": "9f4011849d", "question": "What datasets are acceptable for projects?", "sort_order": 7}, "content": "Choose something non-toy (e.g., at least 100 rows) that lets you demonstrate the full ML pipeline: data preparation → modeling → evaluation → deployment. You may also collect your own data."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/projects/007_i-did-the-first-two-projects-and-skipped-the-last.md", "metadata": {"id": "0291007215", "question": "I did the first two projects and skipped the last one so I wouldn't have two peer reviews in the second capstone, right?", "sort_order": 7}, "content": "Yes. You only need to review peers when you submit your project."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/projects/008_9a458a7035_how-many-models-should-i-train.md", "metadata": {"id": "9a458a7035", "question": "How many models should I train?", "sort_order": 8}, "content": "Regarding Point 4 in the midterm deliverables, which states, \"Train multiple models, tune their performance, and select the best model,\" you might wonder, how many models should you train? The answer is simple: train as many as you can. The term \"multiple\" implies having more than one model, so as long as you have more than one, you're on the right track."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/projects/009_1480ed67d4_do-you-pass-a-project-based-on-the-average-of-ever.md", "metadata": {"id": "1480ed67d4", "question": "Do you pass a project based on the average of everyone else’s scores or based on the total score you earn?", "sort_order": 9}, "content": "“It’s based on all the scores to make sure most of you pass.”"}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/projects/010_2a71bf594b_does-your-mid-term-project-need-to-use-a-neural-ne.md", "metadata": {"id": "2a71bf594b", "question": "Does your mid term project need to use a neural network to get maximum number of points?", "sort_order": 10}, "content": "No, even though it’s mentioned in the marking rubric, it's not compulsory. It’s just one of the many possible methods you may use."}
{"source": "DataTalksClub/faq", "filename": "_questions/machine-learning-zoomcamp/projects/011_e06817165e_how-are-projects-run-and-graded.md", "metadata": {"id": "e06817165e", "question": "How are projects run and graded?", "sort_order": 11}, "content": "- Each project spans ~3 weeks: ~2 weeks building, ~1 week for peer review.\n- You must complete three peer reviews to pass.\n- Rubrics focus on practical ML engineering and deployment."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/general/001_fde155ddfb_course-i-forgot-if-i-registered-can-i-still-join-t.md", "metadata": {"id": "fde155ddfb", "question": "I forgot if I registered, can I still join the zoomcamp?", "sort_order": 1}, "content": "You don't need to register, as registration is not mandatory. It is only used for gauging interest and collecting data for analytics. You can start learning and submitting homework without registering even while a cohort is “live”. There is no check against any registered list."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/general/002_7f10dde6f8_is-it-going-to-be-live-when.md", "metadata": {"id": "7f10dde6f8", "question": "Is it going to be live? When?", "sort_order": 2}, "content": "The course videos are pre-recorded, and you can start watching the course right now.\n\nThe zoomcamps are spread out throughout the year. See the article [Guide to Free Online Courses at DataTalks Club](https://datatalks.club/blog/guide-to-free-online-courses-at-datatalks-club.html).\n\nWe will also occasionally have office hours — live sessions where we will answer your questions. The office hours sessions are recorded too.\n\nYou can see the office hours (playlist with year 20xx) as well as the pre-recorded course videos in the Course Channel’s Bookmarks and/or [DTC’s YouTube channel](https://www.youtube.com/@DataTalksClub/playlists)."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/general/003_2d8b16c2a0_course-can-i-still-join-the-course-after-the-start.md", "metadata": {"id": "2d8b16c2a0", "question": "Course - Can I still join the course after the start date?", "sort_order": 3}, "content": "Yes, even if you don't register, you're still eligible to submit the homeworks as long as the form is still open and accepting submissions.\n\nBe aware, however, that there will be deadlines for turning in the final projects. So don't leave everything to the last minute."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/general/004_418e8948e7_course-how-do-i-start.md", "metadata": {"id": "418e8948e7", "images": [{"description": "image #1", "id": "image_1", "path": "images/mlops-zoomcamp/image_44a55cf5.png"}, {"description": "image #2", "id": "image_2", "path": "images/mlops-zoomcamp/image_fbebe47e.png"}, {"description": "image #3", "id": "image_3", "path": "images/mlops-zoomcamp/image_81e4432c.png"}], "question": "Course: How do I start?", "sort_order": 4}, "content": "No matter if you're with a 'live' cohort or following in self-paced mode, start by:\n\n- Reading pins and bookmarks on the course channel to see what things are where.\n\n  <{IMAGE:image_1}>\n\n  <{IMAGE:image_2}>\n\n- Reading the repository (bookmarked in channel) and watching the video lessons (playlist bookmarked in channel).\n\n- If you have questions, search the channel itself first; someone may have already asked and gotten a solution.\n\n- For the most Frequently Asked Questions, refer to this document:\n\n  <{IMAGE:image_3}>\n\n- If you don't want to read/skimmer/search the FAQ document, tag the `@ZoomcampQABot` when asking questions, and it will summarize answers from its knowledge base.\n\n- For generic, non-zoomcamp queries, consider using tools like ChatGPT, BingCopilot, or Google Gemini, especially for error messages.\n\n- Check if you're on track by checking the deadlines in the Course Management form for homework submissions.\n\nThe main difference if you're not in a \"live\" cohort is that responses to your questions might be delayed because fewer active students are online. This won't be an issue if you do your own due diligence by searching for answers first and reading the documentation of the library.\n\n- If you need to ask questions and the resources above haven't helped, follow the guidelines in the `asking-questions.md` document (bookmarked in channel) and also check the Pins."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/general/005_46b6399b91_course-can-i-still-graduate-when-i-didnt-complete.md", "metadata": {"id": "46b6399b91", "question": "Course - Can I still graduate when I didn’t complete homework for week x?", "sort_order": 5}, "content": "Yes"}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/general/006_b047f348bc_certificate-can-i-follow-the-course-in-a-self-pace.md", "metadata": {"id": "b047f348bc", "question": "Certificate - Can I follow the course in a self-paced mode and get a certificate?", "sort_order": 6}, "content": "No, you can only get a certificate if you finish the course with a “live” cohort. We don't award certificates for the self-paced mode. The reason is you need to peer-review capstone(s) after submitting a project. You can only peer-review projects at the time the course is running."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/general/007_4deedf3100_whats-the-difference-between-the-2023-and-2022-cou.md", "metadata": {"id": "4deedf3100", "question": "What’s the difference between the 2023 and 2022 course?", "sort_order": 7}, "content": "The difference is the Orchestration and Monitoring modules. Those videos will be re-recorded to use Prefect. The rest should mostly be the same.\n\nAlso, all of the homeworks will be changed for the 2023 cohort."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/general/008_99c8a70fdf_cohort-whats-the-difference-between-the-2024-and-2.md", "metadata": {"id": "99c8a70fdf", "question": "Cohort: What’s the difference between the 2024 and 2023 course?", "sort_order": 8}, "content": "The difference is the Orchestration and Monitoring modules. Those videos will be re-recorded to use Mage-AI. The rest should mostly be the same.\n\nAdditionally, all of the homeworks will be changed for the 2024 cohort."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/general/009_4c40b4805c_cohort-will-there-be-a-2024-cohort-when-will-the-2.md", "metadata": {"id": "4c40b4805c", "question": "Cohort: Will there be a 2024 Cohort? When will the 2024 cohort start?", "sort_order": 9}, "content": "Yes, it will start in May 2024."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/general/010_50ebc48eea_cohort-i-missed-the-current-cohort-when-is-the-nex.md", "metadata": {"id": "50ebc48eea", "question": "Cohort: I missed the current cohort, when is the next cohort scheduled for? Will there be a 202x cohort?", "sort_order": 10}, "content": "Please see the summary of all zoomcamps and their respective schedule at [this link](https://datatalks.club/blog/guide-to-free-online-courses-at-datatalks-club.html).\n\nNote that there's no guarantee the zoomcamps will be run indefinitely or that the same zoomcamps will be conducted every year."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/general/011_edf5668d39_homework-what-if-my-answer-is-not-exactly-the-same.md", "metadata": {"id": "edf5668d39", "question": "Homework: What if my answer is not exactly the same as the choices presented?", "sort_order": 11}, "content": "Please choose the closest one to your answer. Also, do not post your answer in the course Slack channel."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/general/012_90b9e103c7_homework-where-can-i-find-the-schedule-andor-deadl.md", "metadata": {"id": "90b9e103c7", "question": "Homework: where can I find the schedule and/or deadlines of each homework assignment?", "sort_order": 12}, "content": "You can find the deadlines for each homework assignment in the course schedule or timeline provided at [https://courses.datatalks.club/mlops-zoomcamp-2024/](https://courses.datatalks.club/mlops-zoomcamp-2024/). The time is your own local time, as it has been automatically converted."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/general/013_df10d8e0e7_homework-is-the-due-date-for-homework-20th-may-how.md", "metadata": {"id": "df10d8e0e7", "question": "Homework: Is the due date for homework 20th May? How do I check the updated playlist and homework for the mlops course?", "sort_order": 13}, "content": "The due date differs due to participants being in different time zones. It was the midnight of May 19th/20th in Berlin, and whatever corresponded to that in your particular time zone. You can find the deadline on the homework submission page:\n\n[Homework Submission Page](https://courses.datatalks.club/mlops-zoomcamp-2024/homework/hw1)\n\nYou can find all cohort-specific information for the 2025 cohort here:\n\n[2025 Cohort Info](https://github.com/DataTalksClub/mlops-zoomcamp/tree/main/cohorts/2025)"}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/general/014_b1a18890f2_homework-why-is-the-experiment-in-question-6-takin.md", "metadata": {"id": "b1a18890f2", "question": "Homework: Why is the experiment in question 6 taking so long to run? Should we use yellow taxi data, or green taxi data?", "sort_order": 14}, "content": "You might need to use the green taxi data rather than yellow taxi data. The preprocessing code provided with the homework expects green taxi data (even though the question specifies yellow taxi data). Using green taxi data seems to be the correct approach, based on similar questions. Note that there is no official confirmation of this yet (as of early morning, May 27th in Berlin)."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/general/015_1497aabe44_homework-i-am-getting-conflicts-on-server-ports-an.md", "metadata": {"id": "1497aabe44", "question": "Homework: I am getting conflicts on server ports and cannot establish a connection to the MLflow server, why?", "sort_order": 15}, "content": "Your port (5000) may be in use by some other process. To resolve this:\n\n1. Run the following command to find out which process is using the port:\n\n   ```bash\n   lsof -i :5000\n   ```\n2. Either kill the process using that port or route to a different port. You can explicitly change the port with the following command:\n\n   ```bash\n   mlflow ui --backend-store-uri sqlite:///mlflow.db --port 5001\n   ```"}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/general/016_b7bd817c2a_homework-3-2025-cohort-do-i-have-to-use-mage-as-th.md", "metadata": {"id": "b7bd817c2a", "question": "Homework 3, 2025 Cohort - Do I have to use MAGE as the orchestrator? Can I use any orchestrator I want?", "sort_order": 16}, "content": "You do not have to use MAGE or any specific orchestrator; it is totally up to you."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/general/017_c842475338_homework-just-found-this-course-can-i-still-submit.md", "metadata": {"id": "c842475338", "question": "Homework: Just found this course, can I still submit homeworks?", "sort_order": 17}, "content": "To clarify on **late homework submissions**:\n\n- You cannot submit after the homework is scored, as the form is closed.\n- Once the form is closed (i.e., scored), no further submissions are possible.\n- You can check your code against the solution by reviewing the `homework.md` file.\n\nIf the due date has passed but the form is still \"Open/Submittable\":\n\n- This is considered a \"late homework submission,\" and the form is still editable.\n- Don’t forget to click the Update button to save any changes.\n\nPlease note, it's uncertain when the form will be closed as this process is currently manual."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/general/018_dfb6836d0b_hi-is-it-too-late-to-start-the-course-i-have-ml-ex.md", "metadata": {"id": "dfb6836d0b", "question": "Hi, is it too late to start the course, I have ML experience?", "sort_order": 18}, "content": "It really depends on how much time and effort you can dedicate to the project over the coming weeks. Since you're late for the homeworks and they aren't required for the certificate, it might make sense to focus on the projects. Even if the first attempt is a struggle, it will be the best preparation for a second attempt, should you need or want one."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/general/019_a0de15febf_project-are-we-free-to-choose-our-own-topics-for-t.md", "metadata": {"id": "a0de15febf", "question": "Project: Are we free to choose our own topics for the final project?", "sort_order": 19}, "content": "Please pick up a problem you want to solve yourself. Potential datasets can be found on either Kaggle, Hugging Face, Google, AWS, or the UCI Machine Learning Datasets Repository. More links are documented in [datasets.md](https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/projects/datasets.md). Please also read the [README.md in 07-project folder](https://github.com/DataTalksClub/mlops-zoomcamp/tree/main/07-project)."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/general/020_7ab6b70d2c_project-is-the-capstone-an-individual-or-team-proj.md", "metadata": {"id": "7ab6b70d2c", "question": "Project: Is the capstone an individual or team project?", "sort_order": 20}, "content": "It is an individual project."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/general/021_f6544a339e_project-for-the-final-project-is-it-required-to-be.md", "metadata": {"id": "f6544a339e", "question": "Project: For the final project, is it required to be put on the cloud?", "sort_order": 21}, "content": "You can get a few cloud points by using Kubernetes even if you deploy it only locally. Alternatively, you can use LocalStack to mimic AWS. Be sure you're clear on the [Evaluation Criteria](https://github.com/DataTalksClub/mlops-zoomcamp/tree/main/07-project#evaluation-criteria)."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/general/022_33700d36a3_homework-and-leaderboard-what-is-the-system-for-po.md", "metadata": {"id": "33700d36a3", "question": "Homework and Leaderboard: what is the system for points in the course management platform?", "sort_order": 22}, "content": "After you submit your homework, it will be graded based on the number of questions in that particular homework. You can see how many points you have on the homework page at the top.\n\nIn the leaderboard, you will find the sum of all points you've earned, which include:\n\n- Points for Homeworks\n- Points for FAQs\n- Points for Learning in Public\n\nIf you submit something to the FAQ, you receive one point. For each Learning in Public link, you also get one point. Hover over the \"?\" for some explanations."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/general/023_ae2520ab59_what-exactly-is-a-learning-in-public-post.md", "metadata": {"id": "ae2520ab59", "question": "What exactly is a learning-in-public post?", "sort_order": 23}, "content": "They are content that you create about what you have learned on a specific topic. Some DOs and DON’Ts are explained by Alexey in the following video:\n\n[https://www.loom.com/share/710e3297487b409d94df0e8da1c984ce](https://www.loom.com/share/710e3297487b409d94df0e8da1c984ce)\n\nAnyone caught abusing and gaming the system will be publicly called out and have their points stripped so they don’t appear high on the Leaderboard (as of 18 June 2024)."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/general/024_4c837a4be1_leaderboard-i-am-not-on-the-leaderboard-how-do-i-k.md", "metadata": {"id": "4c837a4be1", "images": [{"description": "image #1", "id": "image_1", "path": "images/mlops-zoomcamp/image_5a22f8fa.png"}], "question": "Leaderboard: I am not on the leaderboard / how do I know which one I am on the leaderboard?", "sort_order": 24}, "content": "When you set up your account, you are automatically assigned a random name such as “Lucid Elbakyan.” Click on the \"Jump to your record on the leaderboard\" link to find your entry.\n\nTo see what your display name is, click on the **Edit Course Profile** button.\n\n<{IMAGE:image_1}>\n\n- The first field is your nickname/displayed name. Change it if you want to use your Slack username, GitHub username, or any other nickname to remain anonymous.\n- Unless you want \"Lucid Elbakyan\" on your certificate, it is mandatory that you change the second field to your official name as per your identification documents (passport, national ID card, driver's license, etc.). This is the name that will appear on your Certificate!"}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/general/025_896989c9f1_error-creating-lambda-function-invalidparameterval.md", "metadata": {"id": "896989c9f1", "question": "Error: creating Lambda Function (...): InvalidParameterValueException: The image manifest, config or layer media type for the source image ... is not supported.", "sort_order": 25}, "content": "This error occurs when the Docker image you are using is a manifest list (multi-platform). AWS Lambda does not support manifest lists—it only accepts single-platform images with a standard image manifest.\n\n**Quick fix:** Build your Docker image using `docker buildx` and specify the platform explicitly.\n\n```bash\ndocker buildx build --platform linux/amd64 -t your-ecr-image:latest -f Dockerfile .\n```\n\nThis ensures the image is compatible with AWS Lambda. Also, make sure that you push your image using the `--platform` option."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/general/026_c841197256_criteria-for-getting-a-certificate.md", "metadata": {"id": "c841197256", "question": "Criteria for getting a certificate?", "sort_order": 26}, "content": "Finish the Capstone project."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/general/027_5daddf3efc_is-completion-of-homework-necessary-for-a-certific.md", "metadata": {"id": "5daddf3efc", "question": "Is completion of Homework necessary for a certificate?", "sort_order": 27}, "content": "No.\n\nCan I submit the final project on the second attempt and still receive the certificate?\n\nYes, absolutely. It's your choice whether to submit one or two times; passing any one attempt is sufficient to earn the certificate."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-1/001_e865d7adf7_can-i-submit-and-update-my-project-attempt-multipl.md", "metadata": {"id": "e865d7adf7", "images": [{"description": "image #1", "id": "image_1", "path": "images/mlops-zoomcamp/image_10c5ac47.png"}], "question": "Can I submit and update my project attempt multiple times before the final deadline?", "sort_order": 1}, "content": "Yes, you can submit and update your project attempts multiple times before the final deadline.\n\n- It is advisable not to wait until the last minute. Submitting even a partially completed project early allows you to make improvements over time.\n- Continue adding improvements as needed until the final date.\n- Simply update the Git commit SHA to reflect changes.\n\n<{IMAGE:image_1}>"}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-1/002_cd806dd7d0_opening-jupyter-in-vscode.md", "metadata": {"id": "cd806dd7d0", "question": "Opening Jupyter in VSCode", "sort_order": 2}, "content": "You can install the Jupyter extension to open notebooks in VSCode."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-1/003_1b29ba4c54_launching-jupyter-notebook-from-codespace-vm.md", "metadata": {"id": "1b29ba4c54", "question": "Launching Jupyter notebook from codespace VM", "sort_order": 3}, "content": "When you are ready and have installed Anaconda, you can launch a Jupyter notebook in a new terminal with the following command:\n\n```bash\njupyter notebook\n```\n\nBe careful not to make any typos. For instance, entering \"jupyter-notebook\" will result in an error:\n\n```bash\nJupyter command `jupyter-notebook` not found.\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-1/004_db8427d6cf_configuring-github-to-work-from-the-remote-vm.md", "metadata": {"id": "db8427d6cf", "question": "Configuring Github to work from the remote VM", "sort_order": 4}, "content": "In case you want to set up a GitHub repository (e.g., for homeworks) from a remote VM, you can follow these helpful tutorials:\n\n- **Setting up GitHub on AWS instance:** [Tutorial](https://gist.github.com/matthewoden/b29353e266c554e04be8ea2058bcc2a0)\n- **Setting up keys on AWS instance:** [GitHub Documentation](https://docs.github.com/en/authentication/connecting-to-github-with-ssh/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent)\n\nOnce you complete these steps, you should be able to push to your repository successfully.\n\n**AWS Instance Note:**\n\nThe selected AWS instance may not be covered under the free tier due to its size or other factors. Here is what the AWS free tier includes:\n\n- Resizable compute capacity in the Cloud.\n- 750 hours per month of Linux, RHEL, or SLES t2.micro or t3.micro* instance, depending on the region.\n- 750 hours per month of Windows t2.micro or t3.micro* instance, depending on the region.\n- 750 hours per month of public IPv4 address regardless of the instance type.\n\n*Instances launch in Unlimited mode and may incur additional charges."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-1/005_47d9cd9aca_opening-jupyter-in-aws.md", "metadata": {"id": "47d9cd9aca", "question": "Opening Jupyter in AWS", "sort_order": 5}, "content": "Faced issue while setting up Jupyter Notebook on AWS. I was unable to access it from my desktop. (I am not using Visual Studio and hence faced problem)\n\n1. Run the following command:\n   \n   ```bash\n   jupyter notebook --generate-config\n   ```\n\n2. Edit the file `/home/ubuntu/.jupyter/jupyter_notebook_config.py` to add the following line:\n\n   ```python\n   NotebookApp.ip = '*'\n   ```"}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-1/006_23ed7bdf10_wsl-instructions.md", "metadata": {"id": "23ed7bdf10", "question": "WSL: instructions", "sort_order": 6}, "content": "If you wish to use WSL on your Windows machine, here are the setup instructions:\n\n1. Install wget:\n   \n   ```bash\n   sudo apt install wget\n   ```\n\n2. Download Anaconda from the [Anaconda download page](https://www.anaconda.com/download#downloads) using the `wget` command:\n   \n   ```bash\n   wget <download-address>\n   ```\n\n3. Turn on Docker Desktop WSL 2:\n   \n   [Follow the instructions here](https://docs.docker.com/desktop/windows/wsl/#turn-on-docker-desktop-wsl-2).\n\n4. Clone the desired GitHub repository:\n   \n   ```bash\n   git clone <github-repository-address>\n   ```\n\n5. Install Jupyter:\n   \n   ```bash\n   pip3 install jupyter\n   ```\n\n6. Consider using Anaconda, which includes tools like PyCharm and Jupyter.\n\n7. Alternatively, download Miniforge for a lightweight, open-source version of conda that supports mamba for improved environment solving speed. The Texas Tech University High Performance Computing Center provides a detailed guide:\n   \n   [Installing Miniforge3 Guide by TTU HPCC](https://www.depts.ttu.edu/hpcc/userguides/application_guides/Miniforge.php)\n\n8. For Windows, install WSL via:\n    \n    ```bash\n    wsl --install\n    ```\n\n9. If Python shows as version 3.10 after installing Anaconda with Python 3.9, execute:\n    \n    ```bash\n    source .bashrc\n    ```\n    \n    If the issue persists, add the following to your PATH:\n    \n    ```bash\n    export PATH=\"<anaconda-install-path>/bin:$PATH\"\n    ```\n\nFor using VSCode with WSL, refer to [VSCode on WSL](https://code.visualstudio.com/docs/remote/wsl)."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-1/007_2383fdd2f6_git-created-repo-without-gitignore.md", "metadata": {"id": "2383fdd2f6", "question": "Git: Created repo without .gitignore", "sort_order": 7}, "content": "If you created a repository without a .gitignore, follow these steps to add one:\n\n1. Open Terminal.\n2. Navigate to the location of your Git repository.\n3. Create a .gitignore file for your repository:\n\n   ```bash\n   touch .gitignore\n   ```\n\n4. Locate the .gitignore file. If you already have it, open it.\n5. Edit the .gitignore file and add the following lines:\n\n   ```plaintext\n   # Python\n   *.pyc\n   __pycache__/\n   *.py[cod]\n   *$\n   ```\n\n6. Save the changes to the .gitignore file.\n7. Commit the changes."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-1/008_bcd4190972_gitignore-how-to.md", "metadata": {"id": "bcd4190972", "question": ".gitignore: how-to", "sort_order": 8}, "content": "If you create a folder `data` and download datasets or raw files to your local repository, you might want to push all your code to a remote repository without including these files or folders. To achieve this, use a `.gitignore` file.\n\nFollow these steps to create a `.gitignore` file:\n\n1. Create an empty `.txt` file using a text editor or command line.\n2. Save as `.gitignore` (ensure you use the dot symbol).\n3. Add rules:\n   - `*.parquet` to ignore all Parquet files.\n   - `data/` to ignore all files in the `data` folder.\n\nFor more patterns, read the [GIT documentation](https://git-scm.com/docs/gitignore).\n\n<>{IMAGE:image_id}"}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-1/009_11a7ea6c2c_aws-suggestions.md", "metadata": {"id": "11a7ea6c2c", "question": "AWS: Suggestions", "sort_order": 9}, "content": "- Ensure when stopping an EC2 instance that it fully stops. Look for the status indicator: green (running), orange (stopping), and red (stopped). Refresh the page to confirm it shows a red circle and status as stopped.\n  \n- Note that stopping an EC2 instance might still incur charges, such as storage costs for uploaded data on an EBS volume.\n  \n- Consider setting up billing alerts to monitor costs. However, specific instructions for setting them up are not provided here."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-1/010_1fa4030823_ibm-cloud-an-alternative-for-aws.md", "metadata": {"id": "1fa4030823", "question": "IBM Cloud an alternative for AWS", "sort_order": 10}, "content": "You can get an invitation code from Coursera and use it in your account to verify it. IBM Cloud offers different features.\n\n[IBM CLOUD - Coursera Free Feature Code 395 Days](https://www.youtube.com/watch?v=h_GdX6KtXjo1)"}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-1/011_9f6380d76b_aws-costs.md", "metadata": {"id": "9f6380d76b", "question": "AWS costs:", "sort_order": 11}, "content": "I am worried about the cost of keeping an AWS instance running during the course.\n\nWith the instance specified during working environment setup, if you remember to Stop Instance once you finish your work for the day, using that strategy, in a day with about 5 hours of work, you will pay around $0.40 USD, which will account for $12 USD per month. This seems to be an affordable amount.\n\nYou must remember that you will have a different public IP address every time you restart your instance, and you will need to edit your SSH Config file. It's worth the time though.\n\nAdditionally, AWS enables you to set up an automatic email alert if a predefined budget is exceeded. \n\n[Here is a tutorial to set this up](https://www.c-sharpcorner.com/article/set-up-an-iam-user-and-the-alert-for-budget-in-aws/).\n\nAlso, you can estimate the cost yourself using the [AWS pricing calculator](https://calculator.aws/#/). At the time of writing (20.05.2023), a t3a.xlarge instance with 2 hr/day usage (which translates to 10 hr/week and should be enough to complete the course) and 30GB EBS monthly cost is 10.14 USD.\n\nHere’s [a link to the estimate](https://calculator.aws/#/estimate?id=b24f75864e0af54cbfeeb6083e1c74c605923c65)."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-1/012_ce2a690d31_is-the-aws-free-tier-enough-for-doing-this-course.md", "metadata": {"id": "ce2a690d31", "images": [{"description": "image #1", "id": "image_1", "path": "images/mlops-zoomcamp/image_8d8009df.png"}], "question": "Is the AWS free tier enough for doing this course?", "sort_order": 12}, "content": "<{IMAGE:image_1}>\n\nFor many parts - yes. Some services like Kinesis are not in the AWS free tier, but you can use them locally with LocalStack."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-1/013_5af20a05fe_aws-ec2-this-site-cant-be-reached.md", "metadata": {"id": "5af20a05fe", "question": "AWS EC2: this site can’t be reached", "sort_order": 13}, "content": "When I click an open IP address in an AWS EC2 instance, I get an error: \"This site can’t be reached.\" What should I do?\n\nThis IP address is not meant to be opened in a browser. It is used to connect to the running EC2 instance via terminal. Use the following command from your local machine or a remote server:\n\n- Assume the IP address is `11.111.11.111`\n- The downloaded key name is `razer.pem` (ensure the key is moved to a hidden folder `.ssh`)\n- Your username is `user_name`\n\n```bash\nssh -i /Users/user_name/.ssh/razer.pem ubuntu@11.111.11.111\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-1/014_e04431f782_unprotected-private-key-file.md", "metadata": {"id": "e04431f782", "question": "Unprotected private key file!", "sort_order": 14}, "content": "After running the command:\n\n```bash\nssh -i ~/.ssh/razer.pem ubuntu@XX.XX.XX.XX\n```\n\nI encountered the error: \"unprotected private key file\". To resolve this issue, ensure the file permissions are correctly set by running the following command:\n\n```bash\nchmod 400 ~/.ssh/razer.pem\n```\n\nFor more detailed steps, see this [guide](https://99robots.com/how-to-fix-permission-error-ssh-amazon-ec2-instance/)."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-1/015_af5f29b64f_aws-ec2-instance-constantly-drops-ssh-connection.md", "metadata": {"id": "af5f29b64f", "question": "AWS EC2 instance constantly drops SSH connection", "sort_order": 15}, "content": "My SSH connection to AWS cannot last more than a few minutes, whether via terminal or VS Code.\n\nMy config:\n\n```bash\nHost mlops-zoomcamp  # ssh connection calling name\nUser ubuntu  # username AWS EC2\nHostName <instance-public-IPv4-addr>  # Public IP, changes when instance is turned off.\nIdentityFile ~/.ssh/name-of-your-private-key-file.pem  # Private SSH key file path\nLocalForward 8888 localhost:8888  # Connecting to internal service\nStrictHostKeyChecking no\n```\n\nThe disconnection occurs whether I SSH via WSL2 or via VS Code, often after running some code like `import mlflow`.\n\nTo reconnect, I need to stop and restart the instance, which assigns a new IPv4 address.\n\nI've checked the steps at AWS's troubleshooting page: [AWS SSH Connection Errors](https://aws.amazon.com/premiumsupport/knowledge-center/ec2-linux-resolve-ssh-connection-errors/)\n\nInbound rule should allow all IPs for SSH.\n\n### Expected Behavior:\n\n- SSH connection should remain active while using the instance.\n- Should be able to reconnect if disconnected.\n\n### Solution:\n\n- **Memory Issue**: Disconnections may occur if the instance runs out of memory. Use EC2's screenshot feature to troubleshoot. If it's an OS out-of-memory issue, consider:\n  - Using a higher compute VM with more RAM.\n  - Adding a swap file, which uses disk as a RAM substitute to prevent OOM errors.\n  - Follow Ubuntu's documentation: [Ubuntu Swap FAQ](https://help.ubuntu.com/community/SwapFaq).\n  - Alternatively, follow AWS documentation: [AWS Swap File](https://aws.amazon.com/premiumsupport/knowledge-center/ec2-memory-swap-file/).\n\n- **Timeout Issue**: If connections drop due to timeouts, add the following to your local `.ssh/config` file to ping every 50 seconds:\n\n  ```bash\n  ServerAliveInterval 50\n  ```"}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-1/016_cf068009f1_aws-ec2-how-do-i-handle-changing-ip-addresses-on-r.md", "metadata": {"id": "cf068009f1", "question": "AWS EC2: How do I handle changing IP addresses on restart?", "sort_order": 16}, "content": "Every time I restart my EC2 instance, I receive a different IP and need to update the config file manually.\n\n**Solution:**\n\nYou can create a script to automatically update the IP address of your EC2 instance. Refer to this [guide](https://github.com/dimzachar/mlops-zoomcamp/blob/master/notes/Week_1/update_ssh_config.md) for detailed steps."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-1/017_fb9846ce91_vs-code-crashes-when-connecting-to-jupyter.md", "metadata": {"id": "fb9846ce91", "question": "VS Code crashes when connecting to Jupyter", "sort_order": 17}, "content": "Make sure to use an instance with enough compute capabilities such as a `t2.xlarge`. You can check the monitoring tab in the EC2 dashboard to monitor your instance."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-1/018_c670950570_my-connection-to-my-gcp-vm-instance-keeps-timing-o.md", "metadata": {"id": "c670950570", "question": "My connection to my GCP VM instance keeps timing out when I try to connect", "sort_order": 18}, "content": "If you switched off the VM instance completely in GCP, the IP address may change when it switches back on. You need to update the `ssh_config` file with the new external IP address. This can be done in VS Code if you have the Remote-SSH extension installed.\n\n1. Open the command palette and type `Remote-SSH: Open SSH Configuration File…`.\n2. Select the appropriate `ssh_config` file.\n3. Edit the `HostName` to the correct IP address."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-1/019_e8dbc40d0c_x-has-526-features-but-expecting-525-features.md", "metadata": {"id": "e8dbc40d0c", "question": "X has 526 features, but expecting 525 features", "sort_order": 19}, "content": "Error:\n\n```\nValueError: X has 526 features, but LinearRegression is expecting 525 features as input.\n```\n\nSolution:\n\nThe `DictVectorizer` creates an initial mapping for the features (columns). When calling the `DictVectorizer` again for the validation dataset, `transform` should be used as it will ignore features that it did not see when `fit_transform` was last called. For example:\n\n```python\nX_train = dv.fit_transform(train_dict)\n\nX_test = dv.transform(test_dict)\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-1/020_1695e67099_missing-dependencies.md", "metadata": {"id": "1695e67099", "images": [{"description": "image #1", "id": "image_1", "path": "images/mlops-zoomcamp/image_7c6ef087.png"}], "question": "Missing dependencies", "sort_order": 20}, "content": "If some dependencies are missing:\n\n<{IMAGE:image_1}>\n\nInstall the following packages:\n\n- `pandas`\n- `matplotlib`\n- `scikit-learn`\n- `fastparquet`\n- `pyarrow`\n- `seaborn`\n\n```bash\npip install -r requirements.txt\n```\n\nI have seen this error when using `pandas.read_parquet()`. The solution is to install `pyarrow` or `fastparquet` by running the following command in the notebook:\n\n```bash\n!pip install pyarrow\n```\n\n**Note:** If you’re using Conda instead of pip, install `fastparquet` rather than `pyarrow`, as it is much easier to install and it’s functionally identical to `pyarrow` for our needs."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-1/021_b6197e7348_squared-option-not-available-in-mean_squared_error.md", "metadata": {"id": "b6197e7348", "question": "squared Option Not Available in mean_squared_error", "sort_order": 21}, "content": "The `mean_squared_error` function in scikit-learn no longer includes the `squared` parameter. To compute the Root Mean Squared Error (RMSE), use the dedicated function `root_mean_squared_error` from `sklearn.metrics` instead."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-1/022_5c4e6ef8ca_no-rmse-value-in-the-options.md", "metadata": {"id": "5c4e6ef8ca", "question": "No RMSE value in the options", "sort_order": 22}, "content": "The evaluation RMSE I get doesn’t figure within the options!\n\nIf you’re evaluating the model on the entire February data, try to filter outliers using the same technique you used on the train data (`0 ≤ duration ≤ 60`) and you’ll get an RMSE which is (approximately) in the options. Also, don’t forget to convert the columns' data types to `str` before using the `DictVectorizer`.\n\nAnother option:\n\n- Along with filtering outliers, additionally filter on null values by replacing them with `-1`. \n- You will get an RMSE which is (almost) the same as in the options.\n- Use the `.round(2)` method to round it to 2 decimal points.\n\n### Warning Deprecation\n\nThe Python interpreter warns of modules that have been deprecated and will be removed in future releases while also suggesting how to update your code. For example:\n\n```bash\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\seaborn\\distributions.py:2619:\nFutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\nwarnings.warn(msg, FutureWarning)\n```\n\nTo suppress the warnings, you can include this code at the beginning of your notebook:\n\n```python\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-1/023_885a92b3ee_how-to-replace-distplot-with-histplot.md", "metadata": {"id": "885a92b3ee", "question": "How to replace distplot with histplot", "sort_order": 23}, "content": "To replace `sns.distplot` with `sns.histplot`, you can use the following syntax:\n\n```python\nsns.distplot(df_train[\"duration\"])\n```\n\nCan be replaced with:\n\n```python\nsns.histplot(\n    df_train[\"duration\"], kde=True,\n    stat=\"density\", kde_kws=dict(cut=3), bins=50,\n    alpha=.4, edgecolor=(1, 1, 1, 0.4),\n)\n```\n\nThis will give you an almost identical result."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-1/024_84805fe2dd_keyerror-pulocationid-or-dolocationid.md", "metadata": {"id": "84805fe2dd", "question": "KeyError: 'PULocationID'  or  'DOLocationID'", "sort_order": 24}, "content": "You need to replace the capital letter \"L\" with a small one \"l\"."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-1/025_36182df492_importerror-unable-to-find-a-usable-engine-tried-u.md", "metadata": {"id": "36182df492", "question": "ImportError: Unable to find a usable engine; tried using: ‘pyarrow’, ‘fastparquet’.", "sort_order": 25}, "content": "To resolve this error, run the following command:\n\n```bash\n!pip install pyarrow\n```\n\nAfter successfully installing, you can delete the command."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-1/026_ac851e936b_reading-large-parquet-files.md", "metadata": {"id": "ac851e936b", "question": "Reading large parquet files", "sort_order": 26}, "content": "When reading large parquet files, you might encounter the following error:\n\n```\nIndexError: index 311297 is out of bounds for axis 0 with size 131743\n```\n\nHere are some possible solutions:\n\n1. **Run as a Python Script:**\n   - Try executing your code as a standalone Python script instead of within Jupyter Notebook.\n\n2. **Use PySpark Library:**\n   - Consider using the PySpark library, which is optimized for handling large data files.\n\n3. **Read Parquet in Chunks:**\n   - You can read parquet files in chunks using the pyarrow library. Reference this [blog post](http://blog.clairvoyantsoft.com/efficient-processing-of-parquet-files-in-chunks-using-pyarrow-b315cc0c62f9) for more details.\n\nUsing these methods may help manage and process large parquet files more efficiently."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-1/027_108295493e_kernel-getting-killed-during-assignment-tasks-on-l.md", "metadata": {"id": "108295493e", "question": "Kernel getting killed during assignment tasks on local", "sort_order": 27}, "content": "If the Jupyter notebook kernel gets killed repeatedly due to out-of-memory issues when converting a Pandas DataFrame to a dictionary or other memory-intensive steps, try using Google Colab as it offers more memory.\n\nHere's how you can proceed:\n\n1. **Upload the datasets** to Google Drive in the folder \"Colab Notebooks.\"\n\n2. **Mount the drive** on Colab:\n   \n   ```python\n   from google.colab import drive\n   drive.mount('/content/drive')\n   ```\n   \n3. **Pull the data** from uploaded tables in Colab:\n\n   ```python\n   df_jan = pq.read_table('/content/drive/My Drive/Colab Notebooks/yellow_tripdata_2023-01.parquet').to_pandas()\n   ```\n\n4. **Complete the assignment** in Colab.\n\n5. **Download the final assignment** to your local machine and copy it into the relevant repository."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-1/028_575e56b819_what-is-the-difference-between-label-and-one-hot-e.md", "metadata": {"id": "575e56b819", "question": "What is the difference between label and one-hot encoding?", "sort_order": 28}, "content": "Two main encoding approaches are generally used to handle categorical data: label encoding and one-hot encoding.\n\n- **Label Encoding**: Assigns each categorical value an integer based on alphabetical order. Suitable for logical categorical data, such as a rating system or classification.\n\n- **One-Hot Encoding**: Creates new variables using 0s and 1s to represent original categorical data. Useful when there is no inherent order or logic to the categories.\n\n### Tools and Implementation\n\n- **Sci-kit Learn**:\n  - Dictionary Vectorizer: Handles categorical data and generates arrays based on unique instances in a DataFrame or other data structures.\n  - `OneHotEncoder` class: Specifically for applying one-hot encoding.\n\n- **Pandas**:\n  - `pd.get_dummies()`: Similar functionality for one-hot encoding.\n\nNote: Sometimes resetting a dataset into objects is necessary to apply one-hot encoding, especially when there is logical structuring in the data that could influence label encoding, which can be limiting for some applications."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-1/029_2d1c6920b6_distplot-takes-too-long.md", "metadata": {"id": "2d1c6920b6", "question": "Distplot takes too long", "sort_order": 29}, "content": "First, remove the outliers (trips with unusual duration) before plotting."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-1/030_886fddb311_rmse-on-test-set-too-high.md", "metadata": {"id": "886fddb311", "question": "RMSE on test set too high", "sort_order": 30}, "content": "### Problem\n\nRMSE on the test set was too high when hot encoding the validation set using a previously fitted `OneHotEncoder(handle_unknown='ignore')` on the training set. In contrast, `DictVectorizer` yielded the correct RMSE.\n\n### Explanation\n\nIn principle, both transformers should behave identically when treating categorical features, especially in scenarios where there are no sequences of strings in each row (as in this week’s homework):\n\n- Features are put into binary columns encoding their presence (1) or absence (0).\n- Unknown categories are imputed as zeros in the hot-encoded matrix.\n\nThis discrepancy indicates that there might be a difference in how `OneHotEncoder` and `DictVectorizer` handle the data after fitting on the training set and applying to the validation set."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-1/031_52d606a66e_ictvectorizera-alexeys-answer.md", "metadata": {"id": "52d606a66e", "question": "ictVectorizerA: Alexey’s answer", "sort_order": 31}, "content": "In summary:\n\n- `pd.get_dummies` or One-Hot Encoding (OHE) can produce results in different orders and handle missing data differently, potentially causing train and validation sets to have different columns.\n- `DictVectorizer` will ignore missing values (during training) and new values (during validation) in datasets.\n\nOther sources:\n\n- [Data Science Stack Exchange](https://datascience.stackexchange.com/questions/9443/when-to-use-one-hot-encoding-vs-labelencoder-vs-dictvectorizor)\n- [Scikit-learn Feature Extraction](https://scikit-learn.org/stable/modules/feature_extraction.html)\n- [Alteryx: Encode Smarter](https://innovation.alteryx.com/encode-smarter/)\n\n<{IMAGE:image_id}>"}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-1/032_e107978d25_why-did-we-not-use-onehotencodersklearn-instead-of.md", "metadata": {"id": "e107978d25", "question": "Why did we not use OneHotEncoder(sklearn) instead of DictVectorizer?", "sort_order": 32}, "content": "There are several reasons for choosing DictVectorizer over OneHotEncoder:\n\n- **Simple One-Step Process**: DictVectorizer provides a straightforward method to encode both categorical and numerical features from dictionaries, outputting directly to a sparse matrix.\n- **Ideal for ML Pipelines**: The direct output in sparse matrix format makes DictVectorizer a good fit for machine learning pipelines without needing additional preprocessing.\n- **Use Cases**: \n  - Use **OneHotEncoder** if you need full control, are working with sklearn pipelines, or need to handle unknown categories safely.\n  - Use **DictVectorizer** when your data is in dictionary format (e.g., JSON or from APIs) and you aim for quick integration into the pipeline."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-1/033_e8bd670973_clipping-outliers.md", "metadata": {"id": "e8bd670973", "images": [{"description": "image #1", "id": "image_1", "path": "images/mlops-zoomcamp/image_2715561d.png"}], "question": "Clipping outliers", "sort_order": 33}, "content": "How to check that we removed the outliers?\n\nUse the pandas function `describe()` which can provide a report of the data distribution along with the statistics to describe the data. For example, after clipping the outliers using a boolean expression, the min and max can be verified using:\n\n```python\n df['duration'].describe()\n```\n\n<{IMAGE:image_1}>"}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-1/034_63fb307920_replacing-nans-for-pickup-location-and-drop-off-lo.md", "metadata": {"id": "63fb307920", "question": "Replacing NaNs for pickup location and drop off location with -1 for One-Hot Encoding", "sort_order": 34}, "content": "`pd.get_dummies` and `DictVectorizer` both create a one-hot encoding on string values. Therefore, you need to convert the values in `PUlocationID` and `DOlocationID` to string.\n\nIf you convert the values in `PUlocationID` and `DOlocationID` from numeric to string, the NaN values get converted to the string \"nan\". With `DictVectorizer`, the RMSE is the same whether you use \"nan\" or \"-1\" as the string representation for the NaN values. Therefore, the representation doesn't have to be \"-1\" specifically; it could also be some other string."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-1/035_663dda52c8_slightly-different-rmse.md", "metadata": {"id": "663dda52c8", "question": "Slightly different RMSE", "sort_order": 35}, "content": "**Problem:** My LinearRegression RMSE is very close to the answer but not exactly the same. Is this normal?\n\n**Answer:** No, LinearRegression is a deterministic model; it should always output the same results when given the same inputs.\n\n### Check the Following:\n\n- Ensure outliers are properly treated in both the train and validation sets.\n- Verify that one-hot encoding is correctly applied by inspecting the shape of the one-hot encoded feature matrix. If it shows 2 features, there may be an issue.\n  - Hint: Convert drop-off and pick-up codes to the proper data format before fitting with `DictVectorizer`."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-1/036_148d5d0ed7_extremely-low-rmse.md", "metadata": {"id": "148d5d0ed7", "question": "Extremely low RMSE", "sort_order": 36}, "content": "**Problem:** I’m facing an extremely low RMSE score (e.g., 4.3451e-6) - what should I do?\n\n**Answer:**\n\n- Recheck your code to see if your model is inadvertently learning the target before making predictions.\n- Ensure that the target variable is not included as a parameter while fitting the model. Including it can result in misleadingly low scores.\n- Verify that `X_train` does not contain any part of your `y_train`. This applies to the validation set as well.\n- Adjust your data handling to avoid data leakage between your features and the target."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-1/037_8f917f3f3e_enabling-auto-completion-in-jupyter-notebook.md", "metadata": {"id": "8f917f3f3e", "question": "Enabling Auto-completion in Jupyter Notebook", "sort_order": 37}, "content": "**Problem:** How to enable auto-completion in Jupyter Notebook? Tab doesn’t work.\n\n**Solution:**\n\nYou can enable auto-completion by running the following command:\n\n```bash\n!pip install --upgrade jedi==0.17.2\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-1/038_downloading-the-data-from-the.md", "metadata": {"id": "1913720953", "question": "Downloading the data from the NY Taxis datasets gives error: 403 Forbidden", "sort_order": 38}, "content": "Problem: While following the steps in the videos, you may encounter a 403 Forbidden error when trying to download files using `wget`.\n\nSolution:\n\n- The issue occurs because the links point to files on cloudfront.net. An example of such a link is:\n  ```\n  https://d37ci6vzurychx.cloudfront.net/trip+data/green_tripdata_2021-01.parquet\n  ```\n\n- Instead of downloading the dataset directly from the link, use the dataset URL in your file.\n\n- Update (27-May-2023):\n  - You can now download the data from the official NYC trip record page: [TLC Trip Record Data](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page).\n  - Go to the page, right-click, and use \"copy link\" to get the URL since the URL provided might change if NYC updates their system.\n\n- Example command:\n  ```bash\n  wget https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2021-01.parquet\n  ```"}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-1/039_d99433c04b_using-pycharm-conda-env-in-remote-development.md", "metadata": {"id": "d99433c04b", "question": "Using PyCharm & Conda env in remote development", "sort_order": 39}, "content": "**Problem:** PyCharm (remote) doesn’t see the conda execution path, preventing the use of a conda environment located on a remote server.\n\n**Solution:**\n\n1. On the remote server's command line, run:\n   \n   ```bash\n   conda activate envname\n   ```\n   \n2. Then, execute:\n   \n   ```bash\n   which python\n   ```\n   \n   This will provide the Python execution path.\n   \n3. Use this path to add a new interpreter in PyCharm:\n   \n   - Add local interpreter.\n   - Select system interpreter.\n   - Enter the path obtained from the previous step."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-1/040_afdcab5249_running-out-of-memory.md", "metadata": {"id": "afdcab5249", "question": "Running out of memory", "sort_order": 40}, "content": "**Problem:** The output of DictVectorizer was consuming too much memory, causing an inability to fit the linear regression model before running out of memory on a 16 GB machine.\n\n**Solution:**\n\n- In the example for DictVectorizer on the scikit-learn [website](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.DictVectorizer.html), the parameter `sparse` is set as `False`. While this helps with viewing results, it greatly increases memory usage.\n- To address this, either set `sparse=True`, or leave it at the default setting, which is also `True`. \n\nBy using `sparse=True`, memory usage will be reduced, allowing for more efficient model fitting."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-1/041_4aa018f478_activating-anaconda-env-in-bashrc.md", "metadata": {"id": "4aa018f478", "question": "Activating Anaconda env in .bashrc", "sort_order": 41}, "content": "**Problem:** Installing Anaconda didn’t modify the `.bashrc` profile. This means the Anaconda environment was not activated after exiting and relaunching the Unix shell.\n\n**Solution:**\n\n- **For Bash:**\n  - Initiate conda again, which will add entries for Anaconda in the `.bashrc` file.\n  \n  ```bash\n  cd YOUR_PATH_ANACONDA/bin \n  ./conda init bash\n  ```\n  \n  - This will automatically edit your `.bashrc`.\n\n- **Reload:**\n  \n  ```bash\n  source ~/.bashrc\n  ```"}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-1/042_3f9286329a_the-feature-size-is-different-for-training-set-and.md", "metadata": {"id": "3f9286329a", "question": "The feature size is different for training set and validation set", "sort_order": 42}, "content": "While working through HW1, you may notice that the feature sizes for the training and validation datasets are different. This issue often arises when using the incorrect method with a dictionary vectorizer.\n\nEnsure you use the `transform` method on the premade dictionary vectorizer instead of `fit_transform`. Since you already have the dictionary vectorizer created, there's no need to execute the fit pipeline on the model."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-1/043_e00d5b3f65_permission-denied-publickey-error-when-you-remove.md", "metadata": {"id": "e00d5b3f65", "question": "Permission denied (publickey) Error (when you remove your public key on the AWS machine)", "sort_order": 43}, "content": "If you encounter a \"Permission denied (publickey)\" error after removing your public key from an AWS machine, follow these steps:\n\n1. Access your machine via Session Manager to recreate your public key. Refer to the guide for more details: [Fix Permission Denied Errors](https://repost.aws/knowledge-center/ec2-linux-fix-permission-denied-errors).\n\n2. To retrieve your old public key, use this command:\n   \n   ```bash\n   ssh-keygen -y -f /path_to_key_pair/my-key-pair.pem\n   ```\n\n   Replace `/path_to_key_pair/my-key-pair.pem` with the actual path to your key pair.\n\n3. For additional instructions on retrieving the public key, consult the AWS documentation: [Retrieving the Public Key](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/describe-keys.html#retrieving-the-public-key)."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-1/044_3b76daefb2_overfitting-absurdly-high-rmse-on-the-validation-d.md", "metadata": {"id": "3b76daefb2", "question": "Overfitting: Absurdly high RMSE on the validation dataset", "sort_order": 44}, "content": "**Problem:** The February dataset has been used as a validation/test dataset and stripped of the outliers in a similar manner to the train dataset (taking only the rows for the duration between 1 and 60, inclusive). The RMSE obtained afterward is in the thousands.\n\n**Solution:**\n\n- Ensure that the sparse matrix result from `DictVectorizer` is not turned into an `ndarray`. After removing that part of the code, a correct result was achieved.\n\n<{IMAGE:image_id}>\n\nIf there are further issues, carefully review each preprocessing step to ensure consistency between training and validation datasets."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-1/045_c29959db3a_cant-import-sklearn.md", "metadata": {"id": "c29959db3a", "question": "Can’t import sklearn", "sort_order": 45}, "content": "If you encounter an error when trying to import sklearn, specifically:\n\n```python\nfrom sklearn.feature_extraction import DictVectorizer\n```\n\nYou can resolve it by installing scikit-learn with the following command:\n\n```bash\n!pip install scikit-learn\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-1/046_1bc7beaf5d_install-docker-in-wsl2-without-installing-docker-d.md", "metadata": {"id": "1bc7beaf5d", "question": "Install docker in WSL2 without installing Docker Desktop", "sort_order": 46}, "content": "If you want to install Docker in WSL2 on Windows without Docker Desktop, follow these steps:\n\n1. **Install Docker**\n\n   You can ignore the warnings during installation.\n   \n   ```bash\n   curl -fsSL https://get.docker.com -o get-docker.sh\n   sudo sh get-docker.sh\n   ```\n   \n2. **Add Your User to the Docker Group**\n   \n   ```bash\n   sudo usermod -aG docker $USER\n   ```\n\n3. **Enable the Docker Service**\n   \n   ```bash\n   sudo systemctl enable docker.service\n   ```\n\n4. **Test the Installation**\n\n   Verify that both Docker and Docker Compose are installed successfully.\n   \n   ```bash\n   docker --version\n   docker compose version\n   docker run hello-world\n   ```\n\n5. **Ensure Docker Starts Automatically**\n   \n   If the service does not start automatically after restarting WSL, update your `.profile` or `.zprofile` file with:\n   \n   ```bash\n   if grep -q \"microsoft\" /proc/version > /dev/null 2>&1; then\n      if service docker status 2>&1 | grep -q \"is not running\"; then\n         wsl.exe --distribution \"${WSL_DISTRO_NAME}\" --user root \\\n         --exec /usr/sbin/service docker start > /dev/null 2>&1\n      fi\n   fi\n   ```"}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-1/047_zero-elements-in-sparse-matrix.md", "metadata": {"id": "8894361030", "question": "Zero elements in sparse matrix (AKA when dictionary vectorizer / categorical X transformation fails)", "sort_order": 47}, "content": "Seeing a message like:\n\n```\n<2855951x515 sparse matrix of type '<class 'numpy.float64'>' with 0 stored elements in Compressed Sparse Row format>\n```\n\nThis issue might occur because your variables, intended for vectorization, were imported as floating point numbers rather than integers. This can lead to nonsensical models. To resolve this, convert your data with the following code (assuming `dg` is your dataframe and `categorical` stores the names of your variables to be vectorized):\n\n```python\ndg[categorical] = dg[categorical].round(0).astype(int).astype(str)\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-1/048_866d3bbbed_using-a-docker-image-as-development-environment-li.md", "metadata": {"id": "866d3bbbed", "question": "Using a docker image as development environment (Linux)", "sort_order": 48}, "content": "If you don’t want to install Anaconda locally and prefer not to use Codespace or a VPS, you can create and run a Docker image locally.\n\nFor this, use the following `Dockerfile`:\n\n```dockerfile\nFROM docker.io/bitnami/minideb:bookworm\n\nRUN install_packages wget ca-certificates vim less silversearcher-ag\n\n# Uncomment the `COPY` and comment the `RUN` line if you have downloaded anaconda manually\n# I did this to save bandwidth when experimenting with the image creation\n\nRUN wget https://repo.anaconda.com/archive/Anaconda3-2022.05-Linux-x86_64.sh && bash Anaconda3-2022.05-Linux-x86_64.sh -b -p /opt/anaconda3\n\n#COPY Anaconda3-2022.05-Linux-x86_64.sh /tmp/Anaconda3-2022.05-Linux-x86_64.sh\n\nRUN bash /tmp/Anaconda3-2022.05-Linux-x86_64.sh -b -p /opt/anaconda3 && \\\n    rm /tmp/Anaconda3-2022.05-Linux-x86_64.sh\n\nENV PATH=\"/opt/anaconda3/bin:$PATH\" \\\n    HOME=\"/app\"\n\nEXPOSE 8888\n\nWORKDIR /app\n\nUSER 1001\n\nENTRYPOINT [ \"jupyter\", \"notebook\", \"--ip\", \"0.0.0.0\" ]\n```\n\nBuild the image using:\n\n```bash\ndocker build -f Dockerfile -t mlops:v0 .\n```\n\nThen you can run it with:\n\n```bash\nmkdir app\nchmod -R 777 app\ndocker run --name jupyter -p 8888:8888 -v ./app:/app mlops:v0\n```\n\nIn the logs, you will see the Jupyter URL needed to access the environment. The files you create will be stored in the `app` directory."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-1/049_fd2cad423e_use-uv-as-a-package-manager.md", "metadata": {"id": "fd2cad423e", "question": "Use uv as a package manager", "sort_order": 49}, "content": "There is an option to run the project without Anaconda while easily managing multiple Python versions on your machine. The new package manager, `uv`, is a fast and efficient one written in Rust. It's recommended for use in Python projects overall. [Install guide](https://docs.astral.sh/uv/getting-started/installation/)\n\n```bash\nuv venv --python 3.9.7 # install python 3.9.7 used in the course\n\nsource .venv/bin/activate # activate the environment\n\npython -V # should be 3.9.7\n\nuv pip install pandas scikit-learn notebook seaborn pyarrow # install required packages\n\njupyter notebook # run jupyter notebook\n```\n\nCleanup is straightforward. Deactivate the environment and delete the folder:\n\n```bash\ndeactivate\n\nrm -rf .venv\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-1/050_f346d2cc80_i-get-typeerror-got-an-unexpected-keyword-argument.md", "metadata": {"id": "f346d2cc80", "question": "I get `TypeError: got an unexpected keyword argument 'squared'` when using `mean_squared_error(..., squared=False)`. Why?", "sort_order": 50}, "content": "The `squared` parameter was added in scikit-learn 0.22. In earlier versions, it is not recognized, which causes the `TypeError`.\n\nTo compute RMSE in older versions:\n\n- Use `np.sqrt(mean_squared_error(...))`.\n\nIn scikit-learn 1.0 and later, you can use:\n\n```python\nfrom sklearn.metrics import root_mean_squared_error as rmse\n\nrmse_value = root_mean_squared_error(y_train, y_pred)\n\nprint('RMSE:', rmse_value)\n```\n\nThis approach is more explicit and convenient."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-1/051_0d2700ae06_visualizing-outliers-in-large-datasets-with-seabor.md", "metadata": {"id": "0d2700ae06", "question": "Visualizing outliers in large datasets with Seaborn: Boxplot vs Histplot", "sort_order": 51}, "content": "`seaborn.boxplot` is generally faster because it uses a smaller set of summary statistics (min, Q1, median, Q3, max) to represent the data, which requires less computational effort, especially for large datasets.\n\n`seaborn.histplot` can be slower, particularly with large datasets, because it needs to bin the data and compute frequency counts for each bin, which involves more processing.\n\nSo, if speed is a concern, especially with large datasets, boxplots are typically faster than histograms."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-1/052_a55ee615db_reading-parquet-files-with-pandas-pyarrow-dependen.md", "metadata": {"id": "a55ee615db", "question": "Reading parquet files with Pandas (pyarrow dependency)", "sort_order": 52}, "content": "### Error\n\nA module that was compiled using NumPy 1.x cannot be run in NumPy 2.2.4 as it may crash.\n\n```\nAttributeError: module 'pyarrow' has no attribute '__version__'\n```\n\n### Solution\n\nDowngrade the version of your numpy:\n\n```bash\npip uninstall numpy -y\n\nconda remove numpy --force\n\nconda clean --all -y\n\nconda install numpy=1.26 -y\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-2/001_92ff36d48e_kernel-died-during-model-training-on-github-codesp.md", "metadata": {"id": "92ff36d48e", "question": "Kernel died during Model Training on Github Codespaces", "sort_order": 1}, "content": "While training the model in Jupyter Notebook on GitHub Codespaces, the Jupyter kernel may die. To resolve this, upgrade the machine type in Codespaces from 8 cores to 14 cores. It is free to upgrade, but be aware that you will use more hours."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-2/002_d4b4db8e70_do-we-absolutely-need-to-save-data-to-disk-can-we.md", "metadata": {"id": "d4b4db8e70", "question": "Do we absolutely need to save data to disk? Can we use it directly from download?", "sort_order": 2}, "content": "Yes, you can use data directly from a URL without saving it to disk. For example, you can use Pandas to read data from a URL:\n\n[Read more in the Pandas documentation](https://pandas.pydata.org/docs/reference/api/pandas.read_parquet.html)"}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-2/003_e75e477ce2_access-denied-at-localhost5000-authorization-issue.md", "metadata": {"id": "e75e477ce2", "images": [{"description": "image #1", "id": "image_1", "path": "images/mlops-zoomcamp/image_cc0f04fa.png"}], "question": "Access Denied at Localhost:5000 - Authorization Issue", "sort_order": 3}, "content": "### Problem\n\nLocalhost:5000 Unavailable // Access to Localhost Denied // You don’t have authorization to view this page (127.0.0.1:5000)\n\n### Solution\n\nIf you are using Chrome, follow these steps:\n\n1. Navigate to `chrome://net-internals/#sockets`.\n2. Press \"Flush Socket Pools\".\n\n<{IMAGE:image_1}>"}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-2/004_186389973c_connection-in-use-127001-5000.md", "metadata": {"id": "186389973c", "images": [{"description": "image #1", "id": "image_1", "path": "images/mlops-zoomcamp/image_ad7daf39.png"}], "question": "Connection in use: ('127.0.0.1', 5000)", "sort_order": 4}, "content": "You have something running on the 5000 port. You need to stop it. Here are some ways to resolve the issue:\n\n- **Using Terminal on Mac**:\n  1. Run the command:\n     ```bash\n     ps -A | grep gunicorn\n     ```\n  2. Identify the process ID (the first number after running the command).\n  3. Kill the process using the ID:\n     ```bash\n     kill 13580\n     ```\n     where `13580` represents the process number.\n\n  <{IMAGE:image_1}>\n\n- **To Kill All Processes Using Port 5000**:\n  ```bash\n  sudo fuser -k 5000/tcp\n  ```\n\n- **Alternative Command to Kill the Running Port**:\n  ```bash\n  kill -9 $(ps -A | grep python | awk '{print $1}')\n  ```\n\n- **Change to a Different Port**:\n  ```bash\n  mlflow ui --backend-store-uri sqlite:///mlflow.db --port 5001\n  ```\n\nFor more information, refer to the [source](https://stackoverflow.com/questions/60531166/how-to-safely-shutdown-mlflow-ui#:~:text=I%20also%20met%20a%20similar,and%20kill%20%5BPID%5D%20manually)."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-2/005_3a2a22f115_could-not-convert-string-to-float-valueerror.md", "metadata": {"id": "3a2a22f115", "question": "Could not convert string to float - ValueError", "sort_order": 5}, "content": "Running `python register_model.py` results in the following error:\n\n```python\nValueError: could not convert string to float: '0 int\\n1   float\\n2     hyperopt_param\\n3       Literal{n_estimators}\\n4       quniform\\n5         Literal{10}\\n6         Literal{50}\\n7         Literal{1}'\n```\n\n**Full Traceback:**\n\n```python\nTraceback (most recent call last):\n\nFile \"/Users/name/Desktop/Programming/DataTalksClub/MLOps-Zoomcamp/2. Experiment tracking and model management/homework/scripts/register_model.py\", line 101, in <module>\n\nrun(args.data_path, args.top_n)\n\nFile \"/Users/name/Desktop/Programming/DataTalksClub/MLOps-Zoomcamp/2. Experiment tracking and model management/homework/scripts/register_model.py\", line 67, in run\n\ntrain_and_log_model(data_path=data_path, params=run.data.params)\n\nFile \"/Users/name/Desktop/Programming/DataTalksClub/MLOps-Zoomcamp/2. Experiment tracking and model management/xfsub/scripts/register_model.py\", line 41, in train_and_log_model\n\nparams = space_eval(SPACE, params)\n\nFile \"/Users/name/miniconda3/envs/mlops-zoomcamp/lib/python3.9/site-packages/hyperopt/fmin.py\", line 618, in space_eval\n\nrval = pyll.rec_eval(space, memo=memo)\n\nFile \"/Users/name/miniconda3/envs/mlops-zoomcamp/lib/python3.9/site-packages/hyperopt/pyll/base.py\", line 902, in rec_eval\n\nrval = scope._impls[node.name](*args, **kwargs)\n\nValueError: could not convert string to float: '0 int\\n1   float\\n2     hyperopt_param\\n3       Literal{n_estimators}\\n4       quniform\\n5         Literal{10}\\n6         Literal{50}\\n7         Literal{1}'\n```\n\n**Solution:**\n\nThere are two plausible errors related to the `hpo.py` file where hyper-parameter tuning is run. The objective function should be structured as follows:\n\n1. Ensure the `with` statement and the `log_params` function are correctly applied to log all runs and parameters:\n\n    ```python\n    def objective(params):\n        with mlflow.start_run():\n            mlflow.log_params(params)\n            rf = RandomForestRegressor(**params)\n            rf.fit(X_train, y_train)\n            y_pred = rf.predict(X_valid)\n            rmse = mean_squared_error(y_valid, y_pred, squared=False)\n            mlflow.log_metric('rmse', rmse)\n    ```\n\n2. Add the `with` statement immediately before the function, just after:\n\n    ```python\n    X_valid, y_valid = load_pickle(os.path.join(data_path, \"valid.pkl\"))\n    ```\n\n3. Log parameters just after defining the `search_space` dictionary:\n\n    ```python\n    search_space = {....}\n    mlflow.log_params(search_space)\n    ```\n\nLogging parameters in groups can lead to issues because `register_model.py` expects to receive parameters individually. Ensure the objective function matches the example above."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-2/006_42e6f685af_experiment-not-visible-in-mlflow-ui.md", "metadata": {"id": "42e6f685af", "images": [{"description": "image #1", "id": "image_1", "path": "images/mlops-zoomcamp/image_c5935be1.png"}, {"description": "image #2", "id": "image_2", "path": "images/mlops-zoomcamp/image_d1cb06e4.png"}, {"description": "image #3", "id": "image_3", "path": "images/mlops-zoomcamp/image_54ddd284.png"}], "question": "Experiment not visible in MLflow UI", "sort_order": 6}, "content": "<{IMAGE:image_1}>\n\n<{IMAGE:image_2}>\n\n<{IMAGE:image_3}>\n\nMake sure you launch the MLflow UI from the same directory as the code that is running the experiments (the same directory that contains the `mlruns` directory and the database that stores the experiments).\n\nOr, navigate to the correct directory when specifying the `tracking_uri`.\n\nFor example:\n\n- If the `mlflow.db` is in a subdirectory called `database`, the tracking URI would be:\n  ```python\n  sqlite:///database/mlflow.db\n  ```\n  \n- If the `mlflow.db` is a directory above your current directory, the tracking URI would be:\n  ```python\n  sqlite:///../mlflow.db\n  ```\n\nAnother alternative is to use an absolute path to `mlflow.db` rather than a relative path.\n\nYou can also launch the UI from the same notebook by executing the following code cell:\n\n```python\nimport subprocess\n\nMLFLOW_TRACKING_URI = \"sqlite:///data/mlflow.db\"\n\nsubprocess.Popen([\"mlflow\", \"ui\", \"--backend-store-uri\", MLFLOW_TRACKING_URI])\n```\n\nThen, use the same `MLFLOW_TRACKING_URI` when initializing MLflow or the client:\n\n```python\nfrom mlflow.tracking import MlflowClient\n\nclient = MlflowClient(tracking_uri=MLFLOW_TRACKING_URI)\n\nmlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-2/007_ac12e26845_metrics-not-visible-in-mlflow-ui.md", "metadata": {"id": "ac12e26845", "question": "Metrics not visible in mlflow UI", "sort_order": 7}, "content": "I encountered the following issue: I was able to run experiments and the different model parameters were visible. However, the metrics, including the “handmade” metric `rmse` in the training script, were not visible (empty field).\n\nI solved my problem by making sure to specify the “key” and “value” explicitly when using `mlflow.log_metric`:\n\n```python\nmlflow.log_metric(key=\"rmse\", value=rmse)\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-2/008_aed2f8ddb9_unable-to-create-new-experiment.md", "metadata": {"id": "aed2f8ddb9", "images": [{"description": "image #1", "id": "image_1", "path": "images/mlops-zoomcamp/image_d1cb06e4.png"}], "question": "Unable to create new Experiment", "sort_order": 8}, "content": "Following the instructions in the video did not work, even though the Jupyter notebook indicates it was successfully created.\n\n<{IMAGE:image_1}>\n\nIt is recommended to set the URI to the listener directly. This discrepancy might be due to differences in the \"mlflow\" package versions between the video and the latest version we are using. The documentation for the latest \"mlflow\" package suggests setting the URI as follows:\n\n```python\nmlflow.set_tracking_uri(uri=\"http://127.0.0.1:5000\")\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-2/009_c47c3317f6_hash-mismatch-error-with-package-installation.md", "metadata": {"id": "c47c3317f6", "question": "Hash Mismatch Error with Package Installation", "sort_order": 9}, "content": "### Problem:\n\nWhen attempting to install MLFlow using `pip install mlflow`, an error occurs related to a hash mismatch for the Numpy package:\n\n```plaintext\nERROR: THESE PACKAGES DO NOT MATCH THE HASHES FROM THE REQUIREMENTS FILE.\n```\n\n### Error Details:\n\nDuring the installation on 27th May 2022, the following occurred while Numpy was being installed:\n\n```\nCollecting numpy\n  Downloading numpy-1.22.4-cp310-cp310-win_amd64.whl (14.7 MB)\n  |██████████████              | 6.3 MB 107 kB/s eta 0:01:19\nERROR: THESE PACKAGES DO NOT MATCH THE HASHES FROM THE REQUIREMENTS FILE.\nIf you have updated the package versions, please update the hashes. Otherwise, examine the package contents carefully; someone may have tampered with them.\n```\n\n- **Expected SHA256**: `3e1ffa4748168e1cc8d3cde93f006fe92b5421396221a02f2274aab6ac83b077`\n- **Got**: `15e691797dba353af05cf51233aefc4c654ea7ff194b3e7435e6eec321807e90`\n\n### Solution:\n\n1. **Install Numpy Separately**:\n   \n   - Try installing Numpy separately using:\n     \n     ```bash\n     pip install numpy\n     ```\n   \n2. **Install MLFlow**:\n\n   - After successfully installing Numpy, proceed with reinstalling MLFlow:\n     \n     ```bash\n     pip install mlflow\n     ```\n\nThis approach resolved the issue in this instance, although the problem may not be consistently reproducible. Be aware that similar hash mismatch errors might occur during package installations."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-2/010_2dab05f2be_how-to-delete-an-experiment-permanently-from-mlflo.md", "metadata": {"id": "2dab05f2be", "question": "How to Delete an Experiment Permanently from MLFlow UI", "sort_order": 10}, "content": "After deleting an experiment from the UI, it may still persist in the database. To delete this experiment permanently, follow these steps:\n\n1. **Install `ipython-sql`**\n   \n   ```bash\n   pip install ipython-sql\n   ```\n\n2. **Load SQL Magic Scripts in Jupyter Notebook**\n\n   ```python\n   %load_ext sql\n   ```\n\n3. **Load the Database**\n   \n   Replace `nameofdatabase.db` with your actual database name:\n\n   ```python\n   %sql sqlite:///nameofdatabase.db\n   ```\n\n4. **Run SQL Script**\n\n   Use SQL commands to delete the experiment permanently. Refer to [this link](https://stackoverflow.com/a/68431980/14151292) for a detailed guide."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-2/011_d1b7664c6c_how-to-update-git-public-repo-without-overwriting.md", "metadata": {"id": "d1b7664c6c", "question": "How to Update Git Public Repo Without Overwriting Changes", "sort_order": 11}, "content": "**Problem:** I cloned the public repo, made edits, committed, and pushed them to my own repo. Now I want to get the recent commits from the public repo without overwriting my own changes to my own repo. Which command(s) should I use?\n\nBelow is the Git configuration:\n\n```\n[core]\n  repositoryformatversion = 0\n  filemode = true\n  bare = false\n  logallrefupdates = true\n  ignorecase = true\n  precomposeunicode = true\n\n[remote \"origin\"]\n  url = git@github.com:my_username/mlops-zoomcamp.git\n  fetch = +refs/heads/*:refs/remotes/origin/*\n\n[branch \"main\"]\n  remote = origin\n  merge = refs/heads/main\n```\n\n**Solution:**\n\n- Fork the original repository from DataClubsTak instead of cloning it directly.\n- On GitHub, navigate to your forked repository.\n- Click “Fetch and Merge” under the “Fetch upstream” menu on the main page of your own repository."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-2/012_8a8e4f34a7_image-size-of-460x93139-pixels-is-too-large-it-mus.md", "metadata": {"id": "8a8e4f34a7", "question": "Image size of 460x93139 pixels is too large. It must be less than 2^16 in each direction.", "sort_order": 12}, "content": "This issue is caused by `mlflow.xgboost.autolog()` in version 1.6.1 of XGBoost. To resolve this:\n\n- Downgrade XGBoost to version 1.6.0 using the following command:\n\n```bash\npip install xgboost==1.6.0\n```\n\n- Alternatively, update your requirements file to specify `xgboost==1.6.0`."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-2/013_690948b58d_mlflowclient-object-has-no-attribute-list_experime.md", "metadata": {"id": "690948b58d", "question": "MlflowClient object has no attribute 'list_experiments'", "sort_order": 13}, "content": "Since version 1.29, the `list_experiments` method was deprecated and then removed in later versions.\n\nYou should use the following code instead:\n\n```python\n# Register the best model\nmodel_uri = f\"runs:/{best_run.info.run_id}/model\"\nmlflow.register_model(model_uri=model_uri, name=\"RandomForestBestModel\")\n```\n\nFor more details, refer to the [Mlflow documentation](https://mlflow.org/docs/1.29.0/python_api/mlflow.client.html#mlflow.client.MlflowClient.list_experiments)."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-2/014_37e1b6b9bc_mlflow-autolog-not-working.md", "metadata": {"id": "37e1b6b9bc", "question": "MLflow Autolog not working", "sort_order": 14}, "content": "Make sure `mlflow.autolog()` (or framework-specific autolog) is written **before** `with mlflow.start_run()`, not after.\n\nAlso, ensure that all dependencies for the autologger are installed, including `matplotlib`. A warning about uninstalled dependencies will be raised."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-2/015_944a62abe6_mlflow-url-1270015000http1270015000-doesnt-open.md", "metadata": {"id": "944a62abe6", "images": [{"description": "image #1", "id": "image_1", "path": "images/mlops-zoomcamp/image_10aa6cd8.png"}], "question": "MLflow URL ([127.0.0.1:5000](http://127.0.0.1:5000)) doesn't open.", "sort_order": 15}, "content": "If you’re running MLflow on a remote VM, you need to forward the port too, like we did in Module 1 for the Jupyter notebook port 8888. Simply connect your server to VS Code, as we did, and add 5000 to the PORT.\n\n<{IMAGE:image_1}>\n\nIf you are running MLflow locally and 127.0.0.1:5000 shows a blank page, navigate to localhost:5000 instead."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-2/016_7032c161df_mlflowxgboost-autolog-model-signature-failure.md", "metadata": {"id": "7032c161df", "images": [{"description": "image #1", "id": "image_1", "path": "images/mlops-zoomcamp/image_168171a5.png"}], "question": "MLflow.xgboost Autolog Model Signature Failure", "sort_order": 16}, "content": "Got the same warning message as Warrie Warrie when using `mlflow.xgboost.autolog()`:\n\n<{IMAGE:image_1}>\n\nIt turned out that this was just a warning message, and upon checking MLflow UI (making sure that no \"tag\" filters were included), the model was actually automatically tracked in MLflow."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-2/017_fb9f6e511b_mlflowexception-unable-to-set-a-deleted-experiment.md", "metadata": {"id": "fb9f6e511b", "question": "MlflowException: Unable to Set a Deleted Experiment", "sort_order": 17}, "content": "```python\nraise MlflowException(\n\nmlflow.exceptions.MlflowException: Cannot set a deleted experiment 'random-forest-hyperopt' as the active experiment. You can restore the experiment, or permanently delete the experiment to create a new one.\n```\n\nTo resolve this issue, consider the following options:\n\n- **Restore or Permanently Delete the Experiment**: Refer to guidance on [Stack Overflow](https://stackoverflow.com/questions/60088889/how-do-you-permanently-delete-an-experiment-in-mlflow) for methods to permanently delete an experiment in MLflow.\n\n- **Command Line Resolution**: If you have deleted the experiment from the MLflow UI, run the following command in the CLI. Make sure to use the correct database filename.\n  \n  ```bash\n  mlflow gc --backend-store-uri sqlite:///backend.db\n  ```\n\n- **Ensure .trash is Empty**: If the above command does not work and your .trash folder is already empty, confirm this by executing:\n\n  ```bash\n  rm -rf mlruns/.trash/*\n  ```\n  \n  Note: Ensure no files remain in `.trash/` that could be interfering with the experiment reset."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-2/018_c34fc43e93_mlflowexception-unable-to-set-a-deleted-experiment.md", "metadata": {"id": "c34fc43e93", "question": "MlflowException: Unable to Set a Deleted Experiment with Postgres backend", "sort_order": 18}, "content": "If you’re using a Postgres backend locally or remotely and don’t want to delete the entire backend, you can run this script to permanently delete an experiment. This script assumes you have a separate `env.py` file to retrieve your environment variables.\n\n```python\nimport os\nimport sys\nimport psycopg2\n\nsys.path.insert(0, os.getcwd())\nfrom env import DB_NAME, DB_PASSWORD, DB_PORT, DB_USER\n\ndef perm_delete_exp():\n    connection = psycopg2.connect(\n        database=DB_NAME,\n        user=DB_USER,\n        password=DB_PASSWORD,\n        host=\"localhost\",\n        port=int(DB_PORT)\n    )\n    with connection.cursor() as cursor:\n        queries = \"\"\"\n        DELETE FROM experiment_tags WHERE experiment_id=ANY(SELECT experiment_id FROM experiments where lifecycle_stage='deleted');\n        DELETE FROM latest_metrics WHERE run_uuid=ANY(SELECT run_uuid FROM runs WHERE experiment_id=ANY(SELECT experiment_id FROM experiments where lifecycle_stage='deleted'));\n        DELETE FROM metrics WHERE run_uuid=ANY(SELECT run_uuid FROM runs WHERE experiment_id=ANY(SELECT experiment_id FROM experiments where lifecycle_stage='deleted'));\n        DELETE FROM tags WHERE run_uuid=ANY(SELECT run_uuid FROM runs WHERE experiment_id=ANY(SELECT experiment_id FROM experiments where lifecycle_stage='deleted'));\n        DELETE FROM params WHERE run_uuid=ANY(SELECT run_uuid FROM runs where experiment_id=ANY(SELECT experiment_id FROM experiments where lifecycle_stage='deleted'));\n        DELETE FROM runs WHERE experiment_id=ANY(SELECT experiment_id FROM experiments where lifecycle_stage='deleted');\n        DELETE FROM datasets WHERE experiment_id=ANY(SELECT experiment_id FROM experiments where lifecycle_stage='deleted');\n        DELETE FROM experiments where lifecycle_stage='deleted';\n        \"\"\"\n        for query in queries.splitlines()[1:-1]:\n            cursor.execute(query.strip())\n    connection.commit()\n    connection.close()\n\nif __name__ == \"__main__\":\n    perm_delete_exp()\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-2/019_160cd7cf05_no-space-left-on-device-oserrorerrno-28.md", "metadata": {"id": "160cd7cf05", "question": "No Space Left on Device - OSError[Errno 28]", "sort_order": 19}, "content": "You do not have enough disk space to install the requirements. Here are some solutions:\n\n- **Increase EBS Volume on AWS:** Follow [this guide](https://n2ws.com/blog/how-to-guides/how-to-increase-the-size-of-an-aws-ebs-cloud-volume-attached-to-a-linux-machine#:~:text=First%2C%20go%20to%20your%20volume,your%20requirements%20necessitate%20this%20step) to increase the base EBS volume.\n\n- **Add an External Disk on AWS:** Add and configure an external disk to your instance, then configure conda installation to happen on this external disk.\n\n- **Add Persistent Disk on GCP:**\n  1. Add another disk to your VM and follow [this guide](https://cloud.google.com/compute/docs/disks/add-persistent-disk) to mount the disk.\n  2. Confirm the mount by running the following command in the bash shell:\n     \n    ```bash\n    df -H\n    ```\n  3. Delete Anaconda and use Miniconda instead. Download Miniconda on the additional disk that you mounted.\n  4. During the Miniconda installation, enter the path to the extra disk instead of the default disk, so that conda is installed on the extra disk.\n\n</ANSWER>"}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-2/020_cdd6cd3f29_homework-parameters-mismatch-in-homework-q3.md", "metadata": {"id": "cdd6cd3f29", "question": "Homework: Parameters Mismatch in Homework Q3", "sort_order": 20}, "content": "I was using an old version of sklearn, which caused a mismatch in the number of parameters. In the latest version, `min_impurity_split` for `RandomForestRegressor` was deprecated. Upgrading to the latest version resolved the issue."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-2/021_31a648c28a_protobuf-error-when-installing-mlflow.md", "metadata": {"id": "31a648c28a", "images": [{"description": "image #1", "id": "image_1", "path": "images/mlops-zoomcamp/image_3cc4ae4e.png"}], "question": "Protobuf error when installing MLflow", "sort_order": 21}, "content": "### Error:\n\nI installed all the libraries from the `requirements.txt` document in a new environment with the following command:\n\n```bash\npip install -r requirements.txt\n```\n\nThen, when I run `mlflow` from my terminal like this:\n\n```bash\nmlflow\n```\n\nI get this error:\n\n<{IMAGE:image_1}>\n\n### Solution:\n\nYou need to downgrade the version of the `protobuf` module to 3.20.x or lower. Initially, it was version 4.21. Use the following command to install the compatible version:\n\n```bash\npip install protobuf==3.20\n```\n\nAfter doing this, I was able to run `mlflow` from my terminal."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-2/022_3fd406c55c_ssh-connection-to-aws-ec2-instance-from-local-mach.md", "metadata": {"id": "3fd406c55c", "images": [{"description": "image #1", "id": "image_1", "path": "images/mlops-zoomcamp/image_e822dac8.png"}], "question": "SSH: Connection to AWS EC2 instance from local machine WSL getting terminated frequently within a minute of inactivity.", "sort_order": 22}, "content": "If the SSH connection from your local machine’s WSL to an AWS EC2 instance is frequently getting terminated after a short period of inactivity, you might see the following message displayed:\n\n<{IMAGE:image_1}>\n\nTo fix this issue, add the following lines to your `config` file in the `.ssh` directory of your WSL environment:\n\n```\nServerAliveInterval 60\nServerAliveCountMax 3\n```\n\nFor example, after adding these lines, your SSH configuration should look somewhat like this:\n\n```\nHost mlops-zoomcamp\n  HostName 45.80.32.7\n  User ubuntu\n  IdentityFile ~/.ssh/siddMLOps.pem\n  StrictHostKeyChecking no\n  ServerAliveInterval 60\n  ServerAliveCountMax 3\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-2/023_setting-up-artifacts-folders.md", "metadata": {"id": "6686406223", "question": "Setting up Artifacts folders", "sort_order": 23}, "content": "Please check your current directory while running the `mlflow ui` command. You need to run `mlflow ui` or `mlflow server` command in the right directory."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-2/024_76d72f3776_setting-up-mlflow-experiment-tracker-on-gcp.md", "metadata": {"id": "76d72f3776", "question": "Setting up MLflow experiment tracker on GCP", "sort_order": 24}, "content": "If you have problems setting up MLflow for experiment tracking on GCP, you can check these two links:\n\n- [MLFlow on GCP](https://kargarisaac.github.io/blog/mlops/data%20engineering/2022/06/15/MLFlow-on-GCP.html)\n- [Machine Learning Workflow Orchestration with ZenML](https://kargarisaac.github.io/blog/mlops/2022/08/26/machine-learning-workflow-orchestration-zenml.html)"}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-2/025_4cff59a281_setuptools-replacing-distutils-mlflow-autolog-warn.md", "metadata": {"id": "4cff59a281", "question": "Setuptools Replacing Distutils - MLflow Autolog Warning", "sort_order": 25}, "content": "Downgrade setuptools:\n\n- Change from version 62.3.2 to 49.1.0"}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-2/026_85d2a669c6_sorting-runs-in-mlflow-ui.md", "metadata": {"id": "85d2a669c6", "images": [{"description": "image #1", "id": "image_1", "path": "images/mlops-zoomcamp/image_2a1a8e53.png"}], "question": "Sorting runs in MLflow UI", "sort_order": 26}, "content": "I can’t sort runs in MLflow\n\nMake sure you are in table view (not list view) in the MLflow UI.\n\n<{IMAGE:image_1}>"}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-2/027_ad15f21a9f_typeerror-send_file-unexpected-keyword-max_age-dur.md", "metadata": {"id": "ad15f21a9f", "question": "TypeError: send_file() unexpected keyword 'max_age' during MLflow UI Launch", "sort_order": 27}, "content": "**Problem:** When running `$ mlflow ui` on a remote server and attempting to open it in a local browser, the following exception occurs, and the MLflow UI page does not load.\n\n**Solution:**\n\n1. Uninstall Flask on your remote server by using:\n   \n   ```bash\n   pip uninstall flask\n   ```\n   \n2. Reinstall Flask with:\n   \n   ```bash\n   pip install Flask\n   ```\n   \n   This issue arises because the base conda environment includes a version of Flask that's less than 1.2. Cloning this environment retains the older version, causing the error. Installing a newer version of Flask resolves the issue."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-2/028_f7cd7d85ab_mlflow-ui-on-windows-filenotfounderror-winerror-2.md", "metadata": {"id": "f7cd7d85ab", "question": "mlflow ui on Windows FileNotFoundError: [WinError 2] The system cannot find the file specified", "sort_order": 28}, "content": "**Problem:** After successfully installing `mlflow` using `pip install mlflow` on a Windows system, running the `mlflow ui` command results in the error:\n\n```plaintext\nFileNotFoundError: [WinError 2] The system cannot find the file specified\n```\n\n**Solution:**\n\nAdd `C:\\Users\\{User_Name}\\AppData\\Roaming\\Python\\Python39\\Scripts` to the `PATH`."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-2/029_fabdc4421c_unsupported-operand-type-error-in-hpopy.md", "metadata": {"id": "fabdc4421c", "question": "Unsupported Operand Type Error in hpo.py", "sort_order": 29}, "content": "Running the command:\n\n```bash\npython hpo.py --data_path=./your-path --max_evals=50\n```\n\nleads to the following error:\n\n```python\nTypeError: unsupported operand type(s) for -: 'str' and 'int'\n```\n\n**Full Traceback:**\n\n```plaintext\nFile \"~/repos/mlops/02-experiment-tracking/homework/hpo.py\", line 73, in <module>\n  run(args.data_path, args.max_evals)\n\nFile \"~/repos/mlops/02-experiment-tracking/homework/hpo.py\", line 47, in run\n  fmin(\n\nFile \"~/Library/Caches/pypoetry/virtualenvs/mlflow-intro-SyTqwt0D-py3.9/lib/python3.9/site-packages/hyperopt/fmin.py\", line 540, in fmin\n  return trials.fmin(\n\nFile \"~/Library/Caches/pypoetry/virtualenvs/mlflow-intro-SyTqwt0D-py3.9/lib/python3.9/site-packages/hyperopt/base.py\", line 671, in fmin\n  return fmin(\n\nFile \"~/Library/Caches/pypoetry/virtualenvs/mlflow-intro-SyTqwt0D-py3.9/lib/python3.9/site-packages/hyperopt/fmin.py\", line 586, in fmin\n  rval.exhaust()\n\nFile \"~/Library/Caches/pypoetry/virtualenvs/mlflow-intro-SyTqwt0D-py3.9/lib/python3.9/site-packages/hyperopt/fmin.py\", line 364, in exhaust\n  self.run(self.max_evals - n_done, block_until_done=self.asynchronous)\n```\n\n**Solution:**\n\nThe `--max_evals` argument in `hpo.py` is not defined with a datatype, leading to it being interpreted as a string. It should be an integer to ensure the script functions correctly. Modify the argument definition as follows:\n\n```python\nparser.add_argument(\n    \"--max_evals\",\n    type=int,\n    default=50,\n    help=\"the number of parameter evaluations for the optimizer to explore.\"\n)\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-2/030_601b847b43_unsupported-scikit-learn-version.md", "metadata": {"id": "601b847b43", "question": "Unsupported Scikit-Learn version", "sort_order": 30}, "content": "Getting the following warning when running `mlflow.sklearn`:\n\n```\n2022/05/28 04:36:36 WARNING mlflow.utils.autologging_utils: You are using an unsupported version of sklearn. If you encounter errors during autologging, try upgrading / downgrading sklearn to a supported version, or try upgrading MLflow.\n```\n\n**Solution:**\n\n- Use scikit-learn version between 0.24.1 and 1.4.2.\n\n**Reference:** [MLflow Documentation](https://www.mlflow.org/docs/latest/python_api/mlflow.sklearn.html)"}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-2/031_ac877b2d3d_mlflow-cli-does-not-return-experiments.md", "metadata": {"id": "ac877b2d3d", "question": "Mlflow CLI does not return experiments", "sort_order": 31}, "content": "### Problem\nCLI commands (`mlflow experiments list`) do not return experiments.\n\n### Solution\nYou need to set the environment variable for the Tracking URI:\n\n```bash\nexport MLFLOW_TRACKING_URI=http://127.0.0.1:5000\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-2/032_03f9c129fc_viewing-mlflow-experiments-using-mlflow-cli.md", "metadata": {"id": "03f9c129fc", "question": "Viewing MLflow Experiments using MLflow CLI", "sort_order": 32}, "content": "Problem:\n\nAfter starting the tracking server, when trying to use the MLflow CLI commands as listed [here](https://mlflow.org/docs/latest/cli.html), most commands can't find the experiments that have been run with the tracking server.\n\nSolution:\n\n- Set the environment variable `MLFLOW_TRACKING_URI` to the URI of the SQLite database:\n\n  ```bash\n  export MLFLOW_TRACKING_URI=sqlite:///{path to sqlite database}\n  ```\n\n- After setting the environment variable, you can view the experiments from the command line using commands like:\n\n  ```bash\n  mlflow experiments search\n  ```\n\n- Note: Commands like `mlflow gc` may still not get the tracking URI and need to be passed explicitly as an argument every time the command is run."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-2/033_0a545e0e4c_viewing-sqlite-data-raw-deleting-experiments-manua.md", "metadata": {"id": "0a545e0e4c", "images": [{"description": "image #1", "id": "image_1", "path": "images/mlops-zoomcamp/image_10815a9d.png"}, {"description": "image #2", "id": "image_2", "path": "images/mlops-zoomcamp/image_e3defff0.png"}], "question": "Viewing SQLite Data Raw & Deleting Experiments Manually", "sort_order": 33}, "content": "All the experiment and other tracking information in MLflow are stored in an SQLite database provided while initiating the `mlflow ui` command. This database can be inspected using PyCharm’s Database tab by selecting the SQLite database type.\n\nOnce the connection is created, the tables can be queried and inspected using standard SQL. The same applies to any SQL-backed database such as PostgreSQL.\n\nThis approach is useful to understand the entity structure of the data being stored within MLflow and is beneficial for systematic archiving of model tracking for extended periods.\n\n<{IMAGE:image_1}>\n\n<{IMAGE:image_2}>"}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-2/034_86a6321b3d_what-does-launching-the-tracking-server-locally-me.md", "metadata": {"id": "86a6321b3d", "question": "What does launching the tracking server locally mean?", "sort_order": 34}, "content": "Launching the tracking server locally means starting an MLflow server on your machine for remote hosting. This setup is useful when multiple colleagues are collaborating and need to connect to the same MLflow server instead of running it individually on their laptops."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-2/035_dcccce53fc_parameter-adding-in-case-of-max_depth-not-recogniz.md", "metadata": {"id": "dcccce53fc", "question": "Parameter adding in case of max_depth not recognized", "sort_order": 35}, "content": "**Problem:** Parameter was not recognized during the model registry.\n\n**Solution:** Parameters should be added prior to the model registry. Use the following method to add parameters:\n\n```python\nmlflow.log_params(params)\n```\n\nThis way, the dictionary can be directly appended to `data.run.params`."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-2/036_22b02585bf_max_depth-is-not-recognize-even-when-i-add-the-mlf.md", "metadata": {"id": "22b02585bf", "question": "Max_depth is not recognize even when I add the mlflow.log_params", "sort_order": 36}, "content": "### Problem:\n\nMax_depth is not recognized even when I add the `mlflow.log_params`.\n\n### Solution:\n\nThe `mlflow.log_params(params)` should be added to the `hpo.py` script. If you run it, it will append the new model to the previous run that doesn’t contain the parameters. You should either:\n\n- Remove the previous experiment\n- Change it"}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-2/037_d71f8cb2d0_attributeerror-tuple-object-has-no-attribute-tb_fr.md", "metadata": {"id": "d71f8cb2d0", "question": "AttributeError: 'tuple' object has no attribute 'tb_frame'", "sort_order": 37}, "content": "**Problem:** About week_2 homework: The `register_model.py` script, when copied into a Jupyter notebook, fails and produces the following error:\n\n```python\nAttributeError: 'tuple' object has no attribute 'tb_frame'\n```\n\n**Solution:** Remove click decorators."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-2/038_d4c21886e6_wandb-api-error.md", "metadata": {"id": "d4c21886e6", "question": "WandB API error", "sort_order": 38}, "content": "**Problem:** When running the `preprocess_data.py` file, you encounter the following error:\n\n```bash\nwandb: ERROR api_key not configured (no-tty). call wandb.login(key=[your_api_key])\n```\n\n**Solution:**\n\n1. Go to your WandB profile and navigate to user settings.\n2. Scroll down to the “Danger Zone” and copy your API key.\n3. Before running `preprocess_data.py`, add and run the following cell in your notebook:\n\n   ```bash\n   %%bash\n   wandb login <YOUR_API_KEY_HERE>\n   ```"}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-2/039_13fecb68a8_warning-mlflowxgboost-failed-to-infer-model-signat.md", "metadata": {"id": "13fecb68a8", "question": "WARNING mlflow.xgboost: Failed to infer model signature: could not sample data to infer model signature: please ensure that autologging is enabled before constructing the dataset.", "sort_order": 39}, "content": "Please make sure you follow the order below, enabling autologging before constructing the dataset. If you still have this issue, check that your data is in a format compatible with XGBoost.\n\n1. **Enable MLflow autologging for XGBoost**\n   \n   ```python\n   mlflow.xgboost.autolog()\n   ```\n\n2. **Construct your dataset**\n\n   ```python\n   X_train, y_train = ...\n   ```\n\n3. **Train your XGBoost model**\n\n   ```python\n   import xgboost as xgb\n   model = xgb.XGBRegressor(...)\n   \n   model.fit(X_train, y_train)\n   ```"}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-2/040_bfbe05dcd9_old-version-of-glibc-when-running-xgboost.md", "metadata": {"id": "bfbe05dcd9", "question": "Old version of glibc when running XGBoost", "sort_order": 40}, "content": "Starting from version 2.1.0, XGBoost distributes its Python package in two variants:\n\n- **manylinux_2_28**: For recent Linux distributions with glibc 2.28 or newer. This variant includes all features, such as GPU algorithms and federated learning.\n- **manylinux2014**: For older Linux distributions with glibc versions older than 2.28. This variant lacks support for GPU algorithms and federated learning.\n\nIf you're installing XGBoost via pip, the package manager automatically selects the appropriate variant based on your system's glibc version. Starting May 31, 2025, the manylinux2014 variant will no longer be distributed.\n\nThis means that systems with glibc versions older than 2.28 will not be able to install future versions of XGBoost via pip unless they upgrade their glibc version or build XGBoost from source. \n\n[This behavior is disabled for conda.](https://github.com/conda-forge/xgboost-feedstock/pull/240#discussion_r2106138856)"}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-2/041_47c771de0d_wget-not-working.md", "metadata": {"id": "47c771de0d", "question": "wget not working", "sort_order": 41}, "content": "**Problem**\n\nUsing the `wget` command to download either data or Python scripts on Windows, I am using the notebook provided by Visual Studio and despite having a Python virtual environment, it did not recognize the `pip` command.\n\n**Solution**\n\n- Use `python -m pip`, this applies to any other command as well, e.g., `python -m wget`."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-2/042_e525587d40_openrun-github-notebookipynb-directly-in-google-co.md", "metadata": {"id": "e525587d40", "question": "Open/run github notebook(.ipynb) directly in Google Colab", "sort_order": 42}, "content": "**Problem:** Open/run GitHub notebook (.ipynb) directly in Google Colab\n\n**Solution:**\n\n- Change the domain from `github.com` to `githubtocolab.com`. The notebook will open in Google Colab.\n- Note: This only works with public repositories.\n\n---\n\nNavigating in Wandb UI became difficult to me, I had to intuit some options until I found the correct one.\n\n**Solution:** Refer to the official documentation."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-2/043_d30594cef3_why-do-we-use-janfebmarch-for-traintestvalidation.md", "metadata": {"id": "d30594cef3", "question": "Why do we use Jan/Feb/March for Train/Test/Validation Purposes?", "sort_order": 43}, "content": "We use this type of split approach instead of a random split to address specific needs in model evaluation, primarily focusing on seasonality and preventing data leakage.\n\n### Solution:\n\n1. **\"Out of Time\" Validations:**\n   - **Check for Seasonality:**\n     - By using specific periods like Jan/Feb/March, we can assess if there are seasonal effects in the data.\n     - Example: If the RMSE for the test period is 5, but the RMSE for validation is 20, this indicates significant seasonality. This might suggest switching to Time Series approaches.\n   \n2. **Prevent Data Leakage:**\n   - When predicting future outcomes, a \"random sample\" train/test split can introduce data leakage, resulting in overfitting and poor model performance in production.\n   - It's crucial not to use future information when predicting the present in a model context.\n\n### Approach:\n- **Train:** January\n- **Test:** February\n- **Validate:** March\n\nThe validation process is essential for reporting model metrics to leadership, regulators, auditors, and for analyzing target drift in the models.\n\n<Problem and approach discussed were provided by an internal source.>"}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-2/044_6b1c62c9cf_warning-mlflowsklearn-failed-to-log-training-datas.md", "metadata": {"id": "6b1c62c9cf", "question": "WARNING: mlflow.sklearn: Failed to log training dataset information to MLflow Tracking.", "sort_order": 44}, "content": "### Problem\n\nWhen using MLflow’s autolog function, you may encounter the following warning:\n\n```\nWARNING mlflow.sklearn: Failed to log training dataset information to MLflow Tracking. Reason: 'numpy.ndarray' object has no attribute 'toarray'\n```\n\nThis occurs because the autolog function is attempting to log your dataset. MLflow expects the dataset to be in a `pd.DataFrame` format. If you’re following course code that provides a `numpy.ndarray`, MLflow fails as the `numpy.ndarray` is already an array.\n\n### Solution\n\nSince we are not processing datasets in this zoomcamp, use the following parameter in the autolog function to prevent logging datasets:\n\n```python\nlog_datasets = False\n```\n\n---\n\n### Problem\n\nError when running the mlflow server on AWS CLI with an S3 bucket and POSTGRES database:\n\n#### Reproducible Command:\n\n```bash\nmlflow server -h 0.0.0.0 -p 5000 --backend-store-uri postgresql://<DB_USERNAME>:<DB_PASSWORD>@<DB_ENDPOINT>:<DB_PORT>/<DB_NAME> --default-artifact-root s3://<BUCKET_NAME>\n```\n\n#### Error Message:\n\n```\nurllib3 v2.0 only supports OpenSSL 1.1.1+, currently\n\nImportError: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'OpenSSL 1.0.2k-fips  26 Jan 2017'. See: [GitHub](https://github.com/urllib3/urllib3/issues/2168)\n```\n\n### Solution\n\nUpgrade `mlflow` to address compatibility issues:\n\n```bash\npip3 install --upgrade mlflow\n```\n\n### Resolution\n\nThis process will downgrade `urllib3` from version 2.0.3 to 1.26.16, ensuring compatibility with mlflow and `ssl` version 1.0.2. You should see the following output after the upgrade:\n\n```bash\nInstalling collected packages: urllib3\nAttempting uninstall: urllib3\nFound existing installation: urllib3 2.0.3\nUninstalling urllib3-2.0.3:\nSuccessfully uninstalled urllib3-2.0.3\nSuccessfully installed urllib3-1.26.16\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-2/045_76e6d9e8ce_importerror-urllib3-v20-only-supports-openssl-111.md", "metadata": {"id": "76e6d9e8ce", "question": "ImportError: urllib3 v2.0 only supports OpenSSL 1.1.1+", "sort_order": 45}, "content": "If you're encountering an error while running S3 buckets, ensure to resolve the dependencies issue by downgrading urllib3 to a compatible version:\n\n```bash\npip3 install \"urllib3<1.27\"\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-2/046_4d08d35199_attributeerror-mlflowclient-object-has-no-attribut.md", "metadata": {"id": "4d08d35199", "question": "AttributeError: 'MlflowClient' object has no attribute 'list_run_infos'", "sort_order": 46}, "content": "Problem: In the scenario 2 notebook, the error \n\n```python\nAttributeError: 'MlflowClient' object has no attribute 'list_run_infos'\n```\n\nis thrown when running:\n\n```python\nrun_id = client.list_run_infos(experiment_id='1')[0].run_id\n```\n\nSolution: Use the following code instead:\n\n```python\nrun_id = client.search_runs(experiment_ids='1')[0].info.run_id\n```\n\nScenario: This solution works for MLflow version 2.12.2 and might work for other recent versions as of May, 2024."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-2/047_460343cb44_when-using-autologging-do-i-need-to-set-a-training.md", "metadata": {"id": "460343cb44", "question": "When using Autologging, do I need to set a training parameter to track it on Mlflow UI?", "sort_order": 47}, "content": "No, in the official documentation it’s mentioned that autologging keeps track of the parameters even when you do not explicitly set them when calling `.fit`.\n\nYou can run the training, only setting the parameters you want, but you can check all the parameters in the MLflow UI."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-2/048_58a9c117d5_hyperopt-is-not-installable-with-conda.md", "metadata": {"id": "58a9c117d5", "question": "Hyperopt is not installable with Conda", "sort_order": 48}, "content": "**Description**\n\nWhen setting up your virtual environment with\n\n```bash\nconda install --file requirements.txt\n```\n\nyou may encounter the following error:\n\n```bash\nPackagesNotFoundError: The following packages are not available from current channels:\n\n- hyperopt\n```\n\n**Solution**\n\n- Your conda installation might be out of date. You can update Conda with:\n\n  ```bash\n  conda update -n base -c defaults conda\n  ```\n\n- If updating does not solve the issue, consider installing the package via the Intel channel, as advised on the [conda page](https://anaconda.org/intel/hyperopt):\n\n  ```bash\n  conda install intel::hyperopt\n  ```"}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-2/049_a0ac7f4f77_error-importing-xgboost-in-python-with-os-mac-libr.md", "metadata": {"id": "a0ac7f4f77", "question": "Error importing xgboost in python with OS mac: library not loaded: @rpath/libomp.dylib", "sort_order": 49}, "content": "To fix this error, run the following command:\n\n```bash\nbrew install libomp\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-2/050_a76d315b86_size-limit-when-uploading-to-github.md", "metadata": {"id": "a76d315b86", "question": "Size limit when uploading to GitHub", "sort_order": 50}, "content": "To manage size limits effectively when uploading to GitHub, add the `mlruns` and `artifacts` directories to your `.gitignore`, like this:\n\n```\n02-experiment-tracking/mlruns\n02-experiment-tracking/runnin-mflow-examples/mlruns\n02-experiment-tracking/homework/mlruns\n02-experiment-tracking/homework/artifacts\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-3/001_c1768edac2_why-does-mlflowclient-no-longer-support-list_exper.md", "metadata": {"id": "c1768edac2", "question": "Why does MlflowClient no longer support list_experiments?", "sort_order": 1}, "content": "Older versions of MLflow used `client.list_experiments()`, but in recent versions, this method was replaced.\n\nUse `client.search_experiments()` instead."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-3/002_250d8d1c81_mage-shortcut-key-to-open-text-editor-is-not-worki.md", "metadata": {"id": "250d8d1c81", "question": "Mage shortcut key to open Text Editor is not working on Windows", "sort_order": 2}, "content": "On Windows, use the shortcut key `CTRL+WIN+.`.\n\nFor MacOS, the shortcut is `CMD+.`."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-3/003_d317ebf313_mage-pipeline-breaks-with-errno-2-no-such-file-or.md", "metadata": {"id": "d317ebf313", "question": "Mage: Pipeline breaks with `[Errno 2] No such file or directory: '/home/src/mage_data/{…} /.variables/{...}/output_1/object.joblib'`", "sort_order": 3}, "content": "1. Export the pipeline as a zip file.\n2. Create a new Mage project.\n3. Import the pipeline zip to the new project.\n\nAdditionally, check the following:\n\n- Review the logs of the upstream block that was expected to generate `object.joblib`. Ensure it completed successfully.\n- Verify that the expected output (often named `output_1`) was created and saved.\n- Check in the Mage UI or directly in the file system (if accessible) to confirm whether the file exists in the `.variables` directory for that upstream block."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-3/004_60da2ce9c6_docker-update-docker-compose-to-initiate-mage.md", "metadata": {"id": "60da2ce9c6", "question": "Docker: Update docker-compose to initiate Mage", "sort_order": 4}, "content": "When running `./scripts/start.sh`, the following error occurs:\n\n```plaintext\nERROR: The Compose file './docker-compose.yml' is invalid because:\n\nUnsupported config option for networks: 'app-network'\n\nUnsupported config option for services: 'magic-platform'\n```\n\nSolution:\n\n1. **Download the latest version of Docker Compose**\n\n   ```bash\n   sudo curl -L \"https://github.com/docker/compose/releases/latest/download/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-compose\n   ```\n\n2. **Apply executable permissions to the binary**\n\n   ```bash\n   sudo chmod +x /usr/local/bin/docker-compose\n   ```"}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-3/005_ba50d9e0bc_mage-in-codespaces-in-a-subfolder-under-mlops-zoom.md", "metadata": {"id": "ba50d9e0bc", "images": [{"description": "image #1", "id": "image_1", "path": "images/mlops-zoomcamp/image_f76f8e53.png"}], "question": "Mage in Codespaces in a subfolder under mlops-zoomcamp repository", "sort_order": 5}, "content": "**Issue 1:** Errors such as:\n\n```bash\n[+] Running 1/1\n\n✘ magic-database Error too many requests: You have reached your pull rate limit. You may increase the limit by authenticating and upgra...                   \n\nError response from daemon: too many requests: You have reached your pull rate limit. You may increase the limit by authenticating and upgrading: [docker.com](https://www.docker.com/increase-rate-limit)\n```\n\n**Issue 2:** Popups with different percentage values indicating space is in single digits.\n\n<{IMAGE:image_1}>\n\n**Solution:** It is not recommended to set up Mage as a subfolder of mlops-zoomcamp. See findings in this thread for more information."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-3/006_74daf4433d_mage-in-codespaces.md", "metadata": {"id": "74daf4433d", "question": "Mage in Codespaces", "sort_order": 6}, "content": "The below errors seem to occur only when using Mage in Codespaces.\n\n### Errors\n\n1. **Error (1)**\n   \n   ```bash\n   Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?\n   ```\n\n2. **Error (2)**\n   \n   ```bash\n   Error response from daemon: invalid volume specification: '/workspaces/mage-mlops:/:rw': invalid mount config for type \"bind\": invalid specification: destination can't be '/'\n   ```\n\n   **Solution for (1) & (2):**\n   \n   - Stay tuned…still testing.\n   - Running `docker info` and `docker –version` works fine. \n   - Executing `docker compose down`, stopping Codespaces, and reconnecting resolved the errors, though it might not be reproducible for everyone.\n\n3. **Error (3)**\n\n   ```bash\n   warning: unable to access '/home/codespace/.gitconfig': Is a directory\n   ```\n\n   **Solution (3):**\n\n   - This is targeted for 3.5.x Deploying with Mage. If not deploying:\n     - Comment line #20 in `docker-compose.yml`.\n     - Place a dummy empty file named `.gitconfig` in your repo’s root folder.\n     - Copy it in the Dockerfile with this line, place it below line #9:\n\n       ```dockerfile\n       COPY .gitconfig /root/.gitconfig\n       ```\n\n   - The reason this happens is that when the file is missing, Docker auto-creates it as a directory instead of a file. Creating a dummy file prevents this."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-3/007_563feb7fd1_mage-updated-in-ui.md", "metadata": {"id": "563feb7fd1", "question": "Mage updated in UI", "sort_order": 7}, "content": "When you see the mage version change in the UI after you’ve started the container, and you want to update, follow these steps. Read the release notes first to see if there’s a fix that affected your work and would benefit from an update.\n\nIf you want to remain in the previous version, it's also fine unless the fixes were specifically for our zoomcamp coursework (check the repository for any new instructions or PRs added).\n\n1. Close the browser page.\n2. In the terminal console, bring down the container:\n   ```bash\n   docker compose down\n   ```\n3. Rebuild the container with the new mage image:\n   ```bash\n   docker compose build --no-cache\n   ```\n4. Verify that you see:\n   ```bash\n   [magic-platform 1/4] FROM docker.io/mageai/mageai:alpha\n   ```\n   This means that the container is being rebuilt with a new version.\n\n5. If the image is not updated, press `ctrl+c` to cancel the process and pull the image manually:\n   ```bash\n   docker pull mageai/mageai:alpha\n   ```\n   Then rebuild.\n\n6. Restart the docker container as before:\n   ```bash\n   ./scripts/start.sh\n   ```\n\nNote: This is the same sequence of steps if you want to switch to the latest tagged image instead of using the alpha image.\n\n**What does alpha and latest mean?**\n\n- **Latest** is the fully released version ready for production use, and it has gone through verification, testing, QA, and whatever else the release cycle entails.\n\n- **Alpha** is the potentially buggy version with fresh new fixes and newly added features; but not yet put through the full beta test (if there’s one), integration testing, and other QA steps. Expect issues to occur."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-3/008_078039f4e3_mage-time-series-bar-chart-not-showing.md", "metadata": {"id": "078039f4e3", "question": "Mage Time Series Bar Chart Not Showing", "sort_order": 8}, "content": "```python\nimport requests\nfrom io import BytesIO\nfrom typing import List\nimport numpy as np\nimport pandas as pd\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\n\n@data_loader\ndef ingest_files(**kwargs) -> pd.DataFrame:\n    dfs: List[pd.DataFrame] = []\n    for year, months in [(2024, (1, 3))]:\n        for i in range(*months):\n            response = requests.get(\n                f'https://github.com/mage-ai/datasets/raw/master/taxi/green/{year}/{i:02d}.parquet'\n            )\n            if response.status_code != 200:\n                raise Exception(response.text)\n            df = pd.read_parquet(BytesIO(response.content))\n            # if time series chart on mage error, add code below\n            df['lpep_pickup_datetime_cleaned'] = df['lpep_pickup_datetime'].astype(np.int64) // 10**9\n            dfs.append(df)\n    return pd.concat(dfs)\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-3/009_1e05eabc5c_mage-data_exporter-block-not-taking-all-outputs-fr.md", "metadata": {"id": "1e05eabc5c", "question": "Mage: data_exporter block not taking all outputs from previous transformer block", "sort_order": 9}, "content": "I encountered this issue while trying to run the `data_export` block that saves the dict vectorizer and the logs of the linear regression model into MLflow. My two distinct outputs were clearly created by the previous transformer block where the linear regression model is trained and the dict vectorizer is fitted to the training dataset.\n\nI received this error while trying to run my export code:\n\n```\nException: Block mlflow_model_registry may be missing upstream dependencies. It expected to have 2 arguments, but only received 1. Confirm that the @data_exporter method declaration has the correct number of arguments.\n```\n\nThe outputs are stored in a list, and this list is the input with the two outputs as the two elements. I had to modify my code in the `data_exporter` function to take only one argument and to define the two variables after that:\n\n```python\nDv = output[0]\n\nLr = output[1]\n```\n\nThis adjustment resolved the issue."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-3/010_e065665622_mage-dashboard-on-unit_3-is-not-showing-charts.md", "metadata": {"id": "e065665622", "question": "Mage Dashboard on unit_3 is not showing charts", "sort_order": 10}, "content": "```plaintext\nError: Cannot cast DatetimeArray to dtype float64\n```\n\n- Have the runs completed successfully? We need to have successfully running Pipelines in order to populate the mage and mlflow databases.\n\n- If all pipelines are successfully completed and you are still getting this error, please provide further information."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-3/011_85c83560c5_creating-helper-functions-in-mage.md", "metadata": {"id": "85c83560c5", "question": "Creating Helper functions in Mage", "sort_order": 11}, "content": "There’s no need to add the utility functions in each sub-project when you watch the videos as there only needs to be one set. Just verify the code is still the same as in Mage’s mlops repository.\n\nAs for the import statements:\n\n```python\nfrom mlops.utils.[...] import [...]\n```\n\nAll refer to the same path in the main mlops \"parent\" project:\n\n```\n/[mage-mlops-repository-name]/mlops/utils/...\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-3/012_cc4481d2fa_video-321-various-issues-with-global-data-products.md", "metadata": {"id": "cc4481d2fa", "question": "Video 3.2.1 - Various issues with Global Data Products", "sort_order": 12}, "content": "Refer to the following documentation for more details:\n\n- [Project Management](https://docs.mage.ai/platform/projects/management)\n- [Project Structure](https://docs.mage.ai/design/abstractions/project-structure)\n- [Global Data Products Overview](https://docs.mage.ai/orchestration/global-data-products/overview)\n\n## Issues and Solutions\n\n### Running the GDP Block Takes Forever\n\n**Exception:**\n\n```\nPipeline run xx for global data product training_set: failed\nAttributeError: 'NoneType' object has no attribute 'to_dict'\n```\n\n### Potential Causes and Solutions:\n\n- **Ensure Project and Pipeline Matching**:\n  \n  Make sure the following configurations are correct:\n  \n  ```python\n  \"project\": \"unit_2_training\",\n  \"repo_path\": \"/home/src/mlops/unit_2_training\",\n  ```\n\n- **Restart Steps**:\n  \n  1. Interrupt and restart the Kernel from the Run menu.\n  2. Bring Docker down and restart it via the script.\n\n- **Recreate Everything (if above steps fail):**\n  \n  1. Remove connections from the `hyperparameter_tuning/sklearn` block in the Tree panel to its upstream blocks.\n     - Click on the connector → Remove Connection.\n  2. Remove the Global Data Product block from the Tree panel.\n     - Right click → Delete Block (ignore dependencies).\n  3. Click on All blocks, select Global Data Products, drag and drop this block to be the first in the pipeline.\n  4. Rename the block to what is used in the video.\n  5. Run the block to test it (Play button or Ctrl+Enter).\n\n### Note\n\nIf helpful, repeat similar steps for the file in path \"unit_3_observability.\" There is an ongoing attempt to replicate this process.\n\n### Error with Creating Global Data Product on Mage\n\n**Error:**\n\n```\nAttributeError: 'NoneType' object has no attribute 'to_dict'\n```\n\n**Solution:**\n\nGlobal product is currently not cross-product. You will need to create the data preparation pipeline in `unit_2_training` and configure it to build."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-3/013_e2806ae236_how-do-you-remove-a-global-data-product.md", "metadata": {"id": "e2806ae236", "question": "How do you remove a global data product?", "sort_order": 13}, "content": "There is no way to remove this through the UI. You need to manually edit the `global_data_products.yaml`, which is stored in your project’s utils function. You can do this through the Text Editor."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-3/014_fa12df76a7_error-typeerror-string-indices-must-be-integers.md", "metadata": {"id": "fa12df76a7", "question": "Error: TypeError: string indices must be integers", "sort_order": 14}, "content": "If you've removed and re-added blocks, especially due to issues with Global Data Products, try the following steps:\n\n- Remove the connections from the `hyperparameter_tuning/sklearn` block in the Tree panel to its upstream blocks.\n- Re-add these connections.\n- Remember to save the pipeline using `Ctrl+S`.\n\n### Video 3.2.8 Error:\n#### Issue: ValueError: not enough values to unpack (expected 3, got 1)\n\nEnsure your code follows this order:\n\n- **data** → [training_set](http://localhost:6789/pipelines/xgboost_training/edit?sideview=tree#)\n- **data_2** → [hyperparameter_tuning/xgboost](http://localhost:6789/pipelines/xgboost_training/edit?sideview=tree#)\n\nIf not, proceed with:\n\n1. Remove the connections for the xgboost.\n2. Reconnect starting with the training set, followed by `hyperparameter_tuning/xgboost`."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-3/015_8231e2605d_mlflow-container-error-cant-locate-revision-identi.md", "metadata": {"id": "8231e2605d", "question": "MLflow container error: Can't locate revision identified by …", "sort_order": 15}, "content": "This means your MLflow container tries to access a db file which was a backend for a different MLflow version than the one you have in the container. Most likely, the MLflow version in the container does not match the MLflow version of the MLflow server you ran in module 2.\n\nThe easiest solution is to check which version you worked with before, and change the docker image accordingly.\n\n1. Open a terminal on your host and activate the conda environment you worked in:\n\n   ```bash\n   conda activate <your-env-name>\n   ```\n\n2. Run the following command to check your MLflow version:\n\n   ```bash\n   mlflow --version\n   ```\n\n3. Edit the `mlflow.dockerfile` line to your version:\n\n   ```Dockerfile\n   RUN pip install mlflow==2.??.??\n   ```\n\n4. Save the file and rebuild the docker service by running:\n\n   ```bash\n   docker-compose build\n   ```\n\n5. Now you can start up the containers again, and your MLflow container should be able to successfully read your mounted DB file."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-3/016_7100f168a2_permission-denied-in-github-codespace.md", "metadata": {"id": "7100f168a2", "question": "Permission denied in GitHub Codespace", "sort_order": 16}, "content": "When you encounter a permission denied error while setting up the server in GitHub Codespaces, refer to this guide:\n\n[https://askubuntu.com/questions/409025/permission-denied-when-running-sh-scripts](https://askubuntu.com/questions/409025/permission-denied-when-running-sh-scripts)"}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-3/017_841966c903_where-is-the-faq-for-prefect-questions.md", "metadata": {"id": "841966c903", "question": "Where is the FAQ for Prefect questions?", "sort_order": 17}, "content": "[Here](https://docs.google.com/document/d/1Nyktf7WoRec5lDUBREXL5zLI1Edbw9_R8e45fDn4KB8/edit?usp=sharing)"}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-3/018_7deae3a8eb_root-additional-property-mlflow-is-not-allowed.md", "metadata": {"id": "7deae3a8eb", "images": [{"description": "image #1", "id": "image_1", "path": "images/mlops-zoomcamp/image_6484e853.png"}], "question": "(root) Additional property mlflow is not allowed", "sort_order": 18}, "content": "This error means you are not writing below server on the Docker Compose file. To solve the issue:\n\n<{IMAGE:image_1}>"}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-3/019_d94e5cc566_q6-logged-model-artifacts-lost-when-mlflow-contain.md", "metadata": {"id": "d94e5cc566", "question": "Q6: Logged model artifacts lost when mlflow container is down or removed", "sort_order": 19}, "content": "By default, the logged model and artifacts are stored in a local folder in the mlflow container but not in `/home/src/mlflow`. Therefore, when the container is restarted (after a compose down or container removal), the artifacts are deleted and you cannot see them in the mlflow UI.\n\nTo prevent this issue, you can include a new volume in the Docker Compose service for mlflow to map a folder on the local machine to the folder `/mlartifacts` in the mlflow container:\n\n- ```bash\n  \"${PWD}/mlartifacts:/mlartifacts/\"\n  ```\n\nThis way, every data logged to the experiment will be available even when the mlflow container is recreated."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-3/020_b88d825b12_q6-mlflow-not-showing-artifacts.md", "metadata": {"id": "b88d825b12", "question": "Q6: mlflow not showing artifacts", "sort_order": 20}, "content": "When using localstore, try to start `mlflow` where `mlflow.db` is present. For example, if `mlflow.db` is in `mlops/mlflow`, navigate to that folder and run `../scripts/start.sh`. This assumes you followed the instructions in the `homework.md` file of week 3 and set up the `mlops` folder."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-3/021_0afee51849_q6-correct-mlflow-tracking-uri.md", "metadata": {"id": "0afee51849", "question": "Q6: Correct mlflow tracking uri", "sort_order": 21}, "content": "For the correct mlflow tracking URI, use:\n\n```python\nmlflow.set_tracking_uri(uri=\"http://mlflow:5000\")\n```\n\nThis assumes you used the suggested Docker file snippet in Homework Question 6."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-3/022_b395e1c5ec_i-get-the-following-error-invalid-mount-config-for.md", "metadata": {"id": "b395e1c5ec", "question": "I get the following error: invalid mount config for type \"bind\": invalid specification: destination can't be '/' when running docker compose up when running mage", "sort_order": 22}, "content": "You should not run `docker compose up` for the mage repo directly. Instead, use:\n\n```bash\nbash ./scripts/start.sh\n```\n\n### Additional Information\n\n- The `start.sh` script handles necessary environment variable settings before executing `docker compose up`.\n- Key environment variables such as `PROJECT_NAME` and `MAGE_CODE_PATH` should be set, potentially in your `.env` file.\n- Note that if you are starting a new mage project, like in a capstone project, you may not have a `start.sh` script or a `scripts` directory, so ensure the environment variables are set correctly.\n\nUpdate by another student from the MLOps Zoomcamp."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-3/023_376ba649d7_attributeerror-module-mlflow-has-no-attribute-set_.md", "metadata": {"id": "376ba649d7", "question": "AttributeError: module 'mlflow' has no attribute 'set_tracking_url'", "sort_order": 23}, "content": "In a mage block, the Python statement `mlflow.set_tracking_uri()` was returning an attribute error. This issue was observed when running Mage in one container and MLflow in another. If you encounter this, there may be something else in your project with the name \"mlflow.\"\n\n1. **Debugging the Import Issue:**\n   - Insert a print statement before the Python statement that produces the attribute error:\n     \n     ```python\n     print(mlflow.__file__)\n     ```\n     \n   - This will show what the `mlflow` module points to. It should return a site-packages location, something like:\n     \n     ```\n     '/usr/local/lib/python3.10/site-packages/mlflow/__init__.py'\n     ```\n     \n   - If not, you may have another file or folder called \"mlflow\" that is confusing the Python import statement.\n\n2. **Checking Backend Store Location:**\n   - Look at the folder name where the `mlflow.db` is being created via this command (either in command line or in the Dockerfile for the MLflow service):\n     \n     ```bash\n     mlflow server --backend-store-uri sqlite:///home/mlflow/mlflow.db --host 0.0.0.0 --port 5000\n     ```\n     \n   - If the folder name for the backend store is \"mlflow,\" Python may be trying to import that instead of the MLflow package you installed. Change the backend store folder name to something else, like `mlflow_data`.\n\n   - Rename the folder in your local drive (since it gets mounted in `docker-compose.yml`).\n   - Update the folder name in the Dockerfile for the MLflow service:\n     - Specify the backend-store-uri in the MLflow server command with the new folder name.\n   - Update the folder name in `docker-compose.yml` (when mounting the folder for the MLflow service), e.g.,\n     \n     ```yaml\n     volumes:\n       - \"${PWD}/mlflow_data:/home/mlflow_data/\"\n     ```\n\n3. **If `import mlflow` Gives a Module Not Found Error:**\n   - Check the `PYTHONPATH` variable in the container:\n     \n     ```bash\n     docker ps  # Copy the Mage container ID\n     docker exec -it <container-ID> /bin/bash\n     echo $PYTHONPATH\n     ```\n     \n   - If you do not see the path to the site-packages directory for your Python version, add it to the `PYTHONPATH` environment variable.\n   \n   - To find out what path to use, execute this from the running container:\n     \n     ```python\n     import sys\n     print(sys.path)\n     ```\n     \n   - Add this to the `PYTHONPATH` in the Dockerfile for the Mage service:\n     \n     ```dockerfile\n     ENV PYTHONPATH=\"${PYTHONPATH}:/usr/local/lib/python3.10/site-packages\"\n     ```"}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-3/024_3e58074bee_prefect-project-init-error-no-such-command-project.md", "metadata": {"id": "3e58074bee", "question": "prefect project init Error: No such command 'project'.", "sort_order": 24}, "content": "The newest version of Prefect does not have the module `project`. To initiate a project, use the command:\n\n```bash\nproject init\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-3/025_b3fe59d431_video-334-training-metrics-rmse-chart-does-not-sho.md", "metadata": {"id": "b3fe59d431", "question": "Video 3.3.4: Training Metrics RMSE chart does not show due to the error: KeyError: ‘rmse_LinearRegression’", "sort_order": 25}, "content": "Solution: Check the difference between xgboost and sklearn pipelines. In the xgboost pipeline, there is a `track_experiment` callback, which is missing in the sklearn pipeline.\n\nPlease add these lines:\n\nYou can refer to them in the similar commit linked here:\n\n[Lines to be added](https://github.com/nilarte/mlops-zoomcamp-mage/commit/16c01dfcc2541a03a49f4744d0b0f0207c06e99d#diff-a97c890cfc31702e4b94f1e9a05558ebb8a57349fc9ccf153e82ae53af1bd53e)"}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-3/026_f74891d445_how-can-i-enable-communication-between-docker-cont.md", "metadata": {"id": "f74891d445", "images": [{"description": "image #1", "id": "image_1", "path": "images/mlops-zoomcamp/image_09b2050c.png"}], "question": "How can I enable communication between Docker containers when invoked from a Kestra task?", "sort_order": 26}, "content": "![Diagram of Docker Container Communication](<{IMAGE:image_1}>)\n\nUse the `docker.Run` plugin in your Kestra task to run containers. This plugin supports advanced Docker options like custom networks.\n\nFor local development, you can use `networkMode: host` to allow containers to access services on your host (e.g., MLflow running on localhost).\n\nExample:\n\n```yaml\nnetworkMode: host\n```\n\nNote:\n- `host` mode is only supported on Linux.\n- For Docker Desktop on Windows/macOS, use `host.docker.internal` or create a shared Docker network.\n\n**Best Practice:**\n\nIn production setups, tools like MLflow should run outside Kestra and be accessed over a stable URI (e.g., a cloud endpoint or a container with a known hostname in a shared network)."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-4/001_e00253e2df_fix-out-of-memory-error-while-orchestrating-the-wo.md", "metadata": {"id": "e00253e2df", "images": [{"description": "image #1", "id": "image_1", "path": "images/mlops-zoomcamp/image_d1d6da8a.png"}, {"description": "image #2", "id": "image_2", "path": "images/mlops-zoomcamp/image_23d027f2.png"}, {"description": "image #3", "id": "image_3", "path": "images/mlops-zoomcamp/image_09a24598.png"}], "question": "Fix Out of Memory error while orchestrating the workflow on a ML Pipeline for a high volume dataset.", "sort_order": 1}, "content": "We come across situations in data transformation & pre-processing as well as model training in a ML pipeline where we need to handle datasets of high dimensionality or high cardinality (usually millions). We often end up with Out of Memory (OOM) errors like below when the flow is running:\n\n<{IMAGE:image_1}>\n\nIf you do not have the option of increasing your RAM, the following approaches can be effective in mitigating this error:\n\n1. **Read Only Required Features/Columns:**\n   - During the data loading step, read only the necessary features/columns from the dataset.\n   \n   <{IMAGE:image_2}>\n\n2. **Remove Unused Dataframes:**\n   - Before encoding/vectorizing, remove the dataframe when you have obtained `X_train` & `y_train`.\n   \n   <{IMAGE:image_3}>\n\n3. **Create or Resize Swap File:**\n   - If you do not have a swap file or have a small one, create a swap file (size as per memory requirement) or replace the existing one with a properly sized one.\n   \n   To remove an existing swapfile, use:\n   \n   ```bash\n   sudo swapoff /swapfile\n   sudo rm /swapfile\n   ```\n   \n   To create a new properly sized swapfile (e.g., 16 GB), use:\n   \n   ```bash\n   sudo fallocate -l 16G /swapfile\n   sudo chmod 600 /swapfile\n   sudo mkswap /swapfile\n   sudo swapon /swapfile\n   ```\n   \n   To check the swap file created:\n   \n   ```bash\n   free -h\n   ```"}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-4/002_74974bf558_docker-awsexe-error-argument-operation-invalid-cho.md", "metadata": {"id": "74974bf558", "question": "Docker: aws.exe: error: argument operation: Invalid choice — Docker can not login to ECR.", "sort_order": 2}, "content": "When using AWS CLI on Windows, you might encounter the following error:\n\n```\naws.exe: error: argument operation: Invalid choice\n```\n\n### Solution\n\n1. Check your AWS CLI version. For example:\n   \n   ```\n   aws-cli/2.4.24 Python/3.8.8 Windows/10 exe/AMD64 prompt/off\n   ```\n\n2. Instead of using the outdated command, use the updated command provided by AWS:\n\n   ```bash\n   aws ecr get-login-password \\\n   --region <region> \\\n   | docker login \\\n   --username AWS \\\n   --password-stdin <aws_account_id>.dkr.ecr.<region>.amazonaws.com\n   ```\n\n3. Refer to the official AWS documentation for additional details: [AWS CLI ECR Login Password](https://docs.aws.amazon.com/cli/latest/reference/ecr/get-login-password.html)\n\nEnsure that you replace `<region>` and `<aws_account_id>` with your specific values."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-4/003_2d292454ac_multiline-commands-in-windows-powershell.md", "metadata": {"id": "2d292454ac", "question": "Multiline commands in Windows Powershell", "sort_order": 3}, "content": "To use multiline commands in Windows PowerShell, place a backtick (`) at the end of each line except the last. Note that multiline strings do not require a backtick.\n\n- Escape double quotes (`\"`) to `\"\\`\n- Use `$env:` to create environment variables (non-persistent). For example:\n\n```powershell\n$env:KINESIS_STREAM_INPUT=\"ride_events\"\n\naws kinesis put-record --cli-binary-format raw-in-base64-out `\n\n--stream-name $env:KINESIS_STREAM_INPUT `\n\n--partition-key 1 `\n\n--data '{\n\n\\\"ride\\\": {\n\n\\\"PULocationID\\\": 130,\n\n\\\"DOLocationID\\\": 205,\n\n\\\"trip_distance\\\": 3.66\n\n},\n\n\\\"ride_id\\\": 156\n\n}'\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-4/004_a2d10b83d9_pipenv-installation-not-working-attributeerror-mod.md", "metadata": {"id": "a2d10b83d9", "question": "Pipenv installation not working (AttributeError: module 'collections' has no attribute 'MutableMapping')", "sort_order": 4}, "content": "If you encounter pipenv failures with the command `pipenv install` and see the following error:\n\n```python\nAttributeError: module 'collections' has no attribute 'MutableMapping'\n```\n\nThe issue occurs because you are using the system Python (3.10) for pipenv.\n\nTo resolve this issue:\n\n1. If pipenv was previously installed via `apt-get`, remove it using:\n   ```bash\n   sudo apt remove pipenv\n   ```\n\n2. Ensure a non-system Python is installed in your environment. An easy way to achieve this is by installing Anaconda or Miniconda.\n\n3. Install pipenv using your non-system Python:\n   ```bash\n   pip install pipenv\n   ```\n\n4. Re-run `pipenv install <dependencies>` with the relevant dependencies. It should work without issues.\n\nThis solution was tested and worked on an AWS instance similar to the configuration presented in class."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-4/005_7fa713d6c7_module-is-not-available-cant-connect-to-https-url.md", "metadata": {"id": "7fa713d6c7", "question": "module is not available (Can't connect to HTTPS URL)", "sort_order": 5}, "content": "First, check if the SSL module is configured with the following command:\n\n```bash\npython -m ssl\n```\n\nIf the output is empty, there is no problem with the SSL configuration. Then you should upgrade your pipenv package in your current environment to resolve the problem."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-4/006_1b858ee8bb_no-module-named-pip_vendorsix.md", "metadata": {"id": "1b858ee8bb", "question": "No module named 'pip._vendor.six'", "sort_order": 6}, "content": "During scikit-learn installation via the command:\n\n```bash\npipenv install scikit-learn==1.0.2\n```\n\nThe following error is raised:\n\n```\nModuleNotFoundError: No module named 'pip._vendor.six'\n```\n\nTo resolve this issue, follow these steps:\n\n1. Install the `python-six` package:\n   \n   ```bash\n   sudo apt install python-six\n   ```\n\n2. Remove the existing Pipenv environment:\n   \n   ```bash\n   pipenv --rm\n   ```\n\n3. Reinstall `scikit-learn`:\n\n   ```bash\n   pipenv install scikit-learn==1.0.2\n   ```"}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-4/007_ecba2d4d27_pipenv-with-jupyter.md", "metadata": {"id": "ecba2d4d27", "question": "Pipenv with Jupyter", "sort_order": 7}, "content": "**Problem Description:** How can we use Jupyter notebooks with the Pipenv environment?\n\n**Solution:**\n\n1. Install Jupyter and `ipykernel` using Pipenv.\n\n2. Register the kernel within the Pipenv shell using the following command:\n\n   ```bash\n   python -m ipykernel install --user --name=my-virtualenv-name\n   ```\n\n3. If you are using Jupyter notebooks in VS Code, this will also add the virtual environment to the list of kernels.\n\nFor more details, refer to this [Stack Overflow question](https://stackoverflow.com/questions/47295871/is-there-a-way-to-use-pipenv-with-jupyter-notebook)."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-4/008_afd037e097_pipenv-jupyter-no-output.md", "metadata": {"id": "afd037e097", "question": "Pipenv: Jupyter no output", "sort_order": 8}, "content": "**Problem:** I tried to run a starter notebook in a Pipenv environment but had issues with no output on prints. I used `scikit-learn==1.2.2` and `python==3.10`. Tornado version was `6.3.2`.\n\n**Solution:** The error you're encountering seems to be a bug related to Tornado, which is a Python web server and networking library. It's used by Jupyter under the hood to handle networking tasks.\n\n- Downgrading to `tornado==6.1` fixed the issue.\n\nMore information can be found on [this Stack Overflow post](https://stackoverflow.com/questions/54971836/no-output-jupyter-notebook)."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-4/009_ff0f22df21_aws-cli-invalid-base64-error-after-running-aws-kin.md", "metadata": {"id": "ff0f22df21", "question": "AWS CLI: 'Invalid base64' error after running `aws kinesis put-record`", "sort_order": 9}, "content": "**Problem Description:** \nYou might encounter an 'Invalid base64' error after executing the `aws kinesis put-record` command on your local machine. This issue can occur if you are using AWS CLI version 2. In a referenced video (4.4, around 57:42), a warning is visible as the instructor is using version 1 of the CLI.\n\n**Solution:**\nTo resolve this issue, use the argument `--cli-binary-format raw-in-base64-out` when executing the command. This option will encode your data string into base64 before transmitting it to Kinesis.\n\n```bash\naws kinesis put-record --cli-binary-format raw-in-base64-out --other-parameters\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-4/010_363335a1ea_error-index-311297-is-out-of-bounds-for-axis-0-wit.md", "metadata": {"id": "363335a1ea", "question": "Error index 311297 is out of bounds for axis 0 with size 131483 when loading parquet file.", "sort_order": 10}, "content": "Problem description: Running `starter.ipynb` in homework’s Q1 will show this error.\n\nSolution:\n\n- Update pandas along with related dependencies to the latest versions."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-4/011_405de99093_pipenv-pipfilelock-was-not-created-along-with-pipf.md", "metadata": {"id": "405de99093", "question": "Pipenv: Pipfile.lock was not created along with Pipfile", "sort_order": 11}, "content": "Use the following command to force the creation of `Pipfile.lock`:\n\n```bash\npipenv lock\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-4/012_84eaad3372_permission-denied-using-pipenv.md", "metadata": {"id": "84eaad3372", "question": "Permission Denied using Pipenv", "sort_order": 12}, "content": "This issue is usually due to the pythonfinder module in pipenv.\n\nThe solution involves manually changing the scripts as described here: [python_finder_fix](https://github.com/sarugaku/pythonfinder/pull/120/files)"}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-4/013_00b11da94b_going-further-with-google-cloud-platform-load-and.md", "metadata": {"id": "00b11da94b", "question": "Going further with Google Cloud Platform: Load and save data to GCS", "sort_order": 13}, "content": "There is a possibility to load and store data in a Google Cloud Storage bucket. To do that, authenticate through the IDE you are using and allow read and write access to a GCS bucket:\n\n1. **Authenticate gsutil with your GCP account:**\n   ```bash\n   gsutil config\n   ```\n\n2. **Upload the data to your GCS bucket:**\n   ```bash\n   gsutil cp path/to/local/data gs://your-bucket-name\n   ```\n\n3. **Create a service account and manage permissions:**\n   - In the GCP Console, go to \"IAM & Admin,\" then \"Service accounts.\"\n   - Create a new service account, grant it permissions (e.g., \"Storage Object Admin\" for GCS access), and generate a JSON key file.\n\n4. **Install the Google Cloud SDK:**\n   [Google Cloud SDK Installation Guide](https://cloud.google.com/sdk/docs/install)\n\n5. **Authenticate the SDK with your GCP account:**\n   ```bash\n   gcloud auth login\n   ```\n\n6. **Set your GCP project:**\n   ```bash\n   gcloud config set project YOUR_GCP_PROJECT_ID\n   ```\n\n7. **Install the Google Cloud Storage library:**\n   ```bash\n   !pip install google-cloud-storage\n   ```\n\n### Example Script\n\nHere's how to load a CSV file from Google Cloud Storage into a pandas DataFrame:\n\n```python\nfrom google.cloud import storage\nimport pandas as pd\n\n# Set up the storage client with the service account key\nstorage_client = storage.Client.from_service_account_json('path/to/service-account-key.json')\n\n# Get the GCS bucket\nbucket = storage_client.get_bucket('your-bucket-name')\n\n# List the contents of the bucket\nblobs = bucket.list_blobs()\nfor blob in blobs:\n    print(blob.name)\n\n# Load a CSV file from the bucket into a pandas DataFrame\ncsv_blob = bucket.blob('path/to/csv/in/bucket.csv')\ndf = pd.read_csv(csv_blob.download_as_string())\n```\n\nYou can directly save output data by setting the output file name to your desired GCS URI."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-4/014_e781391c86_error-error-while-parsing-arguments-via-cli-valuee.md", "metadata": {"id": "e781391c86", "question": "Error: Error while parsing arguments via CLI [ValueError: Unknown format code 'd' for object of type 'str']", "sort_order": 14}, "content": "When passing arguments to a script via command line and converting it to a 4-digit number using `f’{year:04d}’`, this error can occur.\n\nThis happens because command line inputs are read as strings. They need to be converted to integers before formatting with an f-string:\n\n```python\nyear = int(sys.argv[1])\nf'{year:04d}'\n```\n\nIf you use the `click` library, update your decorator accordingly:\n\n```python\nimport click\n\n@click.command()\n@click.option(\"--year\", help=\"Year for evaluation\", type=int)\ndef your_function(year):\n    # Your code\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-4/015_81c82a4875_docker-dockerizing-tips.md", "metadata": {"id": "81c82a4875", "question": "Docker: Dockerizing tips", "sort_order": 15}, "content": "Ensure the correct image is being used to derive from.\n\n- Copy the data from local to the Docker image using the `COPY` command to a relative path. Using absolute paths within the image might be troublesome.\n- Use paths starting from `/app` and don’t forget to do `WORKDIR /app` before actually performing the code execution.\n\n### Most Common Commands\n\n- Build container:\n\n  ```bash\n  docker build -t mlops-learn .\n  ```\n\n- Execute the script:\n\n  ```bash\n  docker run -it --rm mlops-learn\n  ```\n\n`<mlops-learn>` is just a name used for the image and does not have any significance."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-4/016_9ee43d55d0_running-multiple-services-in-a-docker-container.md", "metadata": {"id": "9ee43d55d0", "question": "Running multiple services in a Docker container", "sort_order": 16}, "content": "If you are trying to run Flask with Gunicorn and an MLFlow server from the same container, defining both services in the Dockerfile with CMD will only run MLFlow and not Flask.\n\n### Solution\n\n1. **Create separate shell scripts with server run commands**:\n   \n   - **For Flask with Gunicorn:**\n     \n     Save as `script1.sh`:\n     \n     ```bash\n     #!/bin/bash\n     \ngunicorn --bind=0.0.0.0:9696 predict:app\n     ```\n\n   - **For MLFlow server:**\n     \n     Save as `script2.sh`:\n     \n     ```bash\n     #!/bin/bash\n     \nmlflow server -h 0.0.0.0 -p 5000 --backend-store-uri=sqlite:///mlflow.db --default-artifact-root=g3://zc-bucket/mlruns/\n     ```\n\n2. **Create a wrapper script** to run the above two scripts:\n\n   Save as `wrapper_script.sh`:\n   \n   ```bash\n   #!/bin/bash\n\n   # Start the first process\n   ./script1.sh &\n\n   # Start the second process\n   ./script2.sh &\n\n   # Wait for any process to exit\n   wait -n\n\n   # Exit with status of process that exited first\n   exit $?\n   ```\n\n3. **Give executable permissions to all scripts**:\n   \n   ```bash\n   chmod +x *.sh\n   ```\n\n4. **Define the last line of your Dockerfile** as:\n   \n   ```bash\n   CMD ./wrapper_script.sh\n   ```\n\n5. **Don't forget to expose all ports defined by the services**."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-4/017_69052b9af2_cannot-generate-pipfilelock-raise-installationerro.md", "metadata": {"id": "69052b9af2", "question": "Cannot generate pipfile.lock raise InstallationError( pip9.exceptions.InstallationError)", "sort_order": 17}, "content": "Problem description: Cannot generate `pipfile.lock`. Raises `InstallationError( pip9.exceptions.InstallationError: Command \"python setup.py egg_info\" failed with error code 1`.\n\nSolution:\n\n- You need to force an upgrade of `wheel` and `pipenv`.\n- Run the following command:\n\n  ```bash\n  pip install --user --upgrade --upgrade-strategy eager pipenv wheel\n  ```"}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-4/018_f192a133f7_connecting-s3-bucket-to-mlflow.md", "metadata": {"id": "f192a133f7", "question": "Connecting s3 bucket to MLFLOW", "sort_order": 18}, "content": "### Problem Description\n\nHow can we connect an S3 bucket to MLflow?\n\n### Solution\n\nTo connect an S3 bucket to MLflow, use `boto3` and AWS CLI to store access keys. These access keys allow `boto3` (AWS' Python API tool) to authenticate and connect with AWS servers. Without access keys, access to the bucket cannot be verified, which could prevent connection attempts by unauthorized individuals.\n\nSteps:\n\n1. **Ensure Access Keys are Available:**\n   - Access keys are essential for `boto3` to communicate with AWS servers securely.\n   - They ensure that only authorized users with the correct permissions can access the bucket.\n\n2. **Set Bucket as Public (Optional):**\n   - Alternatively, you can set the bucket to public access.\n   - In this case, access keys are not needed as anyone can access the bucket without authentication.\n\nFor more detailed information on credentials management, refer to the official documentation:\n[https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html)"}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-4/019_d34afcb205_uploading-to-s3-fails-with-an-error-occurred-inval.md", "metadata": {"id": "d34afcb205", "question": "Uploading to s3 fails with \"An error occurred (InvalidAccessKeyId) when calling the PutObject operation: The AWS Access Key Id you provided does not exist in our records.\"", "sort_order": 19}, "content": "Even though the upload works using AWS CLI and boto3 in Jupyter notebook.\n\n**Solution:**\n\nSet the `AWS_PROFILE` environment variable (the default profile is called `default`)."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-4/020_5bf804473e_docker-dockerizing-lightgbm.md", "metadata": {"id": "5bf804473e", "question": "Docker: Dockerizing LightGBM", "sort_order": 20}, "content": "**Problem Description:**\n\n```plaintext\nlib_lightgbm.so Reason: image not found\n```\n\n**Solution:**\n\n- Add the following command to your Dockerfile:\n  \n  ```bash\n  RUN apt-get install libgomp1\n  ```\n  \n- Modify the installer command based on your OS if needed."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-4/021_164b0e749d_error-raised-when-executing-mlflows-pyfuncload_mod.md", "metadata": {"id": "164b0e749d", "question": "Error raised when executing mlflow’s pyfunc.load_model in lambda function.", "sort_order": 21}, "content": "When the request is processed in a lambda function, the mlflow library raises the following warning:\n\n```plaintext\n2022/09/19 21:18:47 WARNING mlflow.pyfunc: Encountered an unexpected error (AttributeError(\"module 'dataclasses' has no attribute '__version__'\")) while detecting model dependency mismatches. Set logging level to DEBUG to see the full traceback.\n```\n\n**Solution:**\n\n- Increase the memory of the lambda function."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-4/022_fa77367725_43-fyi-notebook-is-end-state-of-video.md", "metadata": {"id": "fa77367725", "question": "4.3 FYI Notebook is end state of Video -", "sort_order": 22}, "content": "Just a note if you are following the video but also using the repo’s notebook. The notebook is the end state of the video which eventually uses MLflow pipelines.\n\nJust watch the video and be patient. Everything will work :)"}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-4/023_af3774c8e2_solution-the-notebook-in-the-repo-is-missing-some.md", "metadata": {"id": "af3774c8e2", "question": "Solution: The notebook in the repo is missing some code, include the code to log the dict_vectorizer. If the error is after using pipelines, update the predict function as seen in the video.", "sort_order": 23}, "content": "Ensure that the code to log the `dict_vectorizer` is included in your notebook. If you're using pipelines and encountering an error, update the `predict` function according to the video instructions to resolve the issue."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-4/024_884bb9afb7_docker-passing-envs-to-my-docker-image.md", "metadata": {"id": "884bb9afb7", "question": "Docker: Passing envs to my docker image", "sort_order": 24}, "content": "**Problem Description:** \n\nI was having issues because my Python script was not reading AWS credentials from environment variables. After building the image, I was running it like this:\n\n```bash\ndocker run -it homework-04 -e AWS_ACCESS_KEY_ID=xxxxxxxx -e AWS_SECRET_ACCESS_KEY=xxxxxx\n```\n\n### Solutions:\n\n1. **Environment Variables Order: **\n   \n   You can set environment variables like `AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY`, and `AWS_SESSION_TOKEN` (if using AWS STS). Ensure these variables are passed before the image name:\n\n   ```bash\n   docker run -e AWS_ACCESS_KEY_ID=xxxxxxxx -e AWS_SECRET_ACCESS_KEY=xxxxxx -it homework-04\n   ```\n\n2. **Using an Env File:**\n\n   You can pass an env file by using the following command, assuming your env file is named `.env`:\n\n   ```bash\n   docker run -it homework-04 --env-file .env\n   ```\n\n3. **AWS Configuration Files:**\n\n   If AWS credentials are not found, the AWS SDKs and CLI will check the `~/.aws/credentials` and `~/.aws/config` files for credentials. You can map these files into your Docker container using volumes:\n\n   ```bash\n   docker run -it --rm -v ~/.aws:/root/.aws homework:v1\n   ```"}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-4/025_fd11200ead_docker-how-to-see-the-model-in-the-docker-containe.md", "metadata": {"id": "fd11200ead", "question": "Docker: How to see the model in the docker container in app/?", "sort_order": 25}, "content": "If you need to view the model inside the Docker container for the image `svizor/zoomcamp-model:mlops-3.10.0-slim`, follow these steps:\n\n1. **Create a Dockerfile:**\n   \n   ```dockerfile\n   FROM svizor/zoomcamp-model:mlops-3.10.0-slim\n   ```\n\n2. **Build the Docker Image:**\n\n   ```bash\n   docker build -t zoomcamp_test .\n   ```\n\n3. **Run the Container and List the Contents of `/app`:**\n\n   ```bash\n   docker run -it zoomcamp_test ls /app\n   ```\n\n   The output should include `model.bin`, confirming the model is present.\n\n### Additional Instructions\n\n- You can copy files into the Docker image by adding lines like `COPY myfile .` to the Dockerfile, and then run a script with arguments: \n\n  ```bash\n  docker run -it myimage myscript arg1 arg2\n  ```\n\n- Remember, a new build is required whenever the Dockerfile is modified.\n\n### Alternative Method\n\nTo list the contents of `/app` when the container runs, modify the Dockerfile:\n\n```dockerfile\nFROM svizor/zoomcamp-model:mlops-3.10.0-slim\n\nWORKDIR /app\n\nCMD ls\n```\n\n- **Note:**\n  - Use `CMD` to specify commands for container runtime. \n  - Use `RUN` for building the image and `CMD` during container execution."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-4/026_b577c9ba98_warning-the-requested-images-platform-linuxamd64-d.md", "metadata": {"id": "b577c9ba98", "question": "WARNING: The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested", "sort_order": 26}, "content": "To resolve this issue, make sure to build the Docker image with the platform tag. Use the following command:\n\n```bash\ndocker build -t homework:v1 --platform=linux/arm64 .\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-4/027_d41cd73af9_httperror-http-error-403-forbidden-when-call-apply.md", "metadata": {"id": "d41cd73af9", "question": "HTTPError: HTTP Error 403: Forbidden when call apply_model() in score.ipynb", "sort_order": 27}, "content": "Solution:\n\nInstead of using the following input file:\n\n```python\ninput_file = f'https://s3.amazonaws.com/nyc-tlc/trip+data/{taxi_type}_tripdata_{year:04d}-{month:02d}.parquet'\n```\n\nUse:\n\n```python\ninput_file = f'https://d37ci6vzurychx.cloudfront.net/trip-data/{taxi_type}_tripdata_{year:04d}-{month:02d}.parquet'\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-4/028_modulenotfounderror-no-module.md", "metadata": {"id": "1355481387", "question": "ModuleNotFoundError: No module named 'pipenv.patched.pip._vendor.urllib3.response'", "sort_order": 28}, "content": "If you're encountering the error:\n\n```\nModuleNotFoundError: No module named 'pipenv.patched.pip._vendor.urllib3.response'\n```\n\nFollow these steps to resolve it:\n\n1. Reinstall `pipenv` with the following command:\n   \n   ```bash\n   pip install pipenv --force-reinstall\n   ```\n\n2. If you see an error referring to `site-packages\\pipenv\\patched\\pip\\_vendor\\urllib3\\connectionpool.py`, then:\n\n   - Upgrade `pip` and install `requests`:\n   \n     ```bash\n     pip install -U pip\n     pip install requests\n     ```"}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-4/029_b53176b742_error-pipenv-command-not-found-after-pipenv-instal.md", "metadata": {"id": "b53176b742", "question": "Error: pipenv command not found after pipenv installation", "sort_order": 29}, "content": "When installing pipenv using the `--user` option, you need to update the PATH environment variable to run pipenv commands. It's recommended to update your `.bashrc` or `.profile` (depending on your OS) to persist the change. Edit your `.bashrc` file to include or update a line like this:\n\n```bash\nPATH=\"<path_to_your_pipenv_install_dir>:$PATH\"\n```\n\nAlternatively, you can reinstall pipenv as root for all users:\n\n```bash\nsudo -H pip install -U pipenv\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-4/030_76ed2ad3e5_homeworkquestion-2-namerror-name-year-is-not-defin.md", "metadata": {"id": "76ed2ad3e5", "question": "Homework/Question 2: Namerror: name ‘year’ is not defined", "sort_order": 30}, "content": "For question 2, which requires you to prepare the dataframe with the output, you need to first define the `year` and `month` as integers."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-4/031_5a508f2d8c_mage-error-error-loading-custom-object-at.md", "metadata": {"id": "5a508f2d8c", "question": "Mage error: Error loading custom object at…", "sort_order": 31}, "content": "When returning an object from a block, you may encounter an error like this:\n\n```plaintext\nError loading custom_object at /home/src/mage_data/*************/pipelines/taxi_duration_pipe/.variables/make_predictions/output_0: [Errno 2] No such file or directory: '/home/src/mage_data/*************/pipelines/taxi_duration_pipe/.variables/make_predictions/output_0/object.joblib'\n\nError loading custom_object at /home/src/mage_data/*************/pipelines/taxi_duration_pipe/.variables/make_predictions/output_0: [Errno 2] No such file or directory: '/home/src/mage_data/*************/pipelines/taxi_duration_pipe/.variables/make_predictions/output_0/object.joblib'\n```\n\nThis occurred when returning a `numpy.ndarray`, specifically the `y_pred` variable containing the predictions for the taxi dataset. It seems Mage struggles with some types of objects and expects data structures like DataFrames instead of `numpy.ndarrays`. To resolve this, you can return a DataFrame that includes both the `y_pred` and the ride IDs."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-4/032_e6ffad6be5_docker-the-arm64-chip-doesnt-match-with-alexeys-do.md", "metadata": {"id": "e6ffad6be5", "question": "Docker: The arm64 chip doesn’t match with Alexey’s docker image", "sort_order": 32}, "content": "You may get a warning similar to the one below when trying to run the docker:\n\n```\nWARNING: The requested image's platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested\n\nPython 3.10.13 (main, Mar 12 2024, 12:22:40) [GCC 12.2.0] on linux\n```\n\nAdd the tag `--platform linux/amd64` when running, and it should work. For example:\n\n```bash\ndocker run -it --platform linux/amd64 --rm -p 9696:9696 homework:v2\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-4/033_2c007273b4_pipenv-installation.md", "metadata": {"id": "2c007273b4", "question": "Pipenv installation", "sort_order": 33}, "content": "Make sure you have Python and pip installed by checking their versions:\n\n```bash\npython --version\n```\n\n```bash\npip --version\n```\n\nTo install Pipenv, use the following command:\n\n```bash\npip install pipenv --user\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-4/034_00b076410d_jupyter-nbconvert-error.md", "metadata": {"id": "00b076410d", "question": "Jupyter: nbconvert error", "sort_order": 34}, "content": "If you encounter an error when converting your `notebook.ipynb` into a Python script using the command:\n\n```bash\njupyter nbconvert --to script your_notebook.ipynb\n```\n\nand you see the error message:\n\n```bash\nJupyter command `jupyter-nbconvert` not found.\n```\n\nfollow these steps:\n\n1. **Verify the Directory**\n   \n   Ensure that you're in the directory containing your Jupyter notebook.\n   \n2. **Install the Necessary Package**\n\n   If the issue persists, you may need to install the `nbconvert` package. Run the following command:\n   \n   ```bash\n   pip install nbconvert\n   ```\n\n3. **Convert the Notebook**\n\n   After installing `nbconvert`, use the following command to convert your notebook to a Python script:\n   \n   ```bash\n   jupyter nbconvert your_notebook.ipynb --to python\n   ```\n   \n   **Note:** The correct command is slightly different (`--to python` instead of `--to script`)."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-4/035_55fcf4bc86_homeworkquestion-6-do-not-forget-to-specify-that-t.md", "metadata": {"id": "55fcf4bc86", "question": "Homework/Question 6: Do not forget to specify that the folder output/yellow should be created in the working directory of your docker file", "sort_order": 35}, "content": "For question 6, which requires you to include your script in a Dockerfile, specify the creation of the folder `output/yellow` in the working directory of your Docker container by adding the following line in your Dockerfile:\n\n```Dockerfile\nRUN mkdir -p output/yellow\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-4/036_d00a9ac4ba_homeworkquestion-6-entry-point-for-running-scoring.md", "metadata": {"id": "d00a9ac4ba", "question": "Homework/Question 6: Entry point for running scoring script in Docker container", "sort_order": 36}, "content": "For question 6, if you are using the script as instructed in the homework and not Flask, your entry point should be `bash`. This can be set by specifying:\n\n```dockerfile\nENTRYPOINT = [\"bash\"]\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-4/037_b9d82f5247_error-unable-to-locate-credentials.md", "metadata": {"id": "b9d82f5247", "question": "Error: Unable to locate credentials", "sort_order": 37}, "content": "This error appeared when I was running the Jupyter notebooks inside Visual Studio Code in Codespaces. I fixed it by running the Jupyter notebooks outside of Codespaces."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-5/001_f831690b89_how-do-i-log-in-to-aws-ecr-from-the-terminal-using.md", "metadata": {"id": "f831690b89", "question": "How do I log in to AWS ECR from the terminal using Docker?", "sort_order": 1}, "content": "Before (deprecated command):\n\n```bash\n$(aws ecr get-login --no-include-email)\n```\n\nNow (updated and secure command):\n\n```bash\naws ecr get-login-password --region us-west-1 | docker login --username AWS --password-stdin <ACCOUNTID>.dkr.ecr.<REGION>.amazonaws.com\n```\n\n- **Note:** Make sure you specify the correct AWS region where your ECR repository is located (e.g., us-west-1).\n- If the region is incorrect or not set properly, the login will fail with a `400 Bad Request` error — which doesn’t clearly indicate the region is the issue.\n\n### Tip: Use a Python Block in Mage to Interact with Your Dockerized ML Model\n\nThe most effective way to integrate your machine learning model into a Mage pipeline is to have your Docker container serve the model via an API (like FastAPI or Flask). Then, a custom Python block within your Mage pipeline can easily call this API to get predictions.\n\n#### Here’s the concise workflow:\n\n1. **In Your Docker Container:**\n   - **Create an API for your model:** Use a framework like FastAPI to wrap your model's prediction logic in an API endpoint. For example, create a `/predict` endpoint that accepts input data and returns the model's output.\n   - **Build and run the Docker container:** Ensure the container is running and the API is accessible. For local development, you can use docker-compose to run both your model's container and the Mage container, connecting them on the same Docker network for easy communication.\n\n2. **In Your Mage Pipeline:**\n   - **Create a custom Python block:** Add a new \"transformer\" or \"data loader\" block to your pipeline.\n   - **Call the model's API:** Inside this block, use a Python library like `requests` to send the data you want to get predictions for to your model's API endpoint.\n   - **Process the results:** The block will receive the predictions back from the API. You can then continue your Mage pipeline, using the model's output for further transformations or exporting it to a database or other destination."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-5/002_78b515b003_importerror-when-using-columnquantilemetric-with-e.md", "metadata": {"id": "78b515b003", "question": "ImportError when using ColumnQuantileMetric with Evidently", "sort_order": 2}, "content": "### Problem Description\n\nWhile working on the monitoring module homework, the instructions mention using `ColumnQuantileMetric`. However, attempting to import it results in an error:\n\n```python\nImportError: cannot import name 'ColumnQuantileMetric' from 'evidently.metrics'\n```\n\n### Solution Description\n\nThe `ColumnQuantileMetric` class does not exist in current versions of Evidently (e.g., 0.7.8+). The correct class to use is `QuantileValue`, which serves the same purpose.\n\nAdditionally, the expected argument is not `column_name`, but `column`. This differs from other metrics like `MissingValueCount` that use `column_name`.\n\nIf you see a `ValidationError: column field required`, you are likely using the wrong parameter name.\n\nYou can use it as follows:\n\n```python\nfrom evidently.metrics import QuantileValue\n\nQuantileValue(column=\"fare_amount\", quantile=0.5)\n```\n\nThis mismatch likely results from outdated references or changes in the library’s API."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-5/003_f71b6beef0_login-window-in-grafana.md", "metadata": {"id": "f71b6beef0", "question": "Login window in Grafana", "sort_order": 3}, "content": "**Problem description:** When running `docker-compose up` as shown in video 5.2, if you go to [http://localhost:3000/](http://localhost:3000/), you are asked for a username and a password.\n\n**Solution:**\n- The default credentials are:\n  - **Username:** `admin`\n  - **Password:** `admin`\n- After logging in, you can set a new password.\n\nFor more details, see [Grafana documentation](https://grafana.com/docs/grafana/latest/setup-grafana/set-password/)."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-5/004_8cb6b3176b_error-in-starting-monitoring-services-in-linux.md", "metadata": {"id": "8cb6b3176b", "question": "Error in starting monitoring services in Linux", "sort_order": 4}, "content": "**Problem Description:**\n\nIn Linux, when starting services using `docker compose up --build` as shown in video 5.2, the services won’t start and instead we get the message:\n\n```\nunknown flag: --build\n```\n\n**Solution:**\n\nSince we install docker-compose separately in Linux, use the following command:\n\n```bash\ndocker-compose up --build\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-5/005_9da67b7e6d_keyerror-content-length-when-running-preparepy.md", "metadata": {"id": "9da67b7e6d", "question": "KeyError ‘content-length’ when running prepare.py", "sort_order": 5}, "content": "**Problem:** When running `prepare.py`, encountering `KeyError: 'content-length'`.\n\n**Solution:**\n\nFrom Emeli Dral: It seems the link used in `prepare.py` to download taxi data is no longer functional. Replace the URL in the script as follows:\n\n```python\nurl = f\"https://nyc-tlc.s3.amazonaws.com/trip+data/{file}\"\n```\n\nwith:\n\n```python\nurl = f\"https://d37ci6vzurychx.cloudfront.net/trip-data/{file}\"\n```\n\nBy making this substitution in `prepare.py`, the problem should be resolved, allowing access to the necessary data."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-5/006_b074e82488_evidently-service-exit-with-code-2.md", "metadata": {"id": "b074e82488", "question": "Evidently service exit with code 2", "sort_order": 6}, "content": "### Problem Description\n\nWhen running the command `docker-compose up –build` and sending data to the real-time prediction service, the service returns \"Max retries exceeded with url: /api\". This issue occurs because the evidently service exits with code 2 due to \"app.py\" in the evidently service being unable to import `from pyarrow import parquet as pq`.\n\n### Solution\n\n1. **Install the pyarrow module**:\n   \n   ```bash\n   pip install pyarrow\n   ```\n\n2. **Restart your machine**.\n\n3. **If the first and second solutions don’t work**:\n   - Comment out the `pyarrow` module in \"app.py\" of the evidently service, as it may not be used, which resolved the issue in some cases."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-5/007_de13b97575_valueerror-incorrect-item-instead-of-a-metric-or-m.md", "metadata": {"id": "de13b97575", "question": "ValueError: Incorrect item instead of a metric or metric preset was passed to Report", "sort_order": 7}, "content": "When using Evidently, if you encounter this error, you likely forgot to add parentheses. Simply include opening and closing parentheses to resolve the issue."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-5/008_c905f283d7_for-the-report-regressionqualitymetric.md", "metadata": {"id": "c905f283d7", "question": "For the report RegressionQualityMetric()", "sort_order": 8}, "content": "You will get an error if you didn’t add a `target='duration_min'`.\n\nIf you want to use `RegressionQualityMetric()`, you need a `target='duration_min'` and you need this added to your `current_data['duration_min']`."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-5/009_481e174dd4_found-array-with-0-samples.md", "metadata": {"id": "481e174dd4", "question": "Found array with 0 sample(s)", "sort_order": 9}, "content": "### Problem Description\n\n```python\nValueError: Found array with 0 sample(s) (shape=(0, 6)) while a minimum of 1 is required by LinearRegression.\n```\n\n### Solution Description\n\nThis error occurs because the generated data is based on an early date, resulting in an empty training dataset.\n\nAdjust the following:\n\n```python\nbegin = datetime.datetime(202X, X, X, 0, 0)\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-5/010_296ea7fed1_adding-additional-metric.md", "metadata": {"id": "296ea7fed1", "question": "Adding additional metric", "sort_order": 10}, "content": "### Problem Description\n\nGetting “target columns” “prediction columns” not present errors after adding a metric.\n\n### Solution Description\n\nMake sure to read through the documentation on what is required or optional when adding the metric. For example, `DatasetCorrelationsMetric` doesn’t require any parameters because the metric evaluates for correlations among the features."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-5/011_f385403427_grafana-standard-login-does-not-work.md", "metadata": {"id": "f385403427", "question": "Grafana: Standard login does not work", "sort_order": 11}, "content": "When trying to log in to Grafana with the standard credentials (admin/admin), an error occurs.\n\n### Solution\n\n1. To reset the admin password, use the following command inside the Grafana container:\n\n   ```bash\n   grafana cli admin reset-admin-password admin\n   ```\n\n   **Note:** The `grafana-cli` command is deprecated. Use `grafana cli` instead.\n\n2. Enter the Docker container with Grafana:\n\n   - Find the Container ID by running:\n     \n     ```bash\n     docker ps\n     ```\n\n   - Use the Container ID to reset the password. Replace `<container_ID>` with the actual Container ID:\n\n     ```bash\n     lpep_pickup_datetime<container_ID> grafana cli admin reset-admin-password admin\n     ```\n\nThis should resolve the login issue."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-5/012_13559b737e_the-chart-in-grafana-doesnt-get-updates.md", "metadata": {"id": "13559b737e", "question": "The chart in Grafana doesn’t get updates", "sort_order": 12}, "content": "**Problem Description:** While my metric generation script was still running, I noticed that the charts in Grafana don’t get updated.\n\n**Solution:**\n\n- **Refresh Interval:** Set it to a small value, such as 5, 10, or 30 seconds.\n- **Timezone Setting:** Ensure you use your local timezone in a call to `pytz.timezone`. For example, change the setting from \"Europe/London\" to your local timezone to get updates."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-5/013_4afbb7db24_prefect-prefect-server-was-not-running-locally.md", "metadata": {"id": "4afbb7db24", "question": "Prefect: Prefect server was not running locally", "sort_order": 13}, "content": "### Problem Description\n\nPrefect server was not running locally. The command `prefect server start` was executed but it stopped immediately.\n\n### Solution\n\n- Used Prefect Cloud to run the script.\n- Created an issue on the Prefect GitHub repository."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-5/014_258d55402b_docker-no-disk-space-left-error-when-doing-docker.md", "metadata": {"id": "258d55402b", "question": "Docker: no disk space left error when doing docker compose up", "sort_order": 14}, "content": "To resolve the \"no disk space left\" error when running `docker compose up`, follow these steps:\n\n1. Run the following command to remove unused objects (build cache, containers, images, etc.):\n   \n   ```bash\n   docker system prune\n   ```\n\n2. If you want to see what is taking up space before pruning, use:\n   \n   ```bash\n   docker system df\n   ```"}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-5/015_007cc84471_failed-to-listen-on-8080-reason-php_network_getadd.md", "metadata": {"id": "007cc84471", "question": "Failed to listen on :::8080 (reason: php_network_getaddresses: getaddrinfo failed: Address family for hostname not supported)", "sort_order": 15}, "content": "Problem: When running `docker-compose up --build`, you may encounter this error.\n\nTo solve this issue, add the following command in the `adminer` block in your `docker-compose.yml` file:\n\n```yaml\na dminer:\n  command: php -S 0.0.0.0:8080 -t /var/www/html\n  image: adminer...\n```\n\nThis configuration specifies the command to be executed when the container starts, setting up PHP to listen on `0.0.0.0:8080`. This addresses the network error by changing the bind address."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-5/016_5c68f358c2_generate-evidently-chart-in-grafana.md", "metadata": {"id": "5c68f358c2", "question": "Generate Evidently Chart in Grafana", "sort_order": 16}, "content": "**Problem:** Can we generate charts like Evidently inside Grafana?\n\n**Solution:**\n\n- In Grafana, you can use a stat panel (just a number) and a scatter plot panel, which may require a plug-in.\n- Unfortunately, there's no native method to directly recreate the Evidently dashboard.\n- Ensure that all relevant information is logged to your Grafana data source, then design your custom plots.\n\n**External Recreation:**\n\n- Export the Evidently output in JSON with `include_render=True` for external visualization:\n  - See more details [here](https://docs.evidentlyai.com/user-guide/customization/json-dict-output).\n- For non-aggregated visuals, use the option \"`raw_data\": True\".\n  - More details [here](https://docs.evidentlyai.com/user-guide/customization/report-data-aggregation).\n\nThis specific plot with under- and over-performance segments is particularly useful during debugging and might be easier to view ad hoc using Evidently."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-5/017_ebd339f9a5_error-when-importing-evidently-package-because-of.md", "metadata": {"id": "ebd339f9a5", "question": "Error when importing evidently package because of numpy version upgraded", "sort_order": 17}, "content": "A new version of Numpy has just been released v 2.0.0 (on Jun 16, 2024), which causes an import error of the package.\n\n```python\n\"`np.chararray` is deprecated and will be removed from \"\n\n419     \"the main namespace in the future. Use an array with a string \"\n\n420     \"or bytes dtype instead.\", DeprecationWarning, stacklevel=2): `np.float_` was removed in the NumPy 2.0 release. Use `np.float64` instead.\n```\n\nOr\n\n```python\nAttributeError: `np.float_` was removed in the NumPy 2.0 release. Use `np.float64` instead.\n```\n\nYou can solve it by reinstalling numpy to a previous version 1.26.4. Just run:\n\n```bash\npython -m pip install numpy==1.26.4\n```\n\nOr modify the `requirements.txt` to freeze the version:\n\n```text\nnumpy==1.26.4\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-5/018_beef541941_bind-for-00005432-failed-port-is-already-allocated.md", "metadata": {"id": "beef541941", "question": "Bind for 0.0.0.0:5432 failed: port is already allocated", "sort_order": 18}, "content": "**Problem:** When trying to start the postgres services through `docker-compose up`, this error occurs:\n\n```\nBind for 0.0.0.0:5432 failed: port is already allocated\n```\n\n**Note:** This issue occurs because port 5432 is already used by another service.\n\n**Solution:** Update the port mapping for the Postgres service to `5433:5432` in the Docker Compose YAML file."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-5/019_5ad72730d6_tabledatabase-not-showing-on-grafana-dashboard.md", "metadata": {"id": "5ad72730d6", "images": [{"description": "image #1", "id": "image_1", "path": "images/mlops-zoomcamp/image_a92c0f4d.png"}], "question": "Table/database not showing on Grafana dashboard", "sort_order": 19}, "content": "**Problem:**\n\nFor version 5.4, when trying to create a new dashboard, Grafana does not list the `dummy_metrics` table in the query tab.\n\n**Note:** Change the datasource name from the default \"PostgreSQL.\"\n\n**Solution 1:**\n\nUpdate the `config/grafana_datasources.yaml` with the following:\n\n```yaml\n# List of datasources to insert/update\n# Available in the database\ndatasources:\n\n- name: NewPostgreSQL\n  type: postgres\n  url: db:5432\n  user: postgres\n  secureJsonData:\n    password: 'example'\n  jsonData:\n    sslmode: 'disable'\n    database: test\n```\n\n**Solution 2:**\n\n- Use the \"Code\" option rather than the \"Builder\" option.\n- Load the data using your own SQL queries. See the screenshot below (box highlighted in red).\n- Tip: If you write your `FROM` statement first, the `SELECT` options can be done through auto-complete.\n\n<{IMAGE:image_1}>"}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-5/020_26a7d0d9b6_adminer-not-loaded.md", "metadata": {"id": "26a7d0d9b6", "question": "Adminer Not Loaded", "sort_order": 20}, "content": "**Problem:** After running Docker Compose, Adminer cannot be accessed on [http://127.0.0.1:8080/](http://127.0.0.1:8080/index.php)\n\n**Solution:** Add `index.php` after the URL, so the URL will be [http://127.0.0.1:8080/index.php](http://127.0.0.1:8080/index.php)"}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-5/021_44a68a830c_grafana-ui-changes.md", "metadata": {"id": "44a68a830c", "images": [{"description": "image #1", "id": "image_1", "path": "images/mlops-zoomcamp/image_4e90873b.png"}, {"description": "image #2", "id": "image_2", "path": "images/mlops-zoomcamp/image_27675c03.png"}], "question": "Grafana: UI Changes", "sort_order": 21}, "content": "**Problem:** When selecting a column from the table, the error message is displayed:\n\n```\nno time column: no time column found\n```\n\n<{IMAGE:image_1}>\n\n**Solution:** Add a timestamp column in the query builder.\n\n<{IMAGE:image_2}>"}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-5/022_982fa87351_runtime-error-failed-to-reach-api-on-prefect.md", "metadata": {"id": "982fa87351", "images": [{"description": "image #1", "id": "image_1", "path": "images/mlops-zoomcamp/image_f4090a89.png"}], "question": "Runtime Error: Failed to Reach API on Prefect", "sort_order": 22}, "content": "**Problem:** When running `evidently_metrics_calculation.py`, the following error is shown:\n\n```\nRuntimeError: Cannot create flow run. Failed to reach API at https://api.prefect.cloud/api/accounts/ee976605-4ca7-4a27-b5e3-0a37da3c7678/workspaces/78b23cf5-38bb-4d8b-9888-5bf8070d6d62/\n```\n\n**Solution:**\n\n- Register or sign up at [https://app.prefect.cloud/account/](https://app.prefect.cloud/account/)\n\n<{IMAGE:image_1}>"}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-5/023_93dfb7d96c_grafana-dashboard-error-after-reset-db-query-error.md", "metadata": {"id": "93dfb7d96c", "images": [{"description": "image #1", "id": "image_1", "path": "images/mlops-zoomcamp/image_03216ab8.png"}], "question": "Grafana dashboard error after reset: db query error: pq: database “test” does not exist", "sort_order": 23}, "content": "Problem: You’ve already loaded your data, created a dashboard, and saved it. However, upon running `docker-compose up` after saving the dashboard, you encounter this error: \n\n```plaintext\ndb query error: pq: database “test” does not exist\n```\n\n<{IMAGE:image_1}>\n\nSolution:\n\nThis error indicates you haven’t run the DB initialization code. If you did run it before and even saw results, the issue likely arises because you restarted the docker-compose services.\n\nThe default `docker-compose.yml` file doesn’t have a volume for the Postgres DB. This means every restart will delete the DB data.\n\nTo resolve this:\n\n1. **If not planning to restart the services again:** Simply rerun the DB initialization and filling code of your exercise.\n\n2. **If you plan to restart services frequently:**\n   - Add a volume to your PostgreSQL service in the `docker-compose.yml` file:\n\n     ```yaml\n     volumes:\n       - ./data/postgres:/var/lib/postgresql/data\n     ```\n\n   - Note: Ensure you create a `./data` directory in your project.\n\n3. To attach the volume, run the following:\n\n   ```bash\n   docker-compose down\n   docker-compose up --build\n   ```"}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-5/024_4d86e49240_are-there-any-alternative-to-evidently-on-cloud-pl.md", "metadata": {"id": "4d86e49240", "question": "Are there any alternative to Evidently on cloud platforms?", "sort_order": 24}, "content": "There are several alternatives to Evidently for monitoring machine learning models in the cloud. Here are a few options on popular cloud platforms:\n\n- **Google Cloud Platform (GCP):** AI Platform Predictions with Cloud Monitoring & Logging\n\n- **Microsoft Azure:** Azure Machine Learning\n\n- **Amazon Web Services (AWS):** Amazon SageMaker Model Monitor\n\nThese services provide model monitoring capabilities, allowing you to track the performance and data quality of your machine learning models within the cloud environment."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-5/025_215ad00cfc_dockererrorsdockerexception-error-while-fetching-s.md", "metadata": {"id": "215ad00cfc", "question": "docker.errors.DockerException: Error while fetching server API version: HTTPConnection.request() got an unexpected keyword argument 'chunked'", "sort_order": 25}, "content": "- Instead of using:\n  \n  ```bash\n  docker-compose up --build\n  ```\n  \n- Use:\n\n  ```bash\n  docker compose up --build\n  ```"}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-5/026_f2458f5a26_docker-docker-compose-deprecated.md", "metadata": {"id": "f2458f5a26", "question": "Docker: Docker-Compose deprecated", "sort_order": 26}, "content": "Docker Compose v1 is deprecated from April 2023 onwards. More information on why v2 is better can be found in this blog post:\n\n[New Docker Compose V2 and V1 Deprecation](https://www.docker.com/blog/new-docker-compose-v2-and-v1-deprecation/)"}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-5/027_b7625d600a_psycopgoperationalerror-connection-failed-connecti.md", "metadata": {"id": "b7625d600a", "question": "psycopg.OperationalError: connection failed: connection to server at \"127.0.0.1\", port 5432 failed: FATAL:  password authentication failed for user \"postgres\"", "sort_order": 27}, "content": "It could be that there is already another Docker container running (for example, from a previous session).\n\nTo resolve this issue:\n\n1. Check for running containers:\n   ```bash\n   docker ps\n   ```\n2. Stop the running container:\n   ```bash\n   docker stop <container_name_or_ID>\n   ```"}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-5/028_6a3a0cc252_login-to-db-not-working-in-adminer-ui-even-after-r.md", "metadata": {"id": "6a3a0cc252", "images": [{"description": "image #1", "id": "image_1", "path": "images/mlops-zoomcamp/image_c66aba56.png"}], "question": "Login to DB not working in Adminer UI even after right DB, user and password.", "sort_order": 28}, "content": "**Problem:** Adminer UI is not responding or showing database details, even with the correct database, user, and password.\n\n<{IMAGE:image_1}>\n\n---\n\n**Solution:** Try accessing the database from the command line using `psql`.\n\nYou can quickly install `psql` via package managers such as `sudo apt`.\n\nHere is an example:\n\n```bash\n(base) cpl@inpne-ed-lab003:~$ psql -h localhost -p 5432 -U postgres\n\nPassword for user postgres: \n\npsql (14.12 (Ubuntu 14.12-0ubuntu0.22.04.1), server 16.4 (Debian 16.4-1.pgdg120+1))\n\nWARNING: psql major version 14, server major version 16.\nSome psql features might not work.\n\nType \"help\" for help.\n\npostgres=# \\l\n\nList of databases\n\n Name    |  Owner   | Encoding |  Collate   |   Ctype    |   Access privileges \n-----------+----------+----------+------------+------------+-----------------------\npostgres  | postgres | UTF8     | en_US.utf8 | en_US.utf8 | \ntemplate0 | postgres | UTF8     | en_US.utf8 | en_US.utf8 | =c/postgres          +\n          |          |          |            |            | postgres=CTc/postgres\ntemplate1 | postgres | UTF8     | en_US.utf8 | en_US.utf8 | =c/postgres          +\n          |          |          |            |            | postgres=CTc/postgres\ntest      | postgres | UTF8     | en_US.utf8 | en_US.utf8 |\n(4 rows)\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-5/029_f4dcbdd4ee_is-it-mandatory-to-use-a-reference-dataset-when-ge.md", "metadata": {"id": "f4dcbdd4ee", "question": "Is it mandatory to use a reference dataset when generating a report with Evidently?", "sort_order": 29}, "content": "No. While Evidently is designed to compare a reference dataset with a current one, it can also be used without a reference dataset.\n\nIn such cases, you can pass `reference_data=None` when creating the report. This is useful for generating descriptive statistics or univariate analyses on a single dataset (e.g., using `ColumnSummaryMetric`, `DatasetMissingValuesMetric`, etc.)."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-5/030_28676cfdf9_what-version-of-evidently-ai-is-used-in-the-course.md", "metadata": {"id": "28676cfdf9", "question": "What version of Evidently AI is used in the course?", "sort_order": 30}, "content": "In the video (current cohort: 2025), the Evidently version used is 0.4.17. However, any version up to 0.6.7 will work with the code provided in the video and the repository.\n\nNote that newer versions have changed the APIs, so the code in the video may not run with versions beyond 0.6.7.\n\n### Error: Failed to create provisioner when running `docker-compose up –build`\n\n```\n✗ Failed to create provisioner: Failed to read dashboards config: could not parse provisioning config file: dashboards.yaml error: read /etc/grafana/provisioning/dashboards/dashboards.yaml: is a directory\n```\n\nTo resolve this error in your `docker-compose.yml` file, update the Grafana `volumes`:\n\n- Change from a YML file reference to a directory reference.\n- Instead of specifying `/etc/grafana/provisioning/dashboards/dashboards.yaml`, use `/etc/grafana/provisioning/dashboards/dashboards`.\n- Apply this change to all file names in the Grafana `volumes` section."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-6/001_9dedd0bb1c_evidently-import-error.md", "metadata": {"id": "9dedd0bb1c", "question": "Evidently: Import Error", "sort_order": 1}, "content": "### Problem Description\n\nWhen running the command:\n\n```python\nfrom evidently import ColumnMapping\n```\n\nThe following import error occurs:\n\n```plaintext\nImportError: cannot import name 'ColumnMapping' from 'evidently'\n```\n\n### Solution\n\n1. Uninstall the latest version of `evidently`:\n   \n   ```bash\n   pip uninstall evidently -y\n   ```\n\n2. Install an older compatible version:\n   \n   ```bash\n   pip install evidently==0.4.18\n   ```\n\n3. Restart the kernel to reload the environment."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-6/002_ce86e574f1_error-following-video-62-mlflow1270.md", "metadata": {"id": "ce86e574f1", "question": "Error following video 6.2: mlflow=1.27.0", "sort_order": 2}, "content": "When following the video instructions and running the Dockerfile, I encountered an error that the Dockerfile build failed on line 8 due to no matching distribution for `mlflow==1.27.0`. Below is the code output:\n\n```bash\n4.900 ERROR: No matching distribution found for mlflow==1.27.0\n\n4.901 ERROR: Couldn't install package: {}\n\n4.901  Package installation failed...\n\n------\n\nDockerfile:8\n\n--------------------\n\n6 |     COPY [ \"Pipfile\", \"Pipfile.lock\", \"./\" ]\n\n7 |\n\n8 | >>> RUN pipenv install --system --deploy\n\n9 |\n\n10 |     COPY [ \"lambda_function.py\", \"model.py\", \"./\" ]\n\n--------------------\n\nERROR: failed to solve: process \"/bin/sh -c pipenv install --system --deploy\" did not complete successfully: exit code: 1\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-6/003_2ce73ee58a_get-an-error-unable-to-locate-credentials-after-ru.md", "metadata": {"id": "2ce73ee58a", "question": "Get an error ‘Unable to locate credentials’ after running localstack with kinesis", "sort_order": 3}, "content": "You may encounter the error `{'errorMessage': 'Unable to locate credentials', …` from the print statement in `test_docker.py` after running localstack with Kinesis.\n\nTo resolve this issue:\n\n1. In the `docker-compose.yaml` file, add the following environment variables:\n   - `AWS_ACCESS_KEY_ID`\n   - `AWS_SECRET_ACCESS_KEY`\n   \n   You can assign any value to these variables (e.g., `abc`).\n\n2. Alternatively, you can run the following command:\n\n   ```bash\n   aws --endpoint-url http://localhost:4566 configure\n   ```\n\n   Provide random values for the following prompts:\n   - AWS Access Key ID\n   - AWS Secret Access Key\n   - Default region name\n   - Default output format\n\n<END>"}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-6/004_5137e6c204_get-an-error-unspecified-location-constraint-is-in.md", "metadata": {"id": "5137e6c204", "question": "Get an error ‘unspecified location constraint is incompatible’", "sort_order": 4}, "content": "You may get an error while creating a bucket with LocalStack and the Boto3 client:\n\n```python\nbotocore.exceptions.ClientError: An error occurred (IllegalLocationConstraintException) when calling the CreateBucket operation: The unspecified location constraint is incompatible for the region specific endpoint this request was sent to.\n```\n\nTo fix this, instead of creating a bucket via:\n\n```python\ns3_client.create_bucket(Bucket='nyc-duration')\n```\n\nCreate it with:\n\n```python\ns3_client.create_bucket(Bucket='nyc-duration', CreateBucketConfiguration={'LocationConstraint': AWS_DEFAULT_REGION})\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-6/005_cb04adb824_get-an-error-botocoreawsrequestawsrequest-object-a.md", "metadata": {"id": "cb04adb824", "question": "Get an error \"<botocore.awsrequest.AWSRequest object at 0x7fbaf2666280>\" after running an AWS CLI command", "sort_order": 5}, "content": "When executing an AWS CLI command (e.g., `aws s3 ls`), you may encounter the error:\n\n```plaintext\n<botocore.awsrequest.AWSRequest object at 0x7fbaf2666280>\n```\n\nTo fix this, set the AWS CLI environment variables:\n\n```bash\nexport AWS_DEFAULT_REGION=eu-west-1\nexport AWS_ACCESS_KEY_ID=foobar\nexport AWS_SECRET_ACCESS_KEY=foobar\n```\n\nTheir values are not important; any values will suffice."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-6/006_cc6b6f1481_pre-commit-triggers-an-error-at-every-commit-mappi.md", "metadata": {"id": "cc6b6f1481", "question": "Pre-commit: Triggers an error at every commit: “mapping values are not allowed in this context”", "sort_order": 6}, "content": "At every commit, the above error is thrown and no pre-commit hooks are run.\n\nEnsure the indentation in `.pre-commit-config.yaml` is correct, particularly the 4 spaces ahead of every `repo` statement."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-6/007_611e6f015f_could-not-reconfigure-pytest-from-zero-after-getti.md", "metadata": {"id": "611e6f015f", "images": [{"description": "image #1", "id": "image_1", "path": "images/mlops-zoomcamp/image_b6cec005.png"}], "question": "Could not reconfigure pytest from zero after getting done with previous folder", "sort_order": 7}, "content": "<{IMAGE:image_1}>\n\nNo option to remove pytest test\n\n- Remove the `.vscode` folder located in the folder you previously used for testing. For example, if you chose to test in the \"week6-best-practices\" folder, remove the `.vscode` directory inside that folder."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-6/008_fa23739c18_empty-records-in-kinesis-get-records-with-localsta.md", "metadata": {"id": "fa23739c18", "question": "Empty Records in Kinesis Get Records with LocalStack", "sort_order": 8}, "content": "**Problem Description**\n\nFollowing video 6.3, at minute 11:23, the get records command returns empty records.\n\n**Solution**\n\nAdd `--no-sign-request` to the Kinesis get records call:\n\n```bash\naws --endpoint-url=http://localhost:4566 kinesis get-records --shard-iterator [...] --no-sign-request\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-6/009_226f8425c4_in-powershell-git-commit-raises-utf-8-encoding-err.md", "metadata": {"id": "226f8425c4", "question": "In Powershell, Git commit raises utf-8 encoding error after creating pre-commit yaml file", "sort_order": 9}, "content": "### Problem Description\n\nWhen executing the following command in PowerShell, an error occurs:\n\n```bash\ngit commit -m 'Updated xxxxxx'\n```\n\nThe error message is:\n\n```bash\nAn error has occurred: InvalidConfigError:\n\n==> File .pre-commit-config.yaml\n\n=====> 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\n```\n\n### Solution Description\n\nSet UTF-8 encoding when creating the pre-commit YAML file:\n\n```powershell\npre-commit sample-config | out-file .pre-commit-config.yaml -encoding utf8\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-6/010_b55fea0fc7_git-commit-with-pre-commit-hook-raises-error-pytho.md", "metadata": {"id": "b55fea0fc7", "question": "Git: Commit with pre-commit hook raises error ‘'PythonInfo' object has no attribute 'version_nodot'", "sort_order": 10}, "content": "### Problem Description\n\nWhen attempting to commit in Git, the following error occurs:\n\n```bash\ngit commit -m 'Updated xxxxxx'\n\n[INFO] Initializing environment for GitHub.\n[INFO] Installing environment for GitHub.\n[INFO] Once installed this environment will be reused.\n\nAn unexpected error has occurred: CalledProcessError: command:\n…\nreturn code: 1\nexpected return code: 0\nstdout:\nAttributeError: 'PythonInfo' object has no attribute 'version_nodot'\n```\n\n### Solution\n\nTo resolve this issue, clear the app-data of the virtual environment using the following command:\n\n```bash\npython -m virtualenv api -vvv --reset-app-data\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-6/011_0cfb31c221_pytest-error-module-not-found-when-using-custom-pa.md", "metadata": {"id": "0cfb31c221", "question": "Pytest error 'module not found' when using custom packages in the source code", "sort_order": 11}, "content": "### Problem Description\n\nProject structure:\n\n```\n/sources/production/model_service.py\n/sources/tests/unit_tests/test_model_service.py\n```\n\nThe test file contains:\n\n```python\nfrom production.model_service import ModelService\n```\n\n- Running `python test_model_service.py` from the `sources` directory works.\n- Running `pytest ./test/unit_tests` fails with: `No module named 'production'`.\n\n### Solution\n\n- Use the following command:\n  \n  ```bash\n  python -m pytest ./test/unit_tests\n  ```\n\n#### Explanation\n\n- `pytest` does not automatically add to the `sys.path` the path where it is run.\n- Alternatives include:\n  - Running `python -m pytest`\n  - Exporting the `PYTHONPATH` before executing `pytest`:\n    \n    ```bash\n    export PYTHONPATH=.\n    ```"}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-6/012_02ff7e82ce_pytest-error-module-not-found-when-using-pre-commi.md", "metadata": {"id": "02ff7e82ce", "question": "Pytest error ‘module not found’ when using pre-commit hooks if using custom packages in the source code", "sort_order": 12}, "content": "### Problem Description\n\nProject structure:\n\n```\n/sources/production/model_service.py\n/sources/tests/unit_tests/test_model_service.py \n```\n\nIn `test_model_service.py`:\n\n```python\nfrom production.model_service import ModelService\n```\n\nA `git commit -t ‘test’` raises `No module named ‘production’` when calling the pytest hook:\n\n```yaml\n- repo: local\n\n  hooks:\n    - id: pytest-check\n      name: pytest-check\n      entry: pytest\n      language: system\n      pass_filenames: false\n      always_run: true\n      args: [\n        \"tests/\"\n      ]\n```\n\n### Solution Description\n\nUse this hook instead:\n\n```yaml\n- repo: local\n\n  hooks:\n    - id: pytest-check\n      name: pytest-check\n      entry: \"./sources/tests/unit_tests/run.sh\"\n      language: system\n      types: [python]\n      pass_filenames: false\n      always_run: true\n```\n\nEnsure that `run.sh` sets the correct directory and runs pytest:\n\n```bash\ncd \"$(dirname \"$0\")\"\n\ncd ../..\n\nexport PYTHONPATH=.\n\npipenv run pytest ./tests/unit_tests\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-6/013_a345ebd834_github-actions-permission-denied-error-when-execut.md", "metadata": {"id": "a345ebd834", "question": "Github actions: Permission denied error when executing script file", "sort_order": 13}, "content": "### Problem Description\n\nThis issue occurs when running the following step in the CI YAML file definition:\n\n```yaml\nyml\n- name: Run Unit Tests\n  working-directory: \"sources\"\n  run: ./tests/unit_tests/run.sh\n```\n\nWhen executing the GitHub CI action, the following error occurs:\n\n```\n…/tests/unit_test/run.sh Permission error\n\nError: Process completed with error code 126\n```\n\n### Solution\n\nTo resolve this issue, add execution permission to the script and commit the changes:\n\n```bash\ngit update-index --chmod=+x ./sources/tests/unit_tests/run.sh\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-6/014_560c0eb368_managing-multiple-docker-containers-with-docker-co.md", "metadata": {"id": "560c0eb368", "question": "Managing Multiple Docker Containers with docker-compose profile", "sort_order": 14}, "content": "### Problem Description\n\nWhen a Docker Compose file contains many containers, running them all may consume too many resources. There is often a need to easily select only a group of containers while ignoring irrelevant ones during testing.\n\n### Solution Description\n\n1. Add `profiles: [\"profile_name\"]` in the service definition within your `docker-compose.yml` file.\n\n2. Start the service with the specific profile using the command:\n\n   ```bash\n   docker-compose --profile profile_name up\n   ```"}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-6/015_8e6e50be33_aws-cli-why-do-aws-cli-commands-throw-botocoreawsr.md", "metadata": {"id": "8e6e50be33", "question": "AWS CLI: Why do AWS CLI commands throw <botocore.awsrequest.AWSRequest object at 0x74c89c3562d0> type messages when listing or creating AWS S3 buckets with LocalStack?", "sort_order": 15}, "content": "If you encounter such messages when trying to list your AWS S3 buckets (e.g., `aws --endpoint-url=http://localhost:4566 s3 ls`), you can try configuring AWS with the same region, access key, and secret key as those in your `docker-compose` file.\n\nTo configure AWS CLI, follow these steps:\n\n1. After installing the AWS CLI, run the following command in your terminal:\n   \n   ```bash\n   aws configure\n   ```\n\n2. Input the required information when prompted:\n   - **AWS Access Key ID:** [Example: `abc`]\n   - **AWS Secret Access Key:** [Example: `xyz`]\n   - **Default region name:** [Example: `eu-west-1`]"}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-6/016_16099520dd_aws-regions-need-to-match-in-docker-compose.md", "metadata": {"id": "16099520dd", "question": "AWS: Regions need to match in docker-compose", "sort_order": 16}, "content": "**Problem Description**\n\nIf you are experiencing issues with integration tests and Kinesis, ensure that your AWS regions are consistent between docker-compose and your local configuration. Otherwise, you may create a stream in an incorrect region.\n\n**Solution Description**\n\n- Set the region in your AWS config file:\n  \n  ```plaintext\n  ~/.aws/config\n  ```\n  \n  Example:\n  \n  ```text\n  region = us-east-1\n  ```\n\n- Ensure that the region in your `docker-compose.yaml` is also set:\n  \n  ```yaml\n  environment:\n    - AWS_DEFAULT_REGION=us-east-1\n  ```"}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-6/017_330f9005e2_isort-pre-commit.md", "metadata": {"id": "330f9005e2", "question": "Isort Pre-commit", "sort_order": 17}, "content": "### Problem Description\n\nPre-commit command was failing with isort repo.\n\n### Solution\n\n- Set version to `5.12.0`"}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-6/018_abb38848b5_how-to-destroy-infrastructure-created-via-github-a.md", "metadata": {"id": "abb38848b5", "question": "How to destroy infrastructure created via GitHub Actions", "sort_order": 18}, "content": "### Problem Description\n\nInfrastructure created in AWS with CD-Deploy Action needs to be destroyed.\n\n### Solution Description\n\nTo destroy the infrastructure from local:\n\n```bash\nterraform init -backend-config=\"key=mlops-zoomcamp-prod.tfstate\" --reconfigure\n```\n\n```bash\nterraform destroy --var-file vars/prod.tfvars\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-6/019_b396205ad7_error-errno-13-permission-denied-homeubuntuawscred.md", "metadata": {"id": "b396205ad7", "question": "Error \"[Errno 13] Permission denied: '/home/ubuntu/.aws/credentials'\" when running any aws command", "sort_order": 19}, "content": "After installing AWS CLI v2 on Linux, you may encounter a permission error when trying to run AWS commands that require access to your credentials. For example, when running `aws configure`, you might insert the key and secret but receive a permission error.\n\nThe issue arises because the `ubuntu` user does not have permission to read or write files in the `.aws` folder, and the `credentials` and `config` files do not exist. To resolve this:\n\n1. Navigate to the `.aws` folder, typically located at `/home/ubuntu/.aws`.\n\n2. Create empty `credentials` and `config` files:\n\n   ```bash\n   touch credentials\n   touch config\n   ```\n\n3. Modify the file permissions:\n\n   ```bash\n   sudo chmod -R 777 credentials\n   sudo chmod -R 777 config\n   ```\n\n4. Run `aws configure`, modify the keys and secret, and save them to the `credentials` file. You can then execute AWS commands from your Python scripts or the command line."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-6/020_b0300fc9b8_why-do-i-get-a-valueerror-invalid-endpoint-error-w.md", "metadata": {"id": "b0300fc9b8", "question": "Why do I get a `ValueError: Invalid endpoint` error when using Boto3 with Docker Compose services?", "sort_order": 20}, "content": "Boto3 does not support underscores (_) in service URLs. Naming your Docker Compose services with underscores will cause Boto3 to throw an error when connecting to the endpoint. (Source: [GitHub Issue](https://github.com/boto/boto3/issues/703))\n\n### Incorrect Docker Compose configuration with underscores\n\n```yaml\ndocker-compose.yml\n\nversion: '3.8'\n\nservices:\n  backend_service:\n    image: my_backend_image\n    ...\n  s3_service:\n    image: localstack/localstack\n    …\n```\n\nRename your services to avoid using underscores. For example, change `s3_service` to `s3service`.\n\nThis way, when you run \n\n```python\nclient = boto3.client('s3', endpoint_url=\"http://s3service:4566\")\n```\n\nYou won’t get any error.\n\n---\n\n### Problem: Pre-commit fails with `RuntimeError: The Poetry configuration is invalid:`\n\n```\ndata.extras.pipfile_deprecated_finder[2] must match pattern ^[a-zA-Z-_.0-9]+$\n```\n\n### Solution:\n\nThis is caused by a version mismatch between the `pre-commit-config.yaml` designated version for your package and the actual versions. Check the versions in `Pipfile.lock` and update as appropriate."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/module-6/021_2eb2863faf_why-do-i-get-a-valueerror-the-truth-value-of-a-dat.md", "metadata": {"id": "2eb2863faf", "question": "Why do I get a “ValueError: The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()” error when doing unit test that involves comparing two data frames?", "sort_order": 21}, "content": "When you compare two Pandas DataFrames, the result is also a DataFrame. The same is true for Pandas Series. To properly compare them, you should not compare data frames directly.\n\nInstead, convert the actual and expected DataFrames into a list of dictionaries, then use `assert` to compare the resulting lists.\n\nExample:\n\n```python\nactual_df_list_dicts = actual_df.to_dict('records')\n\nexpected_df_list_dicts = expected_df.to_dict('records')\n\nassert actual_df_list_dicts == expected_df_list_dicts\n```"}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/project/001_716ff05962_pytest-doesnt-recognize-my-installed-libraries-but.md", "metadata": {"id": "716ff05962", "question": "pytest doesn't recognize my installed libraries, but the script works in the terminal. Why?", "sort_order": 1}, "content": "This usually happens because VS Code is using a different Python interpreter than the one in your terminal. As a result, pytest can't see the packages installed in your virtual environment.\n\nHow to fix:\n\n1. In your terminal, run:\n   \n   ```bash\n   which python\n   ```\n\n2. In VS Code, open the command palette (Ctrl+Shift+P) and select:\n   \n   `Python: Select Interpreter`\n\n3. Choose the same interpreter shown in step 1."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/project/002_0a7c21fee9_is-it-a-group-project.md", "metadata": {"id": "0a7c21fee9", "question": "Is it a group project?", "sort_order": 2}, "content": "No, the capstone is a solo project."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/project/003_9ae63755b8_do-we-submit-2-projects-what-does-attempt-1-and-2.md", "metadata": {"id": "9ae63755b8", "question": "Do we submit 2 projects, what does attempt 1 and 2 mean?", "sort_order": 3}, "content": "You only need to submit one project. If the submission at the first attempt fails, you can improve it and re-submit during the attempt 2 submission window.\n\n- If you want to submit two projects for the experience and exposure, you must use different datasets and problem statements.\n- If you can’t make it to the attempt 1 submission window, you still have time to catch up to meet the attempt 2 submission window.\n\nRemember that the submission does not count towards the certification if you do not participate in the peer review of three peers in your cohort."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/project/004_b6c7eb17d8_how-is-my-capstone-project-going-to-be-evaluated.md", "metadata": {"id": "b6c7eb17d8", "question": "How is my capstone project going to be evaluated?", "sort_order": 4}, "content": "Each submitted project will be evaluated by three randomly assigned students who have also submitted the project.\n\nYou will also be responsible for grading the projects of three fellow students yourself. Please be aware that not complying with this rule will result in failing to achieve the Certificate at the end of the course.\n\nThe final grade you receive will be the median score of the grades from the peer reviewers.\n\nThe peer review criteria for evaluation must follow the guidelines defined here (TBA for link)."}
{"source": "DataTalksClub/faq", "filename": "_questions/mlops-zoomcamp/project/005_0d200c8c58_homework-what-is-the-criteria-of-scoring-home-work.md", "metadata": {"id": "0d200c8c58", "question": "Homework: What is the criteria of scoring home work?", "sort_order": 5}, "content": "Each homework assignment has a scoring system based on the following criteria:\n\n- Answering 6 questions correctly: **6 points**\n- Adding 7 public learning items: **7 points**\n- Adding 1 valid question to the FAQ: **1 point**\n\nIn total, you can earn up to **14 points** per homework, which will contribute to the leaderboard ranking."}
{"source": "DataTalksClub/faq", "filename": "agents.md", "metadata": {}, "content": "use uv for dependency management and running python\n\nperiodically commit to git. use double quotes for commit messages, and put the entire message in one line."}
{"source": "DataTalksClub/faq", "filename": "index.md", "metadata": {"title": "DataTalks.Club FAQ"}, "content": "# DataTalks.Club FAQ\n\nWelcome to the FAQ collection for DataTalks.Club courses.\n\n## Available Courses\n\n- [Data Engineering Zoomcamp](data-engineering-zoomcamp.md)\n- [Machine Learning Zoomcamp](machine-learning-zoomcamp.md)\n- [MLOps Zoomcamp](mlops-zoomcamp.md)\n\n## About\n\nThis site contains frequently asked questions and answers from the DataTalks.Club community courses."}
{"source": "DataTalksClub/faq", "filename": "tests/README.md", "metadata": {}, "content": "# Test Suite for FAQ Site Generator\n\nThis directory contains comprehensive tests for the `generate_website.py` script, organized into focused test files for maintainability and clarity.\n\n## Test Structure\n\n### Unit Tests (`tests/unit/`)\nTests for individual functions and components:\n\n- **`test_frontmatter.py`** - YAML frontmatter parsing\n  - Valid/invalid YAML handling\n  - Edge cases (empty, malformed, special characters)\n  - Multiline values and nested structures\n\n- **`test_url_conversion.py`** - URL detection and conversion\n  - HTTP/HTTPS URL detection\n  - Code block and inline code preservation  \n  - Trailing punctuation handling\n  - Complex mixed scenarios\n\n- **`test_markdown.py`** - Markdown processing\n  - Basic markdown to HTML conversion\n  - Image placeholder replacement\n  - Table and task list processing\n  - Syntax highlighting integration\n\n- **`test_renderer.py`** - HighlightRenderer class\n  - Language alias handling\n  - Syntax highlighting functionality\n  - Code span and link rendering\n  - Error handling for invalid languages\n\n- **`test_course_processing.py`** - Course and metadata processing\n  - Course metadata loading\n  - Section and question file processing\n  - Error recovery and validation\n\n- **`test_jinja_setup.py`** - Jinja2 template environment\n  - Template loading and rendering\n  - Custom filter functionality\n  - Auto-escaping behavior\n\n- **`test_sorting.py`** - Sorting and organization\n  - Question sorting by sort_order\n  - Section ordering by metadata\n  - Handling missing sections\n\n### Integration Tests (`tests/integration/`)\nEnd-to-end workflow tests:\n\n- **`test_site_generation.py`** - Complete site generation\n  - Full pipeline from questions to HTML\n  - Multiple course handling\n  - Asset copying and file structure\n\n- **`test_real_world.py`** - Real-world scenarios\n  - Large courses with many sections\n  - Unicode and special character handling\n  - Complex markdown features\n  - Error recovery and partial processing\n\n## Running Tests\n\n### Run All Tests\n```bash\n# Run all tests\nuv run --extra dev pytest\n\n# Or use the test runner\npython run_tests.py\n```\n\n### Run Specific Test Categories\n```bash\n# Unit tests only\nuv run --extra dev pytest tests/unit/ -v\n\n# Integration tests only  \nuv run --extra dev pytest tests/integration/ -v\n```\n\n### Run Specific Test Files\n```bash\n# Test URL conversion functionality\nuv run --extra dev pytest tests/unit/test_url_conversion.py -v\n\n# Test site generation\nuv run --extra dev pytest tests/integration/test_site_generation.py -v\n```\n\n### Run Specific Test Methods\n```bash\n# Test specific functionality\nuv run --extra dev pytest tests/unit/test_url_conversion.py::TestConvertPlainUrlsToLinks::test_preserve_urls_in_code_blocks -v\n```\n\n## Test Coverage\n\nThe test suite provides comprehensive coverage of:\n\n### Core Functionality\n- ✅ YAML frontmatter parsing (10 tests)\n- ✅ URL detection and conversion (15 tests)\n- ✅ Markdown processing (14 tests)\n- ✅ Syntax highlighting (15 tests)\n- ✅ Course metadata handling (8 tests)\n- ✅ Template rendering (7 tests)\n- ✅ Sorting and organization (8 tests)\n\n### Integration Scenarios\n- ✅ Complete site generation (4 tests)\n- ✅ Large-scale processing (3 tests)\n- ✅ Unicode and special characters (1 test)\n- ✅ Complex markdown features (1 test)\n- ✅ Error recovery (1 test)\n\n### Edge Cases\n- Empty files and directories\n- Invalid YAML frontmatter\n- Missing required fields\n- Binary files in question directories\n- Unicode characters and emojis\n- Complex nested markdown structures\n- Multiple courses and sections\n- Large datasets (50+ questions)\n\n## Key Test Features\n\n### URL Detection Testing\nThe URL conversion tests ensure that:\n- Plain text URLs are converted to clickable links\n- URLs in code blocks remain untouched\n- URLs in inline code are preserved\n- Existing markdown links are not double-converted\n- Trailing punctuation is handled correctly\n\n### Error Handling\nTests verify graceful handling of:\n- Invalid YAML frontmatter\n- Missing required fields\n- Non-existent files and directories\n- Binary files in question directories\n- Malformed markdown content\n\n### Real-World Scenarios\nIntegration tests simulate:\n- Large courses with 10+ sections and 50+ questions\n- International content with unicode characters\n- Complex markdown with tables, task lists, and code blocks\n- Mixed valid/invalid content in the same course\n\n## Adding New Tests\n\nWhen adding new functionality to `generate_website.py`:\n\n1. **Add unit tests** for new functions in appropriate `test_*.py` files\n2. **Add integration tests** if the change affects the complete workflow\n3. **Test edge cases** and error conditions\n4. **Update this README** if new test categories are added\n\n### Test Naming Convention\n- Test files: `test_<functionality>.py`\n- Test classes: `Test<ClassName>`\n- Test methods: `test_<specific_behavior>`\n\n### Example Test Structure\n```python\nclass TestNewFeature:\n    \"\"\"Test the new feature functionality\"\"\"\n    \n    def test_basic_functionality(self):\n        \"\"\"Test basic feature behavior\"\"\"\n        pass\n        \n    def test_edge_cases(self):\n        \"\"\"Test edge cases and error conditions\"\"\"\n        pass\n        \n    def test_integration_with_existing_features(self):\n        \"\"\"Test how feature integrates with existing code\"\"\"\n        pass\n```\n\n## Dependencies\n\nThe test suite requires:\n- `pytest>=7.0.0` (defined in pyproject.toml dev dependencies)\n- All main project dependencies for testing the actual functionality\n\n## Configuration\n\nTest configuration is defined in `pyproject.toml`:\n- Test discovery paths\n- Test file patterns\n- Output formatting\n- Custom markers for test categorization"}
